{"pages":[{"title":"","text":"个人简介 93年生人，从事java服务端开发工作4年，现居上海","link":"/about/index.html"},{"title":"","text":"","link":"/message/index.html"},{"title":"","text":"载入中... $.getScript(\"/js/gitalk_self.min.js\", function () { var gitalk = new Gitalk({ clientID: '17297b562584b5cec3d7', clientSecret: '46c7652ed4f09fb1d847d15df81500a5a2ceeae8', id: '233', repo: 'exceting.github.io', owner: 'exceting', admin: \"exceting\", createIssueManually: true, distractionFreeMode: false }); gitalk.render('comment-container1'); });","link":"/timeline/index.html"},{"title":"","text":"JAVA进化论 JAVA进阶篇 NIO基础教程","link":"/course/index.html"}],"posts":[{"title":"分布式任务调度系统-PowerJob","text":"最近在调研分布式任务如何选型，最终选择了比较年轻的PowerJob，下面会简单介绍下这个框架的使用以及它的运行流程。 一、选择PowerJob的原因1.1：同类产品对比官方文档给出的同类产品对比图： 作者の人生经验：https://zhuanlan.zhihu.com/p/157614020 1.2：特点定制方面：代码较简单，易于理解和改造，比如我们就集成了自己的服务发现平台来管理powerjob服务端节点。 功能方面：很全面，我们能想到的功能它全部支持 体积方面：非常轻量，代码量少，而且不依赖外部乱七八糟的服务（比如zk），仅需要一个mysql即可 1.3：成熟度产品上线仅3个月，已积累1.8k的star： 并且已有较大的公司和机构接入： 二、PowerJob的工作流程2.1：基本概念：app、worker、job、serverapp可以理解为我们的一个工程项目，worker可以理解成一个app的集群节点，而job则是一个任务（可以是简单的定时任务，也可以是复杂的MapReduce），它跟具体某个app对应，而一个app则可以拥有很多job，它们的关系如下： server即为PowerJob节点，主要负责任务的监听和派发，可以单点部署，也可以集群部署，它的工作流程详细参考2.2和2.3 2.2：app&amp;server的绑定首先，在worker里配置上所需的server节点信息（这些节点信息也可以从服务发现获取），在worker启动时会注册到server，此时server便拥有了所有app的worker信息： 这层绑定关系在worker们启动后即可确认，这时worker端会启动两个定时任务，一个是heartbeat，用来给server端发送心跳，这样server端即可知道对应app有多少个worker在运行了，另外一个是discovery，用来同步server端状态，如果有备份server，可以用来做高可用。 图里powerjob服务端是单点部署，这不高可用，下面来看下powerjob是如何实现服务高可用的。 2.3：高可用2.2的过程运转的不错，但是如果server端故障，那么所有的任务将直接终止无法执行，这是我们不愿意看到的，因此需要给powerjob备份一个节点，假设现在它有两个节点，那么当单点故障时，powerjob会通过discovery机制做故障转移： 我们目前基于集群部署，一般有3台机器，一台master，两台slave。 2.4：server端的调度说完了整体的绑定流程，下面来详细看下server端是如何轮询和派发任务的： 2.5：部署顺序 部署PowerJob的server端（这个一般情况下都预先部署好了） 编写自己的job类app项目，写好各类job，在自身配置文件里指定一个server来调度自己 前往PowerJob客户端注册该app信息 启动发布该app项目（此时app集群会跟对应的server绑定上） 在PowerJob客户端利用该app登录，将该app里的job配置上去（此时便可指定cron表达式、并发度、是否mapreduce之类的信息） 经过上面的步骤，你在app内编写的job程序便可被对应的PowerJob的server调度到了，不过上面的过程是裸用powerjob时需要做的，现在已经被我们大仓简化了，具体用法会出使用文档。 三、任务类型&amp;验证3.1：如何定义PowerJob任务任务类必须要实现powerjob提供的一些接口，它们继承关系图如下： 业务方只需要继承（实现）这些类（接口）即可，powerjob在执行任务时会率先从spring上下文里获取实例，如果你没使用spring，那么powerjob就会利用反射机制来触发你的业务逻辑（这在下方具体实例中有所体现，表单里填写的是类的全限定名）。 3.1：任务类型-单机任务这种就是普通定期执行的任务，属于最常用最普通的任务，现在来做下测试，测试用例代码如下： 代码块112345678910@Slf4j@Componentpublic class StandaloneProcessor implements BasicProcessor { //实现BasicProcessor接口 @Override public ProcessResult process(TaskContext context) { //核心触发逻辑 log.info(\"简单定时任务-触发！，参数是：{}\", context.getJobParams()); return new ProcessResult(true, context + \": \" + true); }} 然后将我们的job发布，发布完成后在powerjob平台对应app下配置该任务的调度信息： 配好之后就可以运行我们的job了，来看看日志平台的打印： 3.2：任务类型-广播任务3.2.1：广播任务改造将上面的任务改成广播模式： 其实就是由原先单机触发，广播给worker集群里每个节点都触发一次。 3.2.2：广播模式接口实现实例代码如下： 代码块2123456789101112131415161718192021222324@Slf4j@Componentpublic class BroadcastProcessorDemo extends BroadcastProcessor { //继承BroadcastProcessor类 @Override public ProcessResult preProcess(TaskContext context) throws Exception { //在所有节点广播执行前执行，只会在一台机器执行一次 log.info(\"广播前，参数：{}\", context.getJobParams()); return new ProcessResult(true); } @Override public ProcessResult process(TaskContext taskContext) throws Exception { //核心逻辑，会广播给所有节点并行处理 log.info(\"广播核心逻辑触发！参数：{}\", taskContext.getJobParams()); return new ProcessResult(true); } @Override public ProcessResult postProcess(TaskContext context, List&lt;TaskResult&gt; taskResults) throws Exception { //在所有节点广播执行完成后执行，只会在一台机器执行一次 //通知执行结果，有点类似下面要测试的MapReduce的reduce方法 log.info(\"广播任务执行完毕，reduce触发！TaskContext: {}，List&lt;TaskResult&gt;: {}\", JSONObject.toJSONString(context), JSONObject.toJSONString(taskResults)); return new ProcessResult(true, \"success\"); }} 然后在powerjob设置该任务： 运行结果如下： 广播模式执行流程如下（可以跟下方的MapReduce模式坐下对比）： 3.3：任务类型-Map（大任务拆分）map就是一次大的任务可以被拆分成细碎的小批次任务进行分布式执行，测试用例代码如下： 代码块31234567891011121314151617181920212223242526272829303132333435363738@Slf4j@Componentpublic class MapProcessorDemo extends MapProcessor { //继承MapProcessor private static final int batchSize = 100; //单批发送数据量 private static final int batchNum = 2; //一共2批，默认上限为200批，再多就要适当调整batchSize大小了 @Override public ProcessResult process(TaskContext context) throws Exception { if (isRootTask()) { //如果是根任务（说明map刚被调度到），则触发任务拆分 log.info(\"根任务，需要做任务拆分~\"); List&lt;SubTask&gt; subTasks = Lists.newLinkedList(); for (int j = 0; j &lt; batchNum; j++) { SubTask subTask = new SubTask(); subTask.siteId = j; subTask.itemIds = Lists.newLinkedList(); subTasks.add(subTask); //批次入集合 for (int i = 0; i &lt; batchSize; i++) { //内部id集合，这里只是举例，实际业务场景可以是从db里获取的业务id集合 subTask.itemIds.add(i); } } return map(subTasks, \"MAP_TEST_TASK\"); //最后调用map，触发这些批次任务的执行 } else { //子任务，说明批次已做过拆分，此时被调度到时会触发下方逻辑 SubTask subTask = (SubTask) context.getSubTask(); //直接从上下文对象里拿到批次对象 log.info(\"子任务，拿到的批次实体为：{}\", JSON.toJSONString(subTask)); return new ProcessResult(true, \"RESULT:true\"); } } @Getter @NoArgsConstructor @AllArgsConstructor private static class SubTask { //定义批次实体（业务方可自由发挥） private Integer siteId; //批次id private List&lt;Integer&gt; itemIds; //批次内部所携带的id（可以是我们自己的业务id） }} 代码意义注释已给出，发布完成后可在powerjob平台配置，如下： 然后看下运行结果： 上面就是一次map任务触发的演示过程（注：被拆分的map子任务只要有一个失败，即认为整个map任务为失败状态，但不具备事务性）。 Map任务执行流程如下： 3.4：任务类型-MapReduce（大任务拆分与归并）相比普通map，MapReduce在子任务执行完毕后可以知道它们的执行结果，并做出接下来的自定义逻辑处理，测试用例代码如下： 代码块4123456789101112131415161718192021222324252627282930313233343536373839404142434445@Slf4j@Componentpublic class MapReduceProcessorDemo extends MapReduceProcessor { //需要继承MapReduceProcessor private static final int batchSize = 100; private static final int batchNum = 2; @Override public ProcessResult process(TaskContext context) { //该方法跟普通map方法实现一致，主要用来拆分子任务和执行子任务 if (isRootTask()) { log.info(\"根任务，需要做任务拆分~\"); List&lt;SubTask&gt; subTasks = Lists.newLinkedList(); for (int j = 0; j &lt; batchNum; j++) { SubTask subTask = new SubTask(); subTask.siteId = j; subTask.itemIds = Lists.newLinkedList(); subTasks.add(subTask); //批次入集合 for (int i = 0; i &lt; batchSize; i++) { subTask.itemIds.add(i); } } return map(subTasks, \"MAP_TEST_TASK\"); } else { SubTask subTask = (SubTask) context.getSubTask(); log.info(\"子任务，拿到的批次实体为：{}\", JSON.toJSONString(subTask)); return new ProcessResult(true, \"RESULT:true\"); } } @Override public ProcessResult reduce(TaskContext context, List&lt;TaskResult&gt; taskResults) { //相比普通map任务，多出reduce方法，这里将两个参数全部打印出来 log.info(\"子任务执行完毕，reduce触发！TaskContext: {}，List&lt;TaskResult&gt;: {}\", JSONObject.toJSONString(context), JSONObject.toJSONString(taskResults)); return new ProcessResult(true, \"RESULT:true\"); } @Getter @NoArgsConstructor @AllArgsConstructor private static class SubTask { private Integer siteId; private List&lt;Integer&gt; itemIds; }} 代码意义注释已给出，发布完成后可在powerjob平台配置，如下： 现在看下运行结果： MapReduce的运行流程如下： 3.5：工作流工作量，顾名思义，遵循任务A → 任务B → 任务C这个流程，只需要在表单里选中下方选项即可将任务本身设置成一个工作流任务： 需要注意的是，工作流有自己的调度触发器，因此后面框框即便填了CRON表达式，也不会生效。 现在让我们将前面实验中的所有任务都设置成工作流模式： 现在去工作流编辑里编辑工作流触发顺序： 任务按照编排好的顺序，执行了下来。 现在我们把工作流改成下面这样： 触发顺序就成了下面这样： 四、定时类型&amp;验证4.1：CRON表达式前面的例子均通过该方式触发，支持一般CRON表达式，但是不支持秒级任务（即便配置了每秒执行一次，实际执行却是15s一次，秒级任务可以通过固定频率或固定延迟来做~），由CRON表达式触发的定时任务，在任务本身超时的情况下，仍然保持对应频率执行，比如，我们让某个简单定时任务每1min执行一次，但实际运行的业务逻辑调成2min，系统调度频率如下： 可以看到，即便是任务需要花费很长时间，任务也是按照每一分钟一次的频率调度，但接下来介绍的延时任务就不一样了。 4.2：固定频率如果你需要让某个任务按照固定某个频率执行，可以尝试使用固定频率来做： 来看下它的调度结果： 4.3：固定延迟如果你需要让某个任务按照固定某个频率延迟执行，沿用4.1的例子，现在配置成延迟任务： 它的调度结果如下： 可以看到，现在是2min调度一次，相比CRON和固定频率，这个调度是串行化的，后续的任务需要前面的任务执行完才可以执行。 五、其他5.1：任务表单如果对创建任务时表单的每一项不是很了解，请参考官方文档：https://www.yuque.com/powerjob/guidence/nyio9g#v8uF4 5.2：如何配置工作流？参考文档：https://www.yuque.com/powerjob/guidence/ysug77#xgylz（不太好用，用的时候需要注意）","link":"/2020/09/03/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6%E7%B3%BB%E7%BB%9F-PowerJob/"},{"title":"WRR算法验证实验","text":"一、实验整理1.1：实验服务情报参与实验的服务与集群配置，和P2C验证实验里的一致，请参考：P2C算法验证实验 1.2：实验case整理1.2.1：各节点权重值一致场景：节点权重值均为10，比例为1:1 预期：流量均匀分配 1.2.2：各节点权重值均不一致场景：6个节点权重值分别为：10、20、30、40、50、60 预期：流量分配按照权重值从大到小逐级递减 1.2.3：各节点整体权重比例1:2场景：6个节点中，3个权重值为10，另外3个权重值为20 预期：权重值为20的3个节点qps比权重为10的3个节点多出一倍 1.2.4：中途节点变更场景：在旧节点均为10的基础上灰度新节点 预期：新节点被引入，但qps不如旧节点高，最终全部接流后达到新的平衡，且qps一致。 二、实验结果2.1：各节点权重值一致可以看到，在各节点权重值一致时，流量分配极度均匀，符合预期。 2.2：各节点权重值均不一致共6个节点，当配置权重值为10、20、30、40、50、60时，则流量分配按照权重值从大到小逐级递减，测试结果如下图，符合预期。 2.3：各节点整体权重比例1:26个节点中，3个权重值为10，另外3个权重值为20，测试结果如下图，流量比例1:2，符合预期。 2.4：中途节点变更权重值均为10，后期滚动发版service，效果如下图，符合预期。","link":"/2020/08/28/WRR%E7%AE%97%E6%B3%95%E9%AA%8C%E8%AF%81%E5%AE%9E%E9%AA%8C/"},{"title":"P2C算法验证实验","text":"本篇文档是针对前篇负载均衡-P2C算法进行的实验验证，会利用一个网关和一个服务端的集群来验证P2C在各个业务场景下的表现。 一、实验整理1.1：实验服务情报参与实验的是两套集群，分别是网关系统和基础服务，网关系统在接收到外部请求后会在grpc内部利用P2C算法pick出合适的基础服务节点发起调用，网关系统拥有1台机器，基础服务集群拥有6台机器，初始权重值都是10，如图： 1.2：实验case整理按照P2C算法的实现，参考负载均衡-P2C算法中的P2CLoadBalancer.pick方法，负载率的完整计算方式如下： ps：上述公式中参与运算的数据来源也已在负载均衡-P2C算法中做过详细介绍，如忘记，可前往查看代码。 1.2.1：实验A组，无cpu使用率返回时 👾 解释：用来模拟对端不支持cpu使用率携带的情况，按照之前P2C算法的代码，此时cpu恒等于默认值500，因此整体负载率取决于其余几个属性。 操作 目的 预期 正常请求 分母一致，看下流量分配是否均匀 6台机器的流量分配大致均匀 尝试调小其中一台weight的值 通过调小weight的值来增大其负载率 被降低weight的机器流量分配较其它几个节点明显降低 尝试让某台机器的请求错误量飙升 通过增大错误量来降低client_success的值，从而增大其负载率 错误率飙升的机器流量分配较其它几个节点明显降低，恢复后流量增大 表1 1.2.2：实验B组，有cpu返回，成功率和权重值一致 👾 解释：此时保持client_success和weight不变，专注分子对负载率的变化，在这种情况下，分母固定，负载率则完全取决于分子，分子越小，负载率越低，被pick的概率就越大。 操作 目的 预期 正常请求 不干涉分子分母的情况下，看下流量分配是否均匀 6台机器的流量分配大致均匀 尝试调大其中一台机器的cpu使用率 让该机器分子变大，负载率变小 cpu使用率变大的机器分配到的流量明显低于其他几个节点，恢复后流量增大 尝试调大其中一台机器的延迟率 让该机器分子变大，负载率变小 延迟率变大的机器分配到的流量明显低于其他几个节点 尝试无脑增大某台机器的拥塞度 让该机器分子变大，负载率变小 拥塞度变大的机器分配到的流量明显低于其他几个节点 表2 1.2.3：实验C组，衰减值测试 👾 解释：调整衰减值（下称k值），查看衰减值对P2C的影响。 操作 目的 预期 模拟某台机器某段时间内网络延迟 不干涉分子分母的情况下，看流量是否倾斜 存在网络延迟的节点流量分配明显低于其他几个节点 同上，但调整k的值 测试k值对P2C算法在网络延迟情况下的影响以及它的具体作用 k值越大，网络延迟恢复后流量分配恢复到正常水平的速度越慢，反之越快 表3 1.2.4：实验D组，中途节点变更 👾 解释：不干涉分子分母，中途发版，查看发版对P2C的影响。 操作 目的 预期 中途发版 不干涉分子分母，中途发版，查看发版对P2C的影响 不影响性能，且新入的节点及时均摊流量 表4 二、实验结果 👽 以下实验均通过ab压测进行，通过开启50个线程并发请求1000w次。 2.1：实验组A-结论2.1.1：正常请求实验结果：基本符合预期，每个节点较均匀的访问，至于为什么黄色节点的qps比较低，因为它的平均耗时比较大（配合图3）。 节点平均耗时（将其作为latency代入之前的公式，基本符合上图的qps分配，即latency越小，被pick的概率越大）： 2.1.2：调小其中一个节点的weight将其中某个节点的权重改成5，其余仍为10，可以看到，权重值为5的节点（橙色线）qps相比其他权重值为10的节点，少了一半。 2.1.3：尝试让某台机器的请求错误量飙升让其中某个节点在某时刻故障，一段时间后恢复，测试结果如下图，可以看到，在某节点（橙色线）发生网络故障时，qps会以一定的速度下降，然后到达最低值（此时按照3s一次的概率被pick，可以理解成该节点当前处于”半熔断“状态），期间其他节点qps加大，故障恢复后，又以一定的速度恢复至原qps。 2.2：实验组B-结论2.2.1：正常请求让对端传送cpu使用率，公式中的其余数据不做干涉，得出下图，跟2.1.1一致，大致平均： 2.2.2：尝试调大其中一台机器的cpu使用率让其中某个节点在某时刻cpu使用率飙升，一段时间后恢复，测试结果如下图，可以看到，在某节点（黄色线）cpu使用率飙升时，qps会以一定的速度下降，然后到达最低值，期间其他节点qps加大，故障恢复后，又以一定的速度恢复至原qps。 2.2.3：调大其中一台机器的延迟率让其中某个节点在某时刻延迟率飙升，一段时间后恢复，测试结果如下图，可以看到，在某节点（橙色线）延迟率飙升时，qps会以一定的速度下降，然后到达最低值，期间其他节点qps加大，延迟恢复后，又以一定的速度恢复至原qps。 2.2.4：调大其中一台机器的拥塞度省略...不再实验，同样作为分子，调大和恢复它也一样能达到上图的效果。 2.3：实验组C-结论同样模拟网络延迟，此时让2个节点发生网络延迟，但是衰减值不同（橙色节点为5，青色节点使用600），可以看到，衰减值越大，感知问题的速度相对越慢，恢复速度也相对越慢。 2.4：实验组D-结论在service运行中灰度新节点（注：图中新节点被接入后qps明显比旧节点大的原因是新节点耗时明显更低，请结合图12看）","link":"/2020/08/25/P2C%E7%AE%97%E6%B3%95%E9%AA%8C%E8%AF%81%E5%AE%9E%E9%AA%8C/"},{"title":"负载均衡-P2C算法","text":"P2C算法全称Pick of 2 choices，相比WRR，P2C有着更科学的LB策略，它通过随机选择两个节点后在这俩节点里选择优胜者来避免羊群效应，并通过指数加权移动平均算法统计服务端的实时状态，从而做出最优选择。 一、工作流程P2C算法下的每个节点（下称Node）必须含有下方图中几个指标，它们的计算方法已经标出： 因此最终loadbalancer里保存的节点就会变成下图的结构，pick节点时需要做如下比较： 通过上面的流程可以看到，大体流程还是随机，相比普通的随机LB，它是随机选择两个node，然后比较它们的负载率，然后选出当前负载率最小的node。 二、数据统计通过上面的简单介绍，可以知道P2C算法的大体流程，那么现在问题就变得简单多了，只需要知道负载率，就可以完成这个“简单”的负载均衡器，但是负载率是由上图五个指标共同参与计算完成的，那现在问题的关键就是如何完成这五个指标的统计，下面来介绍下这五个指标如何计算。 2.1：weight这个很简单，主要是人为配置的定制，用于给不同的机器按照机器配置分配上不同的权重，权重越高，越容易被pick到。这个值可以做在服务注册与发现里，进行为每个节点分配一个权重值。 2.2：server_cpu这个值可以通过服务端回写元数据来搞定，比如一次请求： 所以这个值可以通过服务端埋点的方式解决掉。 2.3：inflight这个代表节点请求拥塞度，代表着当前节点有多少个请求未完成或者正开始请求，它的统计也很简单： 按照这种，每次pick到发送请求前先原子+1，response后说明一次请求完成，这时再原子-1，这样一增一减，在超多线程pick下（即高qps），当前线程获取到的inflight瞬间值，就是在这个时段的拥塞度，比如一个节点如果很闲，响应速度也快，那么它的拥塞度肯定极低（因为一增一减的操作很快就完成，不会淤积过多）。 2.4：latency &amp; client_success前面的都很好了理解，这俩属性比较麻烦，它们分别代表请求延迟率和成功率。 这俩值该怎么计算？一般来说是通过平均数的方式来计算，但计算平均数的方式有很多，可以先用我们最熟悉的算术平均数来计算，它的计算公式如下： 利用此算法套入我们的场景后，请求总耗时比上请求总次数，就是latency： 请求总成功数比上请求总次数，就是client_success： 整个过程可以抽象成下面这样： 根据图5的流程，可以通过每次请求，累加总请求数、总耗时、总成功数，然后利用算术平均法更新latency和client_success的值，这一切可以运作的很好，可是算术平均有个很大的缺陷，不够敏感，出现的网络波动一下就被平均了，是不是想到了你和马云放在一起统计资产时被平均的悲哀？ 2.4.1：算术平均数模拟首先模拟1000次请求，让每次请求在0~25之间产生随机数用来作为本次请求的耗时（为了模拟真实情况，让一些请求耗时过高），每次请求计算当前的算术平均数，然后可以得到下面的统计图： 可以看到，正常情况下，算术平均数表现出很好的稳定性，红线一致维持在黑线的中间，且浮动不大。 现在让我们搞点事情，假设在第100次请求到第200次请求间，模拟下网络抖动，让这期间的响应时间变成125ms（相比正常情况翻五倍），得到统计图如下： 可以看到平均数受到这次抖动的影响，需要非常长的时间才能感知到抖动的到来，然后恢复时又需要花很长时间才能恢复到抖动前的较正常的水平，也就是说，第100~200间的100次慢请求，不光要到140次请求后才能较明显的感知平均值的上涨，还导致网络恢复后，到第1000次请求时，平均值还没有恢复到正常水平（但其实从第200次请求后，响应时间就正常了），算术平均数导致的这类问题显然是很严重的，因为我需要的算法是可以立马感知到网络波动，网络恢复后又可以在较短的时间内恢复正常水平。 有没有更好的统计办法来避免这类问题并且达到我们的要求呢？ 2.4.2：指数加权移动平均算法（EWMA）模拟 参考资料：深入解析TensorFlow中滑动平均模型与代码实现 公式如下： 解释：vt代表第t次请求的指数加权平均耗时，vt-1代表上次请求的指数加权平均耗时，θt代表第t次请求的实际耗时。 β值的定义如下： vt ≈ 1/(1 - β) 次的平均耗时 例：假设β等于0.9，1/(1 - β) 就等于10，也就是vt约等于它前10次请求的平均耗时；假设把β值调大道接近1，例如，将β等于0.98，1/(1-β)=50，按照刚刚的说法也就是当前请求的前50次请求的平均耗时。 由此可以推导出： β值越大，统计区间越大，当前平均值的计算受到之前平均值的影响也就越大（曲线越平滑，呈现一个平缓的变化趋势） β值越小，统计区间越小，当前平均值的计算受到之前平均值的影响也就越小（曲线贴近统计原值） β极小时，便可以认为当次的平均耗时约等于当次本身的实际耗时。 上面的结论接下来会通过实验来进行验证。 其实这些根据上面的公式很容易推到出来，比如现在有两次请求，第一次耗时25ms，第二次耗时50ms，代入公式，计算出第一次和第二次的指数加权平均值为： v1 = β * 0 + (1 - β) * 25 v2 = β * v1 + (1 - β) * 50 可以看到，β值越小，意味着本次请求的实际耗时占比越大，β值越大，之前计算得到的平均值占比越大。 根据上面的理解，相比普通平均值的计算，它更在乎的是一段时间内的平均趋势，而不是直接把当前实际耗时累加到总耗时里参与算术平均运算，这样有一个好处，那就是平均数变化会更加平滑（这个取决于β值的大小，后续会给出证明）。 我们再来利用此算法来模拟下1000次请求（同样为了模拟真实情况，让一些请求耗时过高），β我们取值0.9，代表最新请求时的平均值计算会受到最近10次耗时的影响进行平滑过渡，运行结果绘制如下图： 可以看到它得出的平均数曲线并没有算术平均那么稳定，但可以看出，每次网络波动会提升其加权均值，不像算术平均值那样完全不受网络波动影响。 接下来同样假设第100~200次请求，发生了网络延迟，延时5倍，再次利用ewma算法做下模拟： 请将这张图跟图7进行对比，你会发现，利用指数加权平均算法计算出的平均值在网络恢复时，以极快的速度恢复到了正常水平。 相比算术平均的绝对平均值，指数加权移动平均算法更重要的是它平滑的模拟了平均值的趋势，平均值曲线的峰值和负峰值受β影响，β越大，则当前平均数受到前面数据的影响越大，反之越小，比如我们把图9里的β值调整为0.98，此时在计算当前平均值时则受到前面(1/0.02) = 50个平均值的影响，便得到下图： 因为β值大，所以后续每个平均值都会受到更多前面值的影响，而自己的部分仅占很少影响（参考公式理解），所以它相比图9，在出现网络抖动后，更缓慢的恢复为正常均值，但由于它所统计的范围更大，因此平均数曲线会非常平缓，β值越大，统计周期越长，越能体现某个时段的平均趋势（比如图10里的平均数曲线已经趋近于算术平均数）。 再比如，我们将β设置为0.32，那么计算均值时仅受到前面(1/0.68) = 1.47个均值影响，说白了就是平均值轨迹几乎和正常响应时间重叠： 可以看到，当β很小时，受影响因子无限趋近于1，越趋近于1则越贴近原本值。 2.4.3：结论 实验组 对照组 目的 结论 图6 图8（β=0.9） 模拟网络正常情况下，两种算法对均值的统计区别 算术平均值非常稳定，对单次网络抖动完全无感知，ewma均值则会随着响应时间动态变化，因此单次网络抖动后会稍微提升均值，之后便很快恢复 图7 图9（β=0.9） 模拟一段网络延迟，看两种算法的均值变化 算术平均值会缓慢提升，之后再次以极慢的速度下降，对网络延迟反映迟钝，网络延迟结束后仍然要花很长时间才能恢复到正常均值水平，ewma均值则迅速提升，恢复后迅速下降至正常水平 图9（β=0.9） 图10（β=0.98） 调大ewma的β值 β值越大，每次计算均值时受到之前均值影响越大，则平均曲线更加平滑，因此图10的曲线要比图9表现更加平滑，但付出的代价是对网络延迟反应也变的迟钝（但也碾压算术平均） 图9（β=0.9） 图11（β=0.32） 调小ewma的β值 β值越小，每次计算均值时受到之前均值影响越小，则平均曲线更加趋近于每次的实际耗时，因此图11的曲线要比图9表现的更加趋近于每次的实际耗时，顺理成章的，它对网络延迟的反映极迅速 表1 通过实验，可以看出ewma的优势极大，但β的取值需要仔细斟酌，若β太小，则无法很好的体现出平均值，若β太大，很好的体现了平均值，但对网络波动的反应相对迟钝，这里就考虑到一个折中的方案： 实时调整β值，比如ewma可以在网络波动时适当降低β的值，使其快速感知到波动的存在，当网络波动结束后，适当提升β的值，这样就可以在网络稳定的情况下较好的反映一个区段内的均值情况，这样等于结合了图10和图11各自的优点，实现后将达到一种效果：快速感知网络延迟并迅速提高其均值，当网络恢复后，慢慢降回正常水平（均值恢复需要慢慢进行，因为刚恢复的节点稳定性不可信，慢慢恢复到正常水平，以信任其稳定性） 2.4.4：利用衰减函数动态调整β值通过上面的要求，我们需要完善这个变化的β，那么它该如何变化呢？如何能达到碰到网络波动时迅速感知，当波动过后慢慢恢复的效果呢？慢慢恢复需要多慢？可不可以通过调整某个阈值来控制恢复的速率？ 带着上面的问题，需要了解一下：衰减函数（参考：牛顿冷却定律） 计算方法为： e是数学常量，△t表示第t次请求的耗时，k表示衰减系数，它的函数图如下： 我们把k*△t看做x的取值，那么k和△t成正比，即：k和△t取值越大，β就越小 现在来看看这个结论支不支持我们要实现的功能： 网络抖动时，假设△t非常大，即便不乘k值，β值也会变得很小，这是符合我们预期的，我们需要的就是在网络抖动时，迅速感知 网络恢复时，△t迅速降低，假设此时△t非常小，则k值越大，图12里对应的x越大，β的值就越小，事实上通过实验可以得出，如果k值很大，得出的曲线近乎等于图11。 经过上面的梳理，发现k值似乎没有起到衰减作用，反而因为它的存在导致β值降低，它的取值在网络抖动恢复后依旧在削弱β的值，导致网络恢复后迅速降低到正常水平，这是我们不愿意看到的，那么上面的函数需要做下变体，即让△t和k值成反比即可： 此时结论如下： 网络抖动时，假设△t非常大，即便k值起到中和作用，β值较之前也会明显变小，这是符合我们预期的，我们需要的就是在网络抖动时，迅速感知 网络恢复时，即使△t迅速降低，那么由于k值的中和（△t/k的值大小和k值成反比），k越大，β越大，则均值计算受之前波动期的均值影响越大，曲线恢复越缓慢。 这点可以通过下方的验证得到证实，调整衰减系数k，的确可以控制在遇到波动时恢复到正常水平时的速度，衰减系数设置越大，波幅越大（恢复越慢），反之越小（恢复越快）。 2.4.5：衰减系数验证第一组：随机次数的网络抖动，衰减系数分别为600和50 第二组：第100~200次请求响应时间扩大5倍，衰减系数仍然是600和50 三、利用JAVA实现P2C算法首先定义Node类： 代码块11234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980public class Node { //惩罚值 private static final long penalty = 250_000_000_000L; //单位：纳秒（250s） //衰减系数 private static final long tau = 600_000_000L; //单位：纳秒（600ms） protected final String host; protected final int weight; //权重 // client统计数据 protected final AtomicLong lag = new AtomicLong(); //加权移动平均算法计算出的请求延迟度 protected final AtomicLong success = new AtomicLong(1000); // 加权移动平均算法计算出的请求成功率（只记录grpc内部错误，比如context deadline） protected final AtomicLong inflight = new AtomicLong(1); // 当前客户端正在发送并等待response的请求数（pending request） protected final AtomicLong svrCPU = new AtomicLong(500); //对应服务端的CPU使用率 // 最近一次resp时间戳 protected final AtomicLong stamp = new AtomicLong(); // 最近被pick的时间戳，利用该值可以统计被选中后，一次请求的耗时 protected final AtomicLong pick = new AtomicLong(); public Node(String host, int weight) { this.host = host; this.weight = weight; } public boolean valid() { return health() &gt; 500 &amp;&amp; svrCPU.get() &lt; 900; } public long health() { return success.get(); //成功率 } public long load() { long lag = (long) (Math.sqrt((double) this.lag.get()) + 1); long load = this.svrCPU.get() * lag * this.inflight.get(); //根据cpu使用率、延迟率、拥塞度计算出负载率 if (load == 0) { // penalty是初始化没有数据时的惩罚值，默认为1e9 * 250 load = penalty; } return load; } //被pick后，完成请求后触发逻辑 public void responseTrigger(long pickTime, long cpu, boolean error) { this.inflight.decrementAndGet(); long now = System.nanoTime(); long stamp = this.stamp.getAndSet(now); long td = now - stamp; //计算距离上次response的时间差，节点本身闲置越久，这个值越大 if (td &lt; 0) { td = 0; } //实时计算β值，利用衰减函数计算，公式为：β = e^(-t/k)，相比前文给出的衰减公式这里是按照k值的反比计算的，即k值和β值成正比 double w = Math.exp((double) -td / (double) tau); long lag = now - pickTime; //实际耗时 if (lag &lt; 0) { lag = 0; } long oldLag = this.lag.get(); if (oldLag == 0) { w = 0; } //计算指数加权移动平均响应时间 lag = (int) ((double) oldLag * w + (double) lag * (1.0 - w)); this.lag.set(lag); //更新 int success = error ? 0 : 1000; //计算指数加权移动平均成功率 success = (int) ((double) this.success.get() * w + (double) success * (1.0 - w)); this.success.set(success); //更新 //更新本次请求服务端返回的cpu使用率 if (cpu &gt; 0) { this.svrCPU.set(cpu); } }} 再来定义LoadBalancer： 代码块212345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182public class P2CLoadBalancer { //闲置时间的最大容忍值 private static final long forceGap = 3000_000_000L; //单位：纳秒（3s） private static final Random r = new Random(); private final List&lt;Node&gt; nodes; //保存了参与lb的节点集合 public P2CLoadBalancer(List&lt;Node&gt; nodes) { this.nodes = nodes; } public Node pick(long start) { //外界给入start，值为当前时间，resp后应给recycle传同样的值 Node pc, upc; if (nodes == null || nodes.size() &lt;= 0) { throw new IllegalArgumentException(\"no node!\"); } if (nodes.size() == 1) { return nodes.get(0); } Node[] randomPair = prePick(); /** * 这里根据各自当前指标，计算出谁更合适被pick * 计算方式： * nodeA.load nodeB.load * ---------------------------- : ---------------------------- * nodeA.health * nodeA.weight nodeB.health * nodeB.weight * * health和weight都是提权用的，而load是降权用的，所以用load除以heal和weight的乘积，计算出的值越大，越不容易被pick */ if (randomPair[0].load() * randomPair[1].health() * randomPair[1].weight &gt; randomPair[1].load() * randomPair[0].health() * randomPair[0].weight) { pc = randomPair[1]; upc = randomPair[0]; } else { pc = randomPair[0]; upc = randomPair[1]; } // 如果落选的节点，在forceGap期间内没有被选中一次，那么强制选中一次，利用强制的机会，来触发成功率、延迟的衰减 long pick = upc.pick.get(); if ((start - pick) &gt; forceGap &amp;&amp; upc.pick.compareAndSet(pick, start)) { pc = upc; //强制选中 } // 节点未发生切换才更新pick时间 if (pc != upc) { pc.pick.set(start); } pc.inflight.incrementAndGet(); return pc; } //pick出去后，等来了response后，应触发该方法 public void recycle(Node node, long pickTime, long cpu, boolean error) { node.responseTrigger(pickTime, cpu, error); } // 随机选择俩节点 public Node[] prePick() { Node[] randomPair = new Node[2]; for (int i = 0; i &lt; 3; i++) { int a = r.nextInt(nodes.size()); int b = r.nextInt(nodes.size() - 1); if (b &gt;= a) { b += 1; //防止随机出的节点相同 } randomPair[0] = nodes.get(a); randomPair[1] = nodes.get(b); if (randomPair[0].valid() || randomPair[1].valid()) { break; } } return randomPair; } } 四、算法验证算法的验证会以实际压测的方式来进行，请前往：P2C算法验证实验","link":"/2020/08/13/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1-P2C%E7%AE%97%E6%B3%95/"},{"title":"负载均衡-WRR算法","text":"一、工作流程1.1：实现加权轮询的方式WRR叫做加权轮询算法，相比普通的轮询算法，它支持给每个节点配置权重，权重越大，越容易被访问，且符合轮询的特点。 一般情况下，我们会按照下方逻辑设计算法的实现： 如图1中流程执行完毕，接下来就跟普通RR算法一样做轮询即可，保证每个虚拟节点都被均匀访问到，而被访问概率也与权重值成正比。 但正如上图所说，乱序那一步是比较容易出问题的，可能会出现下面这样的情况： 1.2：SWRR算法如果不足够散列，意味着轮询窗口内访问不足够均匀，基于此，便有了SWRR算法，它是一种可以平滑访问的、nginx默认的加权轮询算法，相比普通加权轮询，它在轮询访问节点时表现的更加散列。 在该算法中，每个节点有如下基本属性： 结合上面几个属性，来看下SWRR算法下，选举流程是怎样的： 首先是Node信息的初始化，按照图3的描述，每个Node在初始化的那一刻，effective_weight跟它的weight值相等，current_weight都为0： 然后开始pick，来看下pick的流程： 每次pick，都会经历上面的流程，这样来模拟一下这个轮询过程，假设现在有三个节点，节点名为a、b、c，对应权重值为：4:2:1，结合上面的pick流程，它的轮询流程如下： 上表为SWRR两次轮询的执行效果，可以看到pick出来的节点非常散列，而且每一次轮询得到的顺序是一致的，且每次轮询完成后，各项指标会还原为初始状态，以此类推，它的数学推导&amp;证明请参考：Nginx SWRR 算法解读 1.3：SWRR存在的问题上面的按照SWRR模拟的pick过程很完美，但该算法依然有一些漏洞，请参考：QPS 比 Nginx 提升 60%，阿里 Tengine 负载均衡算法揭秘 简单来理解，SWRR有以下缺陷： 算法复杂度为O(N)，而且在pick节点时为了保证准确性，需要加锁 如果刷新某节点的权重值，会导致该节点流量值瞬间暴增 第一点很好理解，因为每次都要选出current_weight最大的那个节点，必然要循环一次所有的节点。第二点存疑，参考上方文章评论区： 我在自测过程中，在算法启动后动态调整weight和effective_weight的值是不存在这个问题的，访问依旧均匀，并不会出现大规模pick到加权节点上的情况，个人猜测他们可能是在探听到weight变化后，把对应节点的current_weight也给改掉了才可能出现这个问题。 1.4：优化SWRR按照上方文章里描述，我们现在需要将O(N)的算法变成O(1)，这样不仅仅性能迅速提升，pick时也无需加锁，对于使用SWRR算法来说是个不错的选择，那如何实现O(1)呢？表1告诉我们，按照算法pick出的节点是有规律性的，以权重和为模来界定一次轮询，而每个轮询窗口内的节点散列顺序完全一致，那么这样优化起来就简单多了： 经过上图里展示的方法，做一层转换，即可达成O(1)成就，这样在高QPS时的效率会显著提升，后续如果有节点信息变更，只需要以同样的方式，刷新虚拟节点有序集合即可。 二、实现这里采用JAVA语言来实现： 代码块1123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100public class WrrLoadBalancer { private int effectiveWeightSum; //总权重值 private Node[] virtualNodes; //虚拟节点有序集 private AtomicLong pointer; //pick次数 public WrrLoadBalancer(Node... nodes) { refreshVirtual(nodes); } //刷新虚拟节点 public void refreshVirtual(Node... nodes) { int total = 0; for (Node node : nodes) { total += node.getEffectiveWeight(); //累加total } int newEffectiveWeightSum = total; Node[] newVirtualNodes = new Node[newEffectiveWeightSum]; for (int i = 0; i &lt; newEffectiveWeightSum; i++) { newVirtualNodes[i] = pickInit(newEffectiveWeightSum, nodes); } this.virtualNodes = newVirtualNodes; this.effectiveWeightSum = newEffectiveWeightSum; pointer = new AtomicLong((long) (Math.random() * effectiveWeightSum)); //随机开始位置 } public Node pickInit(int effectiveWeightSum, Node... nodes) { Node picked = null; for (Node node : nodes) { node.refreshCurrentWeight(); if (picked == null) { picked = node; } else { if (picked.getCurrentWeight() &lt; node.getCurrentWeight()) { picked = node; } } } if (picked == null) { throw new IllegalArgumentException(\"wrr pick error!\"); } return picked.pick(effectiveWeightSum); } public Node pick() { //注：更新节点时，可能下标溢出 return virtualNodes[(int) (pointer.incrementAndGet() % effectiveWeightSum)]; } public static class Node { private final String host; // 用户所配的权重 private final AtomicInteger weight = new AtomicInteger(); // 有效权重，正常情况下，该值等于weight，但是当node本身发生错误时， // 会适当降低该值，后面被选中一次，若不报错，则累加该值，顺利的话最后会再次恢复到weight private final AtomicInteger effectiveWeight = new AtomicInteger(); //后端目前的权重，一开始为0，后期动态调整，选节点的依据，谁这个值最大就选谁 //计算方式，每次被选中， private final AtomicInteger currentWeight = new AtomicInteger(); public Node(String host, int weight) { this.host = host; this.weight.set(weight); this.effectiveWeight.set(weight); } public Node pick(int total) { try { return this; } finally { //节点被选中后，需要\"降权\"，即减去Sum(effective_weight) this.currentWeight.addAndGet(total * -1); } } //刷新currentWeight值，使其累加当前的effectiveWeight值 public void refreshCurrentWeight() { this.currentWeight.getAndAdd(this.effectiveWeight.get()); } //刷新effectiveWeight值 public int getEffectiveWeight() { return this.effectiveWeight.get(); } public int getCurrentWeight() { return this.currentWeight.get(); } public String getHost() { return host; } }} 四、算法验证算法的验证会以实际压测的方式来进行，请前往：WRR算法验证实验","link":"/2020/08/07/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1-WRR%E7%AE%97%E6%B3%95/"},{"title":"定制grpc负载均衡器","text":"一、负载均衡器是如何被grpc加载的1.1：提供provider类每个LoadBalancer对象都是通过一个对象来实例化的，不同的LoadBalancerProvider对象会实例化出不同的LoadBalancer对象，而LoadBalancerProvider对象会通过SPI机制载入到grpc的客户端中，比如在grpc-core里的MATE-INF下： 由此可知，grpc原生共提供了两种LoadBalancerProvider，那看看它有关核心方法的实现： 首先是PickFirstLoadBalancerProvider： 代码块112345678910111213141516171819202122232425262728public final class PickFirstLoadBalancerProvider extends LoadBalancerProvider { private static final String NO_CONFIG = \"no service config\"; @Override public boolean isAvailable() { //是否有效（若置为false，即便你在grpc client指定了该lb算法，那么它也不会生效） return true; } @Override public int getPriority() { //优先级 return 5; } @Override public String getPolicyName() { //lb算法的名称 return \"pick_first\"; } @Override public LoadBalancer newLoadBalancer(LoadBalancer.Helper helper) { //提供对应的LoadBalancer对象，可以看到，这个实现类为PickFirstLoadBalancer return new PickFirstLoadBalancer(helper); } @Override public ConfigOrError parseLoadBalancingPolicyConfig(Map&lt;String, ?&gt; rawLoadBalancingPolicyConfig) { return ConfigOrError.fromConfig(NO_CONFIG); }} 其次是SecretRoundRobinLoadBalancerProvider.Provider： 代码块2123456789101112131415161718192021222324252627282930313233343536final class SecretRoundRobinLoadBalancerProvider { private SecretRoundRobinLoadBalancerProvider() { } public static final class Provider extends LoadBalancerProvider { private static final String NO_CONFIG = \"no service config\"; @Override public boolean isAvailable() { return true; } @Override public int getPriority() { return 5; } @Override public String getPolicyName() { //这里则是返回round robin的lb算法名称 return \"round_robin\"; } @Override public LoadBalancer newLoadBalancer(LoadBalancer.Helper helper) { //提供对应的LoadBalancer对象，可以看到，这个实现类为RoundRobinLoadBalancer return new RoundRobinLoadBalancer(helper); } @Override public ConfigOrError parseLoadBalancingPolicyConfig( Map&lt;String, ?&gt; rawLoadBalancingPolicyConfig) { return ConfigOrError.fromConfig(NO_CONFIG); } }} 有了不同的LoadBalancerProvider类去实例化不同的LB算法，就可以扩展并指定不同的LB算法了，我们也可以效仿这种做法，来搞一套我们自己的LB算法做替换。 1.2：提供LoadBalancer的实现上面是LB算法的实例提供方，那必定存在LB算法的实现方，上面两个Provider分别提供了一个对应的LB算法实现类，即：RoundRobinLoadBalancer &amp; PickFirstLoadBalancer 因为我们使用的是轮询，因此只说明下RoundRobinLoadBalancer即可。 因为源代码过于复杂，这里仅展示出关键步骤（感兴趣可以直接查看io.grpc.util.RoundRobinLoadBalancer，嫌看代码麻烦，可以调到下方图2，快速浏览其过程）： 代码块3123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151public class RoundRobinLoadBalancer extends LoadBalancer { @VisibleForTesting static final Attributes.Key&lt;Ref&lt;ConnectivityStateInfo&gt;&gt; STATE_INFO = Attributes.Key.create(\"state-info\"); private static final Status EMPTY_OK = Status.OK.withDescription(\"no subchannels ready\"); private final Helper helper; //负责创建channel、将picker传出去 private final Map&lt;EquivalentAddressGroup, Subchannel&gt; subchannels = new HashMap&lt;&gt;(); //保存channel用的 private ConnectivityState currentState; //最近一次建连的连接状态 private WeightRoundRobinPicker currentPicker = new EmptyPicker(EMPTY_OK); //最近一次建连后的picker对象，正常情况下会被刷成ReadyPicker public WeightRoundRobinLoadBalancer(Helper helper) { this.helper = checkNotNull(helper, \"helper\"); } /** * 刷新节点，每当服务发现更新了节点，都会通知到该方法，用来更新现有参与LB的节点信息 * * @param resolvedAddresses 新节点集合 */ @Override public void handleResolvedAddresses(ResolvedAddresses resolvedAddresses) { List&lt;EquivalentAddressGroup&gt; servers = resolvedAddresses.getAddresses(); Set&lt;EquivalentAddressGroup&gt; currentAddrs = subchannels.keySet(); //当前节点集合 Map&lt;EquivalentAddressGroup, EquivalentAddressGroup&gt; latestAddrs = stripAttrs(servers); //最新节点集合 //因为是更新address，所以这里需要过滤出来旧版需要被移除的部分 Set&lt;EquivalentAddressGroup&gt; removedAddrs = setsDifference(currentAddrs, latestAddrs.keySet()); //遍历新节点 for (Map.Entry&lt;EquivalentAddressGroup, EquivalentAddressGroup&gt; latestEntry : latestAddrs.entrySet()) { EquivalentAddressGroup strippedAddressGroup = latestEntry.getKey(); //新节点的节点信息（参考stripAttrs方法） EquivalentAddressGroup originalAddressGroup = latestEntry.getValue(); //新节点的完整信息 Subchannel existingSubchannel = subchannels.get(strippedAddressGroup); //尝试在老节点查找新入的节点 if (existingSubchannel != null) { //若老节点原本就存在该节点，则触发下方逻辑 // 新拉取到的节点，它的Attributes可能被改变（附带信息，比如服务发现上节点的权重值前后发生变化），这里需要刷新 existingSubchannel.updateAddresses(Collections.singletonList(originalAddressGroup)); continue; } //若老节点不存在，则说明需要new一个新的subchannel Attributes.Builder subchannelAttrs = Attributes.newBuilder().set(STATE_INFO, new Ref&lt;&gt;(ConnectivityStateInfo.forNonError(IDLE))); //设置节点初始状态，通过Ref可以修改状态引用 final Subchannel subchannel = checkNotNull( helper.createSubchannel(CreateSubchannelArgs.newBuilder() .setAddresses(originalAddressGroup) .setAttributes(subchannelAttrs.build()) .build()), \"subchannel\"); subchannel.start(new SubchannelStateListener() { //异步建连，收到建连成功的通知后，触发onSubchannelState @Override public void onSubchannelState(ConnectivityStateInfo state) { processSubchannelState(subchannel, state); //建连完成，更新连接状态为可用（若建连没问题，则state应为READY，即就绪状态） } }); subchannels.put(strippedAddressGroup, subchannel); //将连接放进集合里 subchannel.requestConnection(); //触发建连的动作（建连动作处理完毕后，会通知上方onSubchannelState） } ArrayList&lt;Subchannel&gt; removedSubchannels = new ArrayList&lt;&gt;(); for (EquivalentAddressGroup addressGroup : removedAddrs) { //去除掉需要摘除的节点 removedSubchannels.add(subchannels.remove(addressGroup)); } // 更新loadbalance状态 updateBalancingState(); for (Subchannel removedSubchannel : removedSubchannels) { //将需要摘除掉的节点逐个shutdown shutdownSubchannel(removedSubchannel); } } @Override public void handleNameResolutionError(Status error) { updateBalancingState(TRANSIENT_FAILURE, currentPicker instanceof ReadyPicker ? currentPicker : new EmptyPicker(error)); } @Override public void shutdown() { for (Subchannel subchannel : getSubchannels()) { shutdownSubchannel(subchannel); //逐个关闭所持有的连接 } } //这里传入的是服务发现推给的节点集合 private static Map&lt;EquivalentAddressGroup, EquivalentAddressGroup&gt; stripAttrs(List&lt;EquivalentAddressGroup&gt; groupList) { Map&lt;EquivalentAddressGroup, EquivalentAddressGroup&gt; addrs = new HashMap&lt;&gt;(groupList.size() * 2); //为什么要扩2倍？ for (EquivalentAddressGroup group : groupList) { //一个map，k是包含了相同address的一个全新EquivalentAddressGroup对象，v是原始的EquivalentAddressGroup对象 addrs.put(stripAttrs(group), group); } return addrs; } private static EquivalentAddressGroup stripAttrs(EquivalentAddressGroup eag) { return new EquivalentAddressGroup(eag.getAddresses()); } //用来更新连接状态 private void processSubchannelState(Subchannel subchannel, ConnectivityStateInfo stateInfo) { if (subchannels.get(stripAttrs(subchannel.getAddresses())) != subchannel) { return; } if (stateInfo.getState() == IDLE) { subchannel.requestConnection(); //如果是闲置状态，则触发建连操作 } //拿到当前channel的state Ref&lt;ConnectivityStateInfo&gt; subchannelStateRef = getSubchannelStateInfoRef(subchannel); if (subchannelStateRef.value.getState().equals(TRANSIENT_FAILURE)) { if (stateInfo.getState().equals(CONNECTING) || stateInfo.getState().equals(IDLE)) { return; //若连接处于故障状态，则不允许改成\"连接中\"和\"闲置\"状态 } } subchannelStateRef.value = stateInfo; //修改状态 updateBalancingState(); //更新loadbalance状态 } //每次建连事件被异步触发后，都会触发一次该方法，用来刷新picker private void updateBalancingState() { List&lt;Subchannel&gt; activeList = filterNonFailingSubchannels(getSubchannels()); //将当前就绪状态的channel过滤出来 if (activeList.isEmpty()) { //建连触发才会走到这里，因此只要建连没出问题，一般情况下都不为空，除非这批节点没一个可用的 boolean isConnecting = false; Status aggStatus = EMPTY_OK; for (Subchannel subchannel : getSubchannels()) { ConnectivityStateInfo stateInfo = getSubchannelStateInfoRef(subchannel).value; if (stateInfo.getState() == CONNECTING || stateInfo.getState() == IDLE) { isConnecting = true; } if (aggStatus == EMPTY_OK || !aggStatus.isOk()) { aggStatus = stateInfo.getStatus(); } } updateBalancingState(isConnecting ? CONNECTING : TRANSIENT_FAILURE, new EmptyPicker(aggStatus)); } else { updateBalancingState(READY, new ReadyPicker(activeList)); //可以看到，最后会将建连成功的节点丢到ReadyPicker里，用来做lb } } //更新picker的值 private void updateBalancingState(ConnectivityState state, WeightRoundRobinPicker picker) { if (state != currentState || !picker.isEquivalentTo(currentPicker)) { helper.updateBalancingState(state, picker); //非常重要的一步，负责将当前的picker送出去，给ManagedChannelImpl使用，client请求时会触发picker的pickSubchannel方法 currentState = state; //刷新连接状态 currentPicker = picker; //刷新picker对象 } }} 流程简图（不同方法已用不同颜色标出）： 1.3：提供LoadBalancer.SubchannelPicker实现上面说了那么多，都只是在为真正的LB做准备，实际的LB算法保存在Picker类里，我们来看下上文中出现的ReadyPicker的主要方法实现： 代码块412345678910111213141516171819202122232425262728293031323334353637383940static final class ReadyPicker extends RoundRobinPicker { //RoundRobinPicker继承了LoadBalancer.SubchannelPicker private static final AtomicIntegerFieldUpdater&lt;ReadyPicker&gt; indexUpdater = AtomicIntegerFieldUpdater.newUpdater(ReadyPicker.class, \"index\"); private final List&lt;Subchannel&gt; list; // 这里保存的就是当前已就绪的channel（结合图2里ReadyPicker的初始化理解） @SuppressWarnings(\"unused\") private volatile int index; ReadyPicker(List&lt;Subchannel&gt; list, int startIndex) { Preconditions.checkArgument(!list.isEmpty(), \"empty list\"); this.list = list; this.index = startIndex - 1; //轮询算法开始的位置 } @Override public PickResult pickSubchannel(PickSubchannelArgs args) { //实现了pickSubchannel方法，该方法就是对外pick节点的核心方法 return PickResult.withSubchannel(nextSubchannel()); //nextSubchannel里放的就是轮询算法的核心代码了 } private Subchannel nextSubchannel() { //轮询算法 int size = list.size(); int i = indexUpdater.incrementAndGet(this); if (i &gt;= size) { int oldi = i; i %= size; indexUpdater.compareAndSet(this, oldi, i); } return list.get(i); } @Override boolean isEquivalentTo(RoundRobinPicker picker) { if (!(picker instanceof ReadyPicker)) { return false; } ReadyPicker other = (ReadyPicker) picker; return other == this || (list.size() == other.list.size() &amp;&amp; new HashSet&lt;&gt;(list).containsAll(other.list)); }} 上面就是LB的核心算法，重点是pickSubchannel方法，它是LB算法的触发类。 1.4：grpc client指定LB算法正常一个client channel的创建方式如下： 代码块512345678910public static void main(String[] args) { ChannelBuilder builder = ChannelBuilder.forTarget(\"这里填服务的discovery_id\") //这个方法用来设置一个继承了io.grpc.NameResolverProvider的服务发现，可以定制 .nameResolverFactory(new RPCNamingClientNameResolverFactory(zone, resolver, cluster)) .disableRetry() //禁止重试 //这里就是用来启用对应的LB模块了，还记得xxxProvider里的getPolicyName方法吗？这里跟那里面返回的名称匹配，匹配后即可启用对应的LB服务 .defaultLoadBalancingPolicy(\"round_robin\"); ManagedChannel channel = builder.build(); //这样一个grpc的channel client就被创建出来了 } 可以看到grpc client通过defaultLoadBalancingPolicy方法利用LB的名称指定了一个默认负载均衡器。 二、在grpc里定制自己的LB算法经过对第一部分的理解，想要在grpc里定制自己的LB算法就变得简单多了，只需要以下几步： 定义一个继承了LoadBalancer.SubchannelPicker类的XXXPicker，然后通过实现pickSubchannel方法实现自己的LB逻辑 定义一个继承了LoadBalancer的XXXLoadBalancer，用来管理连接以及提供对应的Picker对象 定义一个继承了LoadBalancerProvider的Provider，然后将其按照SPI规范放到自己项目的META-INF下，通过newLoadBalancer方法提供对应的LoadBalancer对象","link":"/2020/08/06/%E5%AE%9A%E5%88%B6grpc%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%99%A8/"},{"title":"详解JDBC的Loadbalance模式","text":"基于依赖程序的版本信息： 一、认识loadbalance模式首先回忆下jdbc协议头都有哪些，下面总结下： 通过表1，可以知道在loadblance模式下允许配置多个mysql节点信息，而我们每次建连时，驱动程序就会按照配置的节点，选中一个，然后完成连接的创建，下面我们来探索下它的实现。 二、基本使用它的基本用法跟其他模式没有区别： 代码块11234567891011121314151617public static void main(String[] args) throws Exception { String url = \"jdbc:mysql:loadbalance://127.0.0.1:3306,127.0.0.2:3306,127.0.0.3:3306/mydb\"; LoadBalancedConnection connection = (LoadBalancedConnection) DriverManager.getConnection(url, \"root\", \"123456\"); Statement statement = connection.createStatement(); ResultSet rs = null; try { if (statement.execute(\"select * from t_season\")) { rs = statement.getResultSet(); } } finally { if (rs != null) { rs.close(); } statement.close(); connection.close(); }} 三、驱动加载流程jdbc是如何知道我们启用了loadbalance模式的？先来了解下DriverManager的getConnection方法，注意这里的DriverManager在java.sql包内，它属于jdk自带的类，目的是扫描所有实现了java.sql.Driver的类，而我们所使用的mysql-connector-java程序就实现了Driver接口，所以很容易被DriverManager载入，下面来看它是如何完成驱动程序扫描与加载的： 代码块21234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798@CallerSensitivepublic static Connection getConnection(String url, String user, String password) throws SQLException { java.util.Properties info = new java.util.Properties(); if (user != null) { info.put(\"user\", user); } if (password != null) { info.put(\"password\", password); } //说明：Reflection.getCallerClass()是个本地方法，会返回调用当前这个方法的那个类的名字（后续我们称其为callerClass） return (getConnection(url, info, Reflection.getCallerClass()));} private static Connection getConnection( String url, java.util.Properties info, Class&lt;?&gt; caller) throws SQLException { //获取callerClass的类加载器 ClassLoader callerCL = caller != null ? caller.getClassLoader() : null; if (callerCL == null) { //若找不到对应的类加载器，则默认为当前线程的类加载器 callerCL = Thread.currentThread().getContextClassLoader(); } if (url == null) { throw new SQLException(\"The url cannot be null\", \"08001\"); } println(\"DriverManager.getConnection(\\\"\" + url + \"\\\")\"); //这个方法会通过SPI机制加载可以加载的实现了JDBC协议的驱动程序，我们通常用的是mysql-connector-java里的驱动类，某些连接池技术也会搞一个自己的驱动类（比如Druid的DruidDriver） //在里面会完成Driver实现类的类加载，而驱动程序只需要在静态块里将自己的实例new出来，注册到DriverManager里即可（可以去mysql-connector-java的Driver类里确认是否有该逻辑） //所以现在我们根本不需要跟以前写jdbc程序那样写一次Class.forName的代码，这个方法已经帮我们做了（参考图1，驱动程序已经满足SPI加载配置的条件） ensureDriversInitialized(); // Walk through the loaded registeredDrivers attempting to make a connection. // Remember the first exception that gets raised so we can reraise it. SQLException reason = null; for (DriverInfo aDriver : registeredDrivers) { //循环已经成功加载到的驱动实现 // If the caller does not have permission to load the driver then // skip it. if (isDriverAllowed(aDriver.driver, callerCL)) { //这个解释在下面对应的方法里，还是挺有意思的一个方法 try { println(\" trying \" + aDriver.driver.getClass().getName()); //这里是利用驱动程序获取到一个Connection对象，后面会详细讲 Connection con = aDriver.driver.connect(url, info); if (con != null) { // Success! println(\"getConnection returning \" + aDriver.driver.getClass().getName()); return (con); } } catch (SQLException ex) { if (reason == null) { reason = ex; } } } else { println(\" skipping: \" + aDriver.getClass().getName()); } } // if we got here nobody could connect. if (reason != null) { println(\"getConnection failed: \" + reason); throw reason; } println(\"getConnection: no suitable driver found for \"+ url); throw new SQLException(\"No suitable driver found for \"+ url, \"08001\");} private static boolean isDriverAllowed(Driver driver, Class&lt;?&gt; caller) { ClassLoader callerCL = caller != null ? caller.getClassLoader() : null; return isDriverAllowed(driver, callerCL);} //该方法主要用来做驱动加载，以及判断加载了驱动Driver对象的类加载器跟callerClass的类加载器是否一致，若一致才返回true，反之为falseprivate static boolean isDriverAllowed(Driver driver, ClassLoader classLoader) { boolean result = false; if (driver != null) { Class&lt;?&gt; aClass = null; try { //这里算是给驱动类调整了类加载器，将第一次进行类加载时加载到的Class对象里的类加载器统一成callerClass的 aClass = Class.forName(driver.getClass().getName(), true, classLoader); } catch (Exception ex) { result = false; } //这里会再确认一次driver对象此时对应的Class是否跟被调整了类加载器的Class一致（如果不出意外，这里应该是一致的） result = ( aClass == driver.getClass() ) ? true : false; } return result;} 驱动程序的SPI支持： 通过上述代码，可以确认最终是通过驱动程序Driver实现类的connect方法产生的Connection对象，下面来看下驱动程序Driver里的实现： 代码块312345678910111213141516171819202122232425262728293031323334353637383940414243//代码所属类：com.mysql.cj.jdbc.NonRegisteringDriver@Overridepublic java.sql.Connection connect(String url, Properties info) throws SQLException { try { //验证传入的url是否符合jdbc连接规范 if (!ConnectionUrl.acceptsUrl(url)) { /* * According to JDBC spec: * The driver should return \"null\" if it realizes it is the wrong kind of driver to connect to the given URL. This will be common, as when the * JDBC driver manager is asked to connect to a given URL it passes the URL to each loaded driver in turn. */ return null; } //根据传入的信息，获得一个包装了连接信息的对象 ConnectionUrl conStr = ConnectionUrl.getConnectionUrlInstance(url, info); switch (conStr.getType()) { //这个Type是根据jdbc协议头分析出来的，应该给一个对应协议头类型的Connection实例 case SINGLE_CONNECTION: //jdbc:mysql:开头的url会命中下方逻辑 return com.mysql.cj.jdbc.ConnectionImpl.getInstance(conStr.getMainHost()); case LOADBALANCE_CONNECTION: //jdbc:mysql:loadbalance:开头的url会命中下方逻辑（负载均衡），也是本节要讲的重点 return LoadBalancedConnectionProxy.createProxyInstance((LoadbalanceConnectionUrl) conStr); case FAILOVER_CONNECTION: //jdbc:mysql:开头且配置了多个节点的情况会命中下方逻辑（故障转移） return FailoverConnectionProxy.createProxyInstance(conStr); case REPLICATION_CONNECTION: //jdbc:mysql:replication:开头的url会命中下方逻辑（主从） return ReplicationConnectionProxy.createProxyInstance((ReplicationConnectionUrl) conStr); default: return null; } } catch (UnsupportedConnectionStringException e) { // when Connector/J can't handle this connection string the Driver must return null return null; } catch (CJException ex) { throw ExceptionFactory.createException(UnableToConnectException.class, Messages.getString(\"NonRegisteringDriver.17\", new Object[] { ex.toString() }), ex); }} 到这里，可以看到驱动程序之所以会知道我们启用了loadbalance模式，是因为我们所配置的jdbc连接协议头，根据协议头的不同，会被路由进不同的Connection实现，然后最终将Connection对象返回给用户。 四、驱动程序对LoadBalance的支持下面重点看下命中LOADBALANCE_CONNECTION条件的LoadBalancedConnectionProxy.createProxyInstance的内部逻辑： 代码块4123456//创建LoadBalancedConnection对象，并为其加上动态代理public static LoadBalancedConnection createProxyInstance(LoadbalanceConnectionUrl connectionUrl) throws SQLException { LoadBalancedConnectionProxy connProxy = new LoadBalancedConnectionProxy(connectionUrl); //返回的是一个被LoadBalancedConnectionProxy代理了的LoadBalancedConnection对象 return (LoadBalancedConnection) java.lang.reflect.Proxy.newProxyInstance(LoadBalancedConnection.class.getClassLoader(), INTERFACES_TO_PROXY, connProxy);} 4.1：LoadBalance模式相关的类关系图到这里为止，我们已经进入了loadblance模式内部，这里返回的是一个被代理了的LoadBalancedConnection对象，下面来梳理下它们的继承和代理关系（之后重点分析的字段和方法字体均已标红）： 理清关系后，来看下最主要的几个属性和方法的实现。 代码块4里直接new出了LoadBalancedConnectionProxy类，并且代理的目标类为LoadBalancedConnection，通过上图，可以知道就是最终返回给用户的那个Connection对象，意味着用户拿着这个Connection做任何操作都会触发LoadBalancedConnectionProxy的invokeMore方法（通过图中展示，其父类实现了InvocationHandler接口，其invoke会触发invokeMore方法，而invokeMore方法的实现在LoadBalancedConnectionProxy里） 4.2：LoadBalancedConnectionProxy.balancer属性这是个属性，它包装了一个Balancer对象，内部有自己的LB算法，它的初始化如下： 代码块4123456789101112131415161718192021//拿到指定的LB算法，不配置的话默认是random，如果你想要指定，可以在jdbc连接后面追加haLoadBalanceStrategy参数，让其等于你指定的LB算法类型即可//LB算法类型可选值在下面的switch内部，当然，你也可以自定义，自定义的话就需要传实现类的路径给这个参数了String strategy = props.getProperty(PropertyKey.ha_loadBalanceStrategy.getKeyName(), \"random\"); try { switch (strategy) { case \"random\": this.balancer = new RandomBalanceStrategy(); //random算法的实现类，默认算法 break; case \"bestResponseTime\": this.balancer = new BestResponseTimeBalanceStrategy(); break; case \"serverAffinity\": this.balancer = new ServerAffinityStrategy(props.getProperty(PropertyKey.serverAffinityOrder.getKeyName(), null)); break; default: //你可以按需自定义LB算法，这里是通过反射的方式初始化你给定的LB算法类的 this.balancer = (BalanceStrategy) Class.forName(strategy).newInstance(); }} catch (Throwable t) { throw SQLError.createSQLException(Messages.getString(\"InvalidLoadBalanceStrategy\", new Object[] { strategy }), MysqlErrorNumbers.SQL_STATE_ILLEGAL_ARGUMENT, t, null);} 我们只关注Random即可，它的内部实现就是简单的从LoadBalancedConnectionProxy.liveConnections里随机选一个节点，然后返回出去，为了更加清晰，不再贴代码，大致流程如下（绿色框逻辑都属于RandomBalancer本身的逻辑，除此之外，图中标注了hostList属性的数据来源）： 它的触发点就是在LoadBalancedConnectionProxy.pickNewConnection方法（参考下方4.3），即发生在选取节点时。 4.3：LoadBalancedConnectionProxy.pickNewConnection方法这个方法是非常核心的功能，每次变换节点时都会触发的一个方法，下面来看下其内部逻辑： 代码块512345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455//pick节点核心方法，利用balancer对象刷新currentConnection的值public synchronized void pickNewConnection() throws SQLException { if (this.isClosed &amp;&amp; this.closedExplicitly) { return; } List&lt;String&gt; hostPortList = Collections.unmodifiableList(this.hostsList.stream().map(hi -&gt; hi.getHostPortPair()).collect(Collectors.toList())); if (this.currentConnection == null) { // startup //如果currentConnection为空，则开始利用balancer选取节点 this.currentConnection = this.balancer.pickConnection(this, hostPortList, Collections.unmodifiableMap(this.liveConnections), this.responseTimes.clone(), this.retriesAllDown); return; //终止 } if (this.currentConnection.isClosed()) { invalidateCurrentConnection(); //若发现当前连接已经被关闭了，则抛弃这个连接 } int pingTimeout = this.currentConnection.getPropertySet().getIntegerProperty(PropertyKey.loadBalancePingTimeout).getValue(); boolean pingBeforeReturn = this.currentConnection.getPropertySet().getBooleanProperty(PropertyKey.loadBalanceValidateConnectionOnSwapServer).getValue(); //重试逻辑，若pick不成功，则在有限的次数内重试（这个次数就是hostsList的size） for (int hostsTried = 0, hostsToTry = this.hostsList.size(); hostsTried &lt; hostsToTry; hostsTried++) { ConnectionImpl newConn = null; try { //pick节点 newConn = (ConnectionImpl) this.balancer.pickConnection(this, hostPortList, Collections.unmodifiableMap(this.liveConnections), this.responseTimes.clone(), this.retriesAllDown); if (this.currentConnection != null) { if (pingBeforeReturn) { //ping检查，检查失败会抛SQLException，下方异常处理里会把它抛弃掉 newConn.pingInternal(true, pingTimeout); } //同步旧currentConnection节点的属性给这个新pick出来的节点，比如read-only、auto-commit什么的 syncSessionState(this.currentConnection, newConn); } //刷新currentConnection的值为新pick出来的这个连接 this.currentConnection = newConn; return; //终止 } catch (SQLException e) { if (shouldExceptionTriggerConnectionSwitch(e) &amp;&amp; newConn != null) { // connection error, close up shop on current connection invalidateConnection(newConn); } } } // no hosts available to swap connection to, close up. this.isClosed = true; //如果将hostsList集合pick了一遍都没有找到可用的连接，则认为pick失败，标记isClosed为true this.closedReason = \"Connection closed after inability to pick valid new connection during load-balance.\";} 4.4：MultiHostConnectionProxy.invoke方法这是MultiHostConnectionProxy对InvocationHandler接口的实现，通过图2可以知道，它的子类LoadBalancedConnectionProxy是返回给用户的LoadBalancedConnection的代理类，意味着用户利用Connection做的每一步操作，都会命中这个invoke方法的调用，下面来看下这个方法的实现： 代码块6123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566public synchronized Object invoke(Object proxy, Method method, Object[] args) throws Throwable { String methodName = method.getName(); //若被调用的方法是getMultiHostSafeProxy，则直接返回代理对象本身（也即是用户正在使用的那个Connection对象） if (METHOD_GET_MULTI_HOST_SAFE_PROXY.equals(methodName)) { return this.thisAsConnection; } //被调用的方法是equals所执行的逻辑 if (METHOD_EQUALS.equals(methodName)) { // Let args[0] \"unwrap\" to its InvocationHandler if it is a proxy. return args[0].equals(this); } //被调用的方法是hashCode所执行的逻辑 if (METHOD_HASH_CODE.equals(methodName)) { return this.hashCode(); } //若被调用的方法是close，则关闭并清理掉所有的连接（liveConnections.clear） if (METHOD_CLOSE.equals(methodName)) { doClose(); this.isClosed = true; //标记isClosed为true this.closedReason = \"Connection explicitly closed.\"; this.closedExplicitly = true; //标记closedExplicitly为true，意思是说这是由用户\"显式关闭\"的 return null; } //被调用的方法是abortInternal所执行的逻辑 if (METHOD_ABORT_INTERNAL.equals(methodName)) { doAbortInternal(); this.currentConnection.abortInternal(); this.isClosed = true; this.closedReason = \"Connection explicitly closed.\"; return null; } //被调用的方法是abort所执行的逻辑 if (METHOD_ABORT.equals(methodName) &amp;&amp; args.length == 1) { doAbort((Executor) args[0]); this.isClosed = true; this.closedReason = \"Connection explicitly closed.\"; return null; } //被调用的方法是isClosed所执行的逻辑 if (METHOD_IS_CLOSED.equals(methodName)) { return this.isClosed; } try { //若调用的方法不是上面的任意一种，则直接触发其子类的invokeMore方法（下面分析） return invokeMore(proxy, method, args); } catch (InvocationTargetException e) { throw e.getCause() != null ? e.getCause() : e; } catch (Exception e) { // Check if the captured exception must be wrapped by an unchecked exception. Class&lt;?&gt;[] declaredException = method.getExceptionTypes(); for (Class&lt;?&gt; declEx : declaredException) { if (declEx.isAssignableFrom(e.getClass())) { throw e; } } throw new IllegalStateException(e.getMessage(), e); }} 4.5：LoadBalancedConnectionProxy.invokeMore方法紧接着上面的代码来看，了解下invokMore方法（根据图2可知，此方法为抽象方法，由子类实现，所以它的实现逻辑在LoadBalancedConnectionProxy里） 代码块7123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778@Overridepublic synchronized Object invokeMore(Object proxy, Method method, Object[] args) throws Throwable { String methodName = method.getName(); //重连机制判断，如果当前连接状态已被关闭，这里的关闭是指currentConnection的isClosed为true，而使isClosed为true的地方为： //① 用户手动调用Connection.close，这种被称为\"显式关闭\"，这种关闭方式连同closedExplicitly也会被置为true //② abort连接、ping检查失败、pickNewConnection时pick不出可用节点，都会使isClosed为true，但closedExplicitly依然为false if (this.isClosed &amp;&amp; !allowedOnClosedConnection(method) &amp;&amp; method.getExceptionTypes().length &gt; 0) { // TODO remove method.getExceptionTypes().length ? //结合上方说的②，如果你设置了autoReconnect机制（自动重连），那么就可以在任意\"非显式\"close的情况下，刷新currentConnection的值，使其可用 if (this.autoReconnect &amp;&amp; !this.closedExplicitly) { this.currentConnection = null; pickNewConnection(); //在自动重连开启的情况下，当你的连接被非正常关闭后，会尝试重新pick节点，确保其可用 this.isClosed = false; this.closedReason = null; } else { //如果没有开启重连模式，那么在isClose为true时，就直接抛出错误 String reason = \"No operations allowed after connection closed.\"; if (this.closedReason != null) { reason += \" \" + this.closedReason; } for (Class&lt;?&gt; excls : method.getExceptionTypes()) { if (SQLException.class.isAssignableFrom(excls)) { throw SQLError.createSQLException(reason, MysqlErrorNumbers.SQL_STATE_CONNECTION_NOT_OPEN, null /* no access to an interceptor here... */); } } throw ExceptionFactory.createException(CJCommunicationsException.class, reason); } } if (!this.inTransaction) { this.inTransaction = true; this.transactionStartTime = System.nanoTime(); this.transactionCount++; } Object result = null; try { //触发实际的方法逻辑 result = method.invoke(this.thisAsConnection, args); if (result != null) { if (result instanceof com.mysql.cj.jdbc.JdbcStatement) { ((com.mysql.cj.jdbc.JdbcStatement) result).setPingTarget(this); } //这里是给方法返回的对象加一层代理（跟本次重点无关，不再详述） result = proxyIfReturnTypeIsJdbcInterface(method.getReturnType(), result); } } catch (InvocationTargetException e) { dealWithInvocationException(e); } finally { //重点是，当发现触发的方法是commit或rollback时，会刷新一下currentConnection的值，重新pick出一个 if (\"commit\".equals(methodName) || \"rollback\".equals(methodName)) { this.inTransaction = false; // Update stats String host = this.connectionsToHostsMap.get(this.currentConnection); // avoid NPE if the connection has already been removed from connectionsToHostsMap in invalidateCurrenctConnection() if (host != null) { synchronized (this.responseTimes) { Integer hostIndex = (this.hostsToListIndexMap.get(host)); if (hostIndex != null &amp;&amp; hostIndex &lt; this.responseTimes.length) { //更新对应节点执行事务所花费的时间 this.responseTimes[hostIndex] = System.nanoTime() - this.transactionStartTime; } } } //刷新currentConnection的值 pickNewConnection(); } } return result;} 通过这块代码可以知道，一个LoadBalance模式下的连接被创建出来后，除非是commit或rollback事务，否则该Connection对象里的currentConnection永远都不会变，当然，通过上述代码看，还有一种情况是会变的，那就是当前连接坏掉，然后ping检查失败isClose被标记为true，你的配置里恰好又开启了autoReconnect，这时才会重新pick新的节点。 五、猜想通过对其代码实现的分析，可以得出如下猜想： 不配autoReconnect的情况下，只有在利用该连接对象提交、回滚事务时才会pick新的节点。 配置autoReconnect的情况下，在节点坏掉后，会pick一次节点，事务提交、回滚一样会pick节点。 综上，如果我要实现一个select查询也需要pick节点实现负载均衡的情况下，不可以用单例Connection，因为普通select并不会触发pick操作。 综合3，想要实现全部意义的LB，必须要使用多实例模式，这样虽然实现了我想要的LB效果，但代价是巨大的，因为每次都会建连。 利用连接池可以一定程度上解决这种问题，连接池可以预先建连一堆Connection对象，这些对象如果在创建时启用jdbc LoadBalance模式，那么意味着每个连接都是随机节点。 六、验证为了验证第五节的结论，我们简单做个试验验证下。 6.1：单例模式下的Query操作代码块812345678910111213141516171819202122232425262728293031323334public class Test { public static void main(String[] args) throws Exception { String url = \"jdbc:mysql:loadbalance://172.22.119.38:4000,172.22.119.8:4000,172.22.119.30:4000/tidb_test\"; LoadBalancedConnection connection = (LoadBalancedConnection) DriverManager.getConnection(url, \"tidb_test\", \"lPoK3QMSWY1BhSa3WCT1IWOXYkMc3Aqd\"); Test test = new Test(); for (int i = 0; i &lt; 100; i++) { //利用同一个connection对象执行100次查询操作 try { test.triggerQuery(connection); } finally { //为了保证是单例模式，这里不再close //connection.close(); } } } private void triggerQuery(LoadBalancedConnection connection) throws Exception { Statement statement = connection.createStatement(); ResultSet rs = null; try { //这里打印下当前参与执行sql的连接host System.out.println(\"current conn host: \" + connection.getHost()); if (statement.execute(\"select * from t_student where id = 1\")) { rs = statement.getResultSet(); } } finally { if (rs != null) { rs.close(); } statement.close(); } }} 这段代码运行结果打印如下： 代码块912345678current conn host: 172.22.119.30current conn host: 172.22.119.30current conn host: 172.22.119.30current conn host: 172.22.119.30current conn host: 172.22.119.30current conn host: 172.22.119.30current conn host: 172.22.119.30...省略，共100条... 可以看到，如果一直用同一个Connection对象去createStatement，然后执行Query，那么节点始终是一开始pick好的那个，且永远不会变。 6.2：多实例下的Query操作这个其实根本没有测试的必要，多实例意味着每次Query前都会新建一个连接对象，新建一个意味着会pick一次，那肯定是random的，我们改下main方法的代码： 代码块10123456789101112131415public static void main(String[] args) throws Exception { String url = \"jdbc:mysql:loadbalance://172.22.119.38:4000,172.22.119.8:4000,172.22.119.30:4000/tidb_test\"; Test test = new Test(); for (int i = 0; i &lt; 100; i++) { //将创建connection连接放到循环体里面，使每次传给triggerQuery方法的都是不同的Connection对象 LoadBalancedConnection connection = (LoadBalancedConnection) DriverManager.getConnection(url, \"tidb_test\", \"lPoK3QMSWY1BhSa3WCT1IWOXYkMc3Aqd\"); try { test.triggerQuery(connection); } finally { //别忘了close资源 connection.close(); } }} 这次运行结果如下： 代码块1112345678910current conn host: 172.22.119.8current conn host: 172.22.119.8current conn host: 172.22.119.8current conn host: 172.22.119.38current conn host: 172.22.119.30current conn host: 172.22.119.38current conn host: 172.22.119.8current conn host: 172.22.119.8current conn host: 172.22.119.38...省略，共100条... 可以看到，已经触发了LB算法了（这是预料之中的）。 你可能会问，如果调用了close呢？会不会close不是真的close，而是触发pick呢？正常情况下肯定会这样实现的吧？其实并不会，close的逻辑是把里面所有的liveConnections清空，然后close一遍，所以下面的代码会报错： 代码块121234567891011121314public static void main(String[] args) throws Exception { String url = \"jdbc:mysql:loadbalance://172.22.119.38:4000,172.22.119.8:4000,172.22.119.30:4000/tidb_test\"; //仍然是单例的Connection对象 LoadBalancedConnection connection = (LoadBalancedConnection) DriverManager.getConnection(url, \"tidb_test\", \"lPoK3QMSWY1BhSa3WCT1IWOXYkMc3Aqd\"); Test test = new Test(); for (int i = 0; i &lt; 100; i++) { try { test.triggerQuery(connection); } finally { connection.close(); //这里close掉 } }} 执行结果如下： 是的。。它直接报错了，也就是说，在select这种语句的执行下，要么你用多实例，每次都建连，实现你心目中的LB，要么你就用单例，然后一个连接用到底，这到底算哪门子的LoadBalance啊。。😓 但是它并非一无是处，比如，你可以结合连接池来用它，这样既可以保证连接复用，也可以保证池内每个连接对象的host在最大限度上不是同一个。 6.3：单例模式下的事务操作按照源码上的理解，理论上即便是单例，开启事务并提交的时候也会切换一次host，现在将前面的测试代码改成下面这样： 代码块131234567891011121314151617181920212223242526272829303132333435public static void main(String[] args) throws Exception { String url = \"jdbc:mysql:loadbalance://172.22.119.38:4000,172.22.119.8:4000,172.22.119.30:4000/tidb_test\"; //仍然是单例的Connection对象 LoadBalancedConnection connection = (LoadBalancedConnection) DriverManager.getConnection(url, \"tidb_test\", \"lPoK3QMSWY1BhSa3WCT1IWOXYkMc3Aqd\"); Test test = new Test(); for (int i = 0; i &lt; 100; i++) { try { test.triggerTransaction(connection); } finally { //为了保证是单例模式，这里不再close //connection.close(); } }} //开启事务的方法private void triggerTransaction(LoadBalancedConnection connection) throws Exception { connection.setAutoCommit(false); //关闭自动提交 Statement statement = connection.createStatement(); ResultSet rs = null; try { //这里打印下当前参与执行sql的连接host System.out.println(\"current conn host: \" + connection.getHost()); if (statement.execute(\"select * from t_student where id = 1\")) { rs = statement.getResultSet(); } connection.commit(); //事务提交 } finally { if (rs != null) { rs.close(); } statement.close(); }} 打印结果如下： 代码块141234567891011current conn host: 172.22.119.8current conn host: 172.22.119.30current conn host: 172.22.119.8current conn host: 172.22.119.30current conn host: 172.22.119.38current conn host: 172.22.119.30current conn host: 172.22.119.38current conn host: 172.22.119.30current conn host: 172.22.119.8current conn host: 172.22.119.30...省略，共100条... 这是符合预期的，因为即便是单实例，每次处理事务的节点也发生了变换。 多实例就不再试了，没有必要。 6.4：开启autoReconnect时，单例执行Query操作按照我们的结论，这个只有在节点坏掉时才会重新pick节点，以保证可用性，那么我们现在来开启它，然后依然用单例模式操作Query，代码改写如下： 代码块15123456789101112131415public static void main(String[] args) throws Exception { String url = \"jdbc:mysql:loadbalance://172.22.119.38:4000,172.22.119.8:4000,172.22.119.30:4000/tidb_test?autoReconnect=true\"; //带上autoReconnect参数，使其为true //仍然是单例的Connection对象 LoadBalancedConnection connection = (LoadBalancedConnection) DriverManager.getConnection(url, \"tidb_test\", \"lPoK3QMSWY1BhSa3WCT1IWOXYkMc3Aqd\"); Test test = new Test(); for (int i = 0; i &lt; 100; i++) { try { test.triggerQuery(connection); } finally { //为了保证是单例模式，这里不再close //connection.close(); } }} 运行结果打印如下： 代码块1612345678current conn host: 172.22.119.8current conn host: 172.22.119.8current conn host: 172.22.119.8current conn host: 172.22.119.8current conn host: 172.22.119.8current conn host: 172.22.119.8current conn host: 172.22.119.8...省略，共100条... 符合我们的预期，因为它的作用不是干这个的。 七、结论JDBC驱动程序的LoadBalance模式，是针对每一个被新建出来的Connection对象的LB，它并非很多人第一眼看到它协议头时所理解的那种将jdbc连接里配置的所有节点视作一个整体，每次利用Connection对象做一些操作时都会pick出来一个节点使用，以达到某种意义上的负载均衡，而是每次新建Connection对象时，从那堆host里pick出来其中一个，创建对应的Connection对象，这跟我们第一眼看到它的感觉不太一样，但是实现上确实没什么太大的问题，因为单纯使用JDBC时本就不提倡Connection单例复用，若想要复用，需要结合各类连接池一起使用，通过对JDBC的LB模式的了解可以知道，结合某种连接池技术来支撑，就可以达到我们想象中的LB效果，因为池内每一个Connection对象在创建时，总会触发JDBC的LB策略。","link":"/2020/06/02/%E8%AF%A6%E8%A7%A3JDBC%E7%9A%84Loadbalance%E6%A8%A1%E5%BC%8F/"},{"title":"JAVA有关位运算的全套梳理","text":"一、在计算机中数据是如何进行计算的？1.1：java中的byte型数据取值范围我们最开始学习java的时候知道，byte类型的数据占了8个bit位，每个位上或0或1，左边第一位表示符号位，符号位如果为1表示负数，为0则表示正数，因此要推算byte的取值范围，只需要让数值位每一位上都等于1即可。我们来用我们的常规思维来分析下byte类型的取值范围： 如果按照这种思路来推算，七个1的二进制数转换为十进制是127，算上符号位，取值范围应为：-127~127，但事实上我们知道，byte的取值范围是-128~127，这里先打个问号，接着往下看。现在让我们计算下byte类型的7加上byte类型的-2是多少： 诶？跟我们预想的不一样，因为我们是知道7和-2的和应该是5才对，结果应该表示为：00000101，但事实上通过图2的结果来看确实跟预想的不一样，所以计算机在做计算的时候，肯定不是表面上的符号位+数值位的方式进行的计算的。 1.2：原码，反码，补码我们先来看下定义： 👉 原码定义：符号位加后面的数值，比如图2里的00000111和10000010都是原码，原码比较简单，就是我们在上面单纯理解上的原值。 👉 反码定义：正数的反码就是它的原码，负数的反码符号位不变，其余数值位全部按位取反，例如： 00000111的反码：00000111 10000010的反码：11111101 👉 补码定义：同样的，正数的补码仍然等于它的原码本身，负数的补码等于它自己的反码+1，例如： 00000111的补码：00000111 10000010的补码：11111110 🌴 总结：正数的原码、反码、补码完全一致，负数的反码等于它原码的数值位按位取反，负数的补码等于它的反码+1 现在让我们用反码的方式来计算下图2中的式子： 利用数值的反码计算出的结果已经很接近正确答案了，+4的反码等于它的原码，现在只需要让它+1就是正确答案，还记得补码的定义吗？负数的补码等于它的反码+1，那现在让我们用补码做下计算试试？ ok，我们发现，用它们的补码做加法，得到的数值就是我们想要的正确答案，事实上，计算机并没有减法运算器，所有的减法运算，都是以一个正数加上一个负数的形式来交给加法运算器计算的，由于负数的符号位为1，虽然我们人是知道它的含义，但是作为计算机，它是不知道第一位是符号位的，它要做的就仅仅是让两个数相加而已，正是因为如此，我们才不能简简单单保存负数，通过图4我们知道，两个数的补码相加，可以得到一个准确的数值。 再举个相加结果为负数的例子，让两个负数相加： 如果结果为负数的话，也是适用的，只是它仍然是以补码的形式存放的，需要转成原码才符合我们人的理解方式。 现在回到上面留下的问题，为什么byte的取值范围是-128~127呢？ 我们之前按照图1里的理解，理所应当的以为它应该是-127~127的范围，那是因为我们按照图1的理解方式，数值就是以符号位+数值位的方式理解的（也就是按照原码的方式理解的），但是你可以想一下，如果按照图1那种理解方式，是不是会存在两个0值呢？ 即：10000000和00000000，+0和-0； 其次如果站在机器角度上来说，所有的负数都很大，至少要比所有正数大，因为负数的最高位也就是符号位都是1，显然这是不对的，通过本节我们知道了，所有的数均通过自己的补码完成计算，如果将最后得到的结果转成原码，就是我们人眼可以理解的最终值（符号位+数值位），如果现在利用补码的方式做理解，符号位为0的数没啥好说的，自然取值区间为：0~127，但是符号位为1的负数呢？负数就存在一个特殊值（也就是我们之前片面理解的-0）：10000000，如果按照原码理解它是-0，但我们前面说过，计算机里所有数字，都是以补码的方式参与运算的，而负数的补码不等于其原码，这个10000000在计算机里显然是某个负数的补码，那么问题就变的简单多了，即10000000是谁的补码呢？答案是：-128，这也是为什么负数的取值范围会比正数多一个的原因，byte类型如此，其它类型也是如此，比如int型的负数取值也比正数多1。 这一块的定义要清晰，对理解后面的位运算会有很大的帮助。 二、java中的位运算2.1：与运算与运算符号：&amp; 与运算特点：1&amp;1=1、1&amp;0=0、0&amp;1=0、0&amp;0=0 现在我们来举一个例子： 让我们再来试试负数： 2.2：或、异或跟与运算的运算方式一致，只不过规则不太一样： 或运算符号：| 或运算规则：1|1=1、1|0=1、0|1=1、0|0=0 异或运算符号：^ 异或运算规则：1^1=0、1^0=1、0^1=1、0^0=0 2.3：按位取反取反符号：~ 即一个数对自己取反，例如： 某个数字a的二进制为： 1010110，则~a为： 0101001 2.4：左移运算左移运算符：&lt;&lt; 例如： 位运算越界&amp;数位抛弃： 图8中的116的二进制数的数值位为7位，符号位为0，此时如果左移超过24位，就会出现负数，为什么会这样？因为java中的位移越界时，java会抛弃高位越界部分，我们知道java里int类型的第一位是符号位，如果符号位是1，则表示其为负数，现在将数值位占7bit符号位为0的116左移24位，就会出现下方结果： 01110100000000000000000000000000 正好31位占全，顶至符号位，低位补0，我们称24为116的不越界的最大左移值，若超出这个值，就会越界，比如左移25位： 11101000000000000000000000000000 显然左移25位后会把数值位的1移动到符号位，这时它表示为一个负数的补码。根据这个规则，我们如果让其左移28位，则值为： 01000000000000000000000000000000 也就是十进制的1073741824，即：116 &lt;&lt; 28 = 1073741824，那如果越界过多呢？比如int型的数据，左移32位：116 &lt;&lt; 32 = 116 会发现，如果左移自己位数一样多的位数，那么这个数就等于它本身，因此运算符合以下规则： 设x为被位移值，y为本次位移的位数，z为x所属类型的最大存储位数： x &lt;&lt; y = x &lt;&lt; (y%z) 如果是int型（32位，long型就用64代入计算），符合如下规则： 116 &lt;&lt; 4 = 116 &lt;&lt; (4%32) = 116 &lt;&lt; 4 = 1856 116 &lt;&lt; 32 = 116 &lt;&lt; (32%32) = 116 &lt;&lt; 0 = 116 116 &lt;&lt; 36 = 116 &lt;&lt; (36%32) = 116 &lt;&lt; 4 = 1856 2.5：有符号右移运算&amp;无符号右移运算有符号右移运算符：&gt;&gt; 无符号右移运算符：&gt;&gt;&gt; 例如：a &gt;&gt; b表示a右移b位，跟上面的左移例子一样，右移也会有越界问题，只是右移越界是从右边开始抛弃越界部分的，右移操作有符号位干扰，如果是正数右移，无此干扰项，因为符号位本就是0右移不会影响值的准确性，但如果是负数，第一位是符号位，且值为1，右移就有影响了，现在仍然以116为例： 正数右移： 上述是正数，右移无影响，但是负数，这里以-116为例，我们知道负数在计算机里是以补码的形式存储的，所以图里直接用-116的补码做运算，位移过程如下： 你会发现右移跟左移不一样，左移是不用担心自己符号位存在“补位”问题的，但是右移存在，如图中-116右移4位后，左边第一位，也就是符号位，就面临着补位的问题，那我现在是该补1呢，还是补0呢？这也就是为什么右移操作会存在有符号右移和无符号右移两种移动方式： ☘️ 有符号右移：依照原符号位，如果原符号位是1，那么图4里需要补位的空位全部补1，如果原符号位为0，则全部补0 ☘️ 无符号右移：无视原符号位，全部补0 现在让我们用有符号的方式将-116右移4位，即-116 &gt;&gt; 4，按照有符号的规则，补位符合原符号位，则右边4位全部补1： 得到的仍然是个负数，它仍然是一个补码，图里展示不开，它的结果为：11111111111111111111111111111000，经转换可知它是-8的补码，即：-116 &gt;&gt; 4 = -8 现在再试试用无符号右移，根据无符号的特点，右移后的前四位无脑补0： 图里展示不开，它的结果为：00001111111111111111111111111000 可见它是个正数，转换成十进制为：268435448，即：-116 &gt;&gt;&gt; 4 = 268435448 最后说一下，跟左移一样，右移里不管是有符号还是无符号，也符合取余的方式，计算出位移的最终位数： -116 &gt;&gt; 4 = -116 &gt;&gt; (4%32) = -116 &gt;&gt; 4 = -8 -116 &gt;&gt; 32 = -116 &gt;&gt; (32%32) = -116 &gt;&gt; 0 = -116 -116 &gt;&gt; 36 = -116 &gt;&gt; (36%32) = -116 &gt;&gt; 4 = -8 2.6：类型转换溢出了解完位运算，来看一个比较实际的问题，看下面的代码： 12long a = 8934567890233345621L;int b = (int) a; //b的值为-1493678507 最终b的值是一个负数，这是由于long型64位，让int型强行接收，会出现位溢出的问题，这个流程如下： 三、位运算在实际项目中的运用位运算的性能是非常好的，相比运算流程，计算机更喜欢这种纯粹的逻辑门和移动位置的运算，但位运算在平常的业务代码里并不太常见，因为它的可读性不太好，但是我们仍然可以利用位运算来解决一些实际项目里的问题。 比如用来表示开关的功能，比如需求里经常有这种字段：是否允许xx（0不允许，1允许），是否有yy权限（0没有，1有），是否存在zz（0不存在，1存在） 上面只是举例，类似这种只有两种取值状态的属性，如果当成数据库字段放进去的话，太过浪费，如果之后又有类似的字段，又得新增数据库字段，为了只有两种取值的字段，实在是不太值得。 这个时候何不用一个字段来表示这些字段呢？你可能已经猜到要怎么做了： 顶一个int型或者long型的字段，让它的每一个二进制位拥有特殊含义即可，然后按照位运算将其对应的位置上的数值变成0或1，那如何将某个数的二进制位第x位上的数值变成1或0呢？其实这在位图结构里经常用到，就是利用1这个特殊的值作位移运算后再与原值进行位运算，让我们看下这个过程： 把一个数的第2位的字符变成1，现在假设这个数初始化为0，int型，我们把它当成二进制展示出来： 现在如何把这个数的第二位变成1呢？目前是这样做的： 10 | 1 &lt;&lt; 1 即原值跟1左移1位后的值作或运算，先来看看1 &lt;&lt; 1的结果： 然后拿着图16的结果，跟原数（也就是0）进行或运算： 可以看到，原数的第二位已经被置为1了，它的十进制对应2，其它位的数置为1也大同小异，例如，现在让第6位也变成1只需要： 12 | 1 &lt;&lt; 5 即拿着原值（现在为2）跟1左移5位后的数做或运算，这个流程如下： 看完了把某个位置的数值置为1，那如何把某位设置为0呢？我们现在把图18里的结果的第6位重新置回0，目前的做法为： 134 &amp; ~(1 &lt;&lt; 5) 即拿着原值（经过上面几步的运算，现在值为34）跟1左移5位按位取反后的数做与运算，来看下这个流程： 经过上面的流程，就可以把原值的第6位变成0了。 那么我们知道了让一个数的二进制位的某位变成0或1的方法，那如何知道一个数的某位上究竟是0还是1呢？毕竟我们业务代码需要知道第几位代表什么意思并且获取到对应位置上的值。 假如我现在想知道十进制int型数34的第6位是0还是1，写法如下： 134 &gt;&gt; 5 &amp; 1 即让原值（34）右移5位后跟1做与运算，来看下这个流程： 由图可以看出，想要知道一个数的第几位是1还是0，只需要将其对应位置上的值“逼”到最后一位，然后跟1相与即可，如果对应位置上的值是0，那么与1相与后的结果一定为0，反之一定为1. ☘️ 总结 到这里已经说完了为什么要用一个数表示那么多开关，以及如何给一个开关位设置对应的开关值，以及如何找到对应开关位的值，有了这些操作，我们再也不需要为这种只有0和1取值的字段新增数据库字段了，因为一个int型的数字，就可以表达32个开关属性，如果超了，还可以扩成64位的long型~","link":"/2020/03/10/JAVA%E6%9C%89%E5%85%B3%E4%BD%8D%E8%BF%90%E7%AE%97%E7%9A%84%E5%85%A8%E5%A5%97%E6%A2%B3%E7%90%86/"},{"title":"IDEA插件开发（一）一个简单的表单demo","text":"🐜 版本信息：&nbsp;&nbsp; 🐝 插件项目基于gradle构建。 🦟 知识背景：swing 🦇 参考文档： http://www.jetbrains.org/intellij/sdk/docs/tutorials/build_system/prerequisites.html http://www.jetbrains.org/intellij/sdk/docs/user_interface_components/tool_windows.html http://www.jetbrains.org/intellij/sdk/docs/user_interface_components/dialog_wrapper.html https://intellij-support.jetbrains.com/hc/en-us/community/posts/360003338799-Build-compatible-plugin 目标本实例实现一个Idea的插件，弹出一个表单Dialog，然后点击按钮，获取表单里输入的内容，然后将内容打印在表单的上方。 成品图展示： 一、项目初始化新建一个gradle项目，修改其build.gradle文件： 代码块112345678910111213141516171819202122232425262728293031323334353637383940plugins { id 'java' id 'org.jetbrains.intellij' version '0.4.14' //引入intellij的gradle插件} group 'org.example'version '1.0' //定义jar包/zip包的版本号 sourceCompatibility = 1.8 //限制jdk的使用版本号，这里限制到8，表示生成的idea插件只能运行在jdk8以上的环境中 repositories { mavenCentral() //远程仓库} dependencies { //这里引别的依赖包 testCompile group: 'junit', name: 'junit', version: '4.12'} // See https://github.com/JetBrains/gradle-intellij-plugin/intellij { // 这里是指打插件包的时候用idea什么版本的依赖包打 // 比如这里用2019.3打包，如果你的插件实现源码里用了2019.3不存在的依赖包或类，就会报错 // 一般就填当前IDEA的版本号即可 version \"2019.3\"} patchPluginXml { //changeNotes里的内容展示位置参考图14 changeNotes \"\"\" 1.0版本. 第1.0版本：初始化这个测试插件项目\"\"\" // 这个意思是说当前定义的这个插件最早支持到什么版本的IDEA // 这里配置sinceBuild=191，表示插件只能被版本号大于等于2019.1版本的IDEA安装，低于这个版本的将抛无法兼容的错误 // ↑上方参考这篇问答：https://intellij-support.jetbrains.com/hc/en-us/community/posts/360003338799-Build-compatible-plugin sinceBuild \"191\"} 然后Idea的右边栏gradle将会多出intellij选项： 这里说下runIde，它用来调试插件，运行它会再次启动一个Idea，这个Idea会自动安装上你当前定义的插件包，让你用来调试。 二、新增plugin.xml这个文件非常重要，它可以指定你定义的插件出现在IDEA的哪个位置，可以指定具体的处理逻辑，还可以定义插件名称、子名称等等。 这个文件位于MATE-INF下： 配置内容为： 代码块2123456789101112131415161718192021222324252627282930313233343536&lt;idea-plugin&gt; &lt;!--插件的id，注意不要跟其他插件重复，这个id全局唯一，尽可能复杂些--&gt; &lt;id&gt;plugin.test&lt;/id&gt; &lt;!--插件的名称--&gt; &lt;name&gt;PluginTest&lt;/name&gt; &lt;vendor email=\"xxxx@qq.com\" url=\"http://www.bilibili.com\"&gt;你公司的名字&lt;/vendor&gt; &lt;!--插件的描述信息，支持html，展示的位置参考图14--&gt; &lt;description&gt;&lt;![CDATA[ Plugin Test&lt;br&gt; 第一行：单纯只是个测试&lt;br&gt; 第二行：都说了只是个测试(●￣(ｴ)￣●)&lt;br&gt; &lt;a href='https://www.bilibili.com'&gt;你猜猜这是哪个网站？&lt;/a&gt; &lt;em&gt;v1.0&lt;/em&gt; ]]&gt;&lt;/description&gt; &lt;extensions defaultExtensionNs=\"com.intellij\"&gt; &lt;!-- Add your extensions here --&gt; &lt;/extensions&gt; &lt;!--跟build.gradle里的sinceBuild一致即可，意义相同，必须配置--&gt; &lt;idea-version since-build=\"191\"/&gt; &lt;actions&gt; &lt;!--下面的group是分组，分组需要有一个唯一的id标识，text用来控制分组出现在IDEA时呈现的文案，description是描述，不会展现出来，简单描述下分组就行--&gt; &lt;group id=\"PluginTest\" text=\"插件测试组\" description=\"插件测试描述\"&gt; &lt;!--add-to-group控制把该分组加到IDEA里，group-id用来描述加在哪个位置，MainMenu表示加在IDEA上方的主菜单栏里， anchor表示顺序，last表示最后一个，所以下面的配置可以描述为：将该插件加到IDEA上方主菜单栏的最后一位--&gt; &lt;add-to-group group-id=\"MainMenu\" anchor=\"last\"/&gt; &lt;!--这个用来指定一个分组下的触发动作，同样的需要一个id，自定义；class就是用来处理这个动作的逻辑类，具体的插件逻辑都会写到对应的action类里，text用来控制文案，description为描述--&gt; &lt;action id=\"Plugin.Test.Action\" class=\"plugin.test.FromAction\" text=\"表单测试\" description=\"表单测试描述\"/&gt; &lt;/group&gt; &lt;/actions&gt;&lt;/idea-plugin&gt; 然后定义一个Action类，记为FormAction，继承AnAction，实现其抽象方法actionPerformed即可： 代码块3123456public class FromAction extends AnAction { @Override public void actionPerformed(@NotNull AnActionEvent e) { //TODO 这里放插件逻辑 }} 三、启动现在双击runIde即可调出另外一个安装了这个插件的IDEA界面，然后可以看运行结果进行调试。 runIde还支持debug模式，不过运行时要右击选择： 来看下调试IDEA的界面运行效果： 四、定义Action4.1：定义会话框类经过上面三步的配置，插件的基本样式已经展示出来，但是点击下方“表单测试”的action，并没有什么用，因为其绑定的FormAction类里没有任何有意义的实现。现在来实现开始的目标，点击“表单测试”后，弹出一个自定义的表单会话框，然后点击按钮，获取表单内容后打印在会话框内。 会话框（Dialog）需要定义一个继承了IDEA的DialogWrapper抽象类的子类，这个子类就是自定义的会话框实现，所有的样式定义、功能触发都是放到这个子类里的，现定于如下子类： 代码块412345678910111213141516171819202122232425262728293031public class FormTestDialog extends DialogWrapper { private String projectName; //假如需要获取到项目名，作为该类的属性放进来 // DialogWrapper没有默认的无参构造方法，所以需要重写构造方法，它提供了很多重载构造方法， // 这里使用传project类型参数的那个，通过Project对象可以获取当前IDEA内打开的项目的一些属性， // 比如项目名，项目路径等 public FormTestDialog(@Nullable Project project) { super(project); setTitle(\"表单测试~~\"); // 设置会话框标题 this.projectName = project.getName(); } // 重写下面的方法，返回一个自定义的swing样式，该样式会展示在会话框的最上方的位置 @Override protected JComponent createNorthPanel() { return null; } // 重写下面的方法，返回一个自定义的swing样式，该样式会展示在会话框的最下方的位置 @Override protected JComponent createSouthPanel() { return null; } // 重写下面的方法，返回一个自定义的swing样式，该样式会展示在会话框的中央位置 @Override protected JComponent createCenterPanel() { return null; }} 4.2：会话框模块&amp;类元素对照找个实际的会话框为例，针对上述中几个方法所控制的会话框里的元素如下： 4.3：自定义会话框元素4.3.1：会话框方法重定义按照本文的实现目标，自定义的表单主体部分可以位于createCenterPanel里，然后表单的大标题可以放到createNorthPanel里，提交按钮可以放到createSouthPanel里，现在改写如下： 代码块512345678910111213141516171819202122232425262728293031public class FormTestDialog extends DialogWrapper { private String projectName; //swing样式类，定义在4.3.2 private FormTestSwing formTestSwing = new FormTestSwing(); public FormTestDialog(@Nullable Project project) { super(true); setTitle(\"表单测试~~\"); //设置会话框标题 this.projectName = project.getName(); //获取到当前项目的名称 init(); //触发一下init方法，否则swing样式将无法展示在会话框 } @Override protected JComponent createNorthPanel() { return formTestSwing.initNorth(); //返回位于会话框north位置的swing样式 } // 特别说明：不需要展示SouthPanel要重写返回null，否则IDEA将展示默认的\"Cancel\"和\"OK\"按钮 @Override protected JComponent createSouthPanel() { return formTestSwing.initSouth(); } @Override protected JComponent createCenterPanel() { //定义表单的主题，放置到IDEA会话框的中央位置 return formTestSwing.initCenter(); }} 4.3.2：自定义swing样式下面是放置swing样式的类： 代码块612345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public class FormTestSwing { private JPanel north = new JPanel(); private JPanel center = new JPanel(); private JPanel south = new JPanel(); //为了让位于底部的按钮可以拿到组件内容，这里把表单组件做成类属性 private JLabel r1 = new JLabel(\"输出：\"); private JLabel r2 = new JLabel(\"NULL\"); private JLabel name = new JLabel(\"姓名：\"); private JTextField nameContent = new JTextField(); private JLabel age = new JLabel(\"年龄：\"); private JTextField ageContent = new JTextField(); public JPanel initNorth() { //定义表单的标题部分，放置到IDEA会话框的顶部位置 JLabel title = new JLabel(\"表单标题\"); title.setFont(new Font(\"微软雅黑\", Font.PLAIN, 26)); //字体样式 title.setHorizontalAlignment(SwingConstants.CENTER); //水平居中 title.setVerticalAlignment(SwingConstants.CENTER); //垂直居中 north.add(title); return north; } public JPanel initCenter() { //定义表单的主体部分，放置到IDEA会话框的中央位置 //一个简单的3行2列的表格布局 center.setLayout(new GridLayout(3, 2)); //row1：按钮事件触发后将结果打印在这里 r1.setForeground(new Color(255, 47, 93)); //设置字体颜色 center.add(r1); r2.setForeground(new Color(139, 181, 20)); //设置字体颜色 center.add(r2); //row2：姓名+文本框 center.add(name); center.add(nameContent); //row3：年龄+文本框 center.add(age); center.add(ageContent); return center; } public JPanel initSouth() { //定义表单的提交按钮，放置到IDEA会话框的底部位置 JButton submit = new JButton(\"提交\"); submit.setHorizontalAlignment(SwingConstants.CENTER); //水平居中 submit.setVerticalAlignment(SwingConstants.CENTER); //垂直居中 south.add(submit); return south; }} 现在点击下runIde按钮，同样的，在调试IDE里点击“表单测试”，然后就会弹出如下表单框： 🌿 除非有特殊情况需要自定义swing样式，否则建议不加任何swing样式，这样自定义的swing界面是会随着IDEA的主题改变而去自适应的，比如将图7中的调试IDE的主题设置成Darcula，自定义的表单也会自适应的变成黑色背景： 4.3.3：事件绑定定义好了样式，现在给“提交”按钮绑定一个事件，现在改写下FormTestSwing.initSouth方法： 代码块71234567891011121314151617181920public JPanel initSouth() { //定义表单的提交按钮，放置到IDEA会话框的底部位置 JButton submit = new JButton(\"提交\"); submit.setHorizontalAlignment(SwingConstants.CENTER); //水平居中 submit.setVerticalAlignment(SwingConstants.CENTER); //垂直居中 south.add(submit); //按钮事件绑定 submit.addActionListener(e -&gt; { //获取到name和age String name = nameContent.getText(); String age = ageContent.getText(); //刷新r2标签里的内容，替换为name和age r2.setText(String.format(\"name:%s, age:%s\", name, age)); }); return south;} 现在再来点击下“提交”按钮，就可以输出表单内容了： 4.4：插件绑定类：FormAction之前讲过，这个类是插件的入口，结合上面定义好的表单Dialog，来看下它是怎么写的： 代码块812345678public class FromAction extends AnAction { @Override public void actionPerformed(@NotNull AnActionEvent e) { FormTestDialog formTestDialog = new FormTestDialog(e.getProject()); formTestDialog.setResizable(true); //是否允许用户通过拖拽的方式扩大或缩小你的表单框，我这里定义为true，表示允许 formTestDialog.show(); }} 五、插件的打包&amp;安装截止到第四步，都只是在调试IDE里查看效果，如果一个插件开发完成后，需要被实际的IDEA安装，这个时候就需要借助打包选项来打包你的插件，点击下面的选项构建插件： 构建完成后，查看build包下的distributions目录，里面的zip包就可以直接安装进你的IDEA： 然后选择IDEA的Preferences下的plugins选项，弹出如下框，按照图里的指示选择zip包安装即可： 然后安装完成，重启IDEA即可： 各个展示模块对应插件项目里配置的来源参考下图： 重启后出现了跟调试IDEA里一样的菜单栏，选中后运行成功： 写在最后截止到这里，一个插件的开发、调试、安装就完成了，理论上通过这个简单的例子就可以实现一些实际的功能了，因为其完整展示了数据输入到数据获取整个过程。 因为工作当中需要写一个代码生成器，想要以一个IDEA插件的方式提供服务，所以在这里做个记录，防止以后再次用到时从零开始。。 要有一定的swing基础，我在开发代码生成器的时候，就是因为swing基础太差，布局花了非常多的时间。 🍒 之后不会深入去研究插件的开发，如果后续工作中有用到插件开发的其他的功能点，会更新在这个系列里，如果想深入搞IDEA插件开发，建议看IDEA的官方文档，官方文档有点乱，有很多只是简单介绍几句甚至没有示例，好在他们有个问答社区，建议搜索时用google搜英文关键词（google对英文搜索支持强大，没试过度娘，应该也可以搜到），里面会有人提问，比如版本兼容的问题就是google出来的，社区里正好有一篇问答。（链接放到开头里了）","link":"/2019/12/13/IDEA%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91%EF%BC%88%E4%B8%80%EF%BC%89%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E8%A1%A8%E5%8D%95demo/"},{"title":"Resilience4j熔断器-使用与源码解析","text":"🌏 环境：&nbsp;&nbsp; 🌾 依赖版本： 🍃 知识依赖：JUC，位图 一、什么是熔断在分布式系统中，各服务间的相互调用更加频繁，上下游调用中充满了可能性，一个服务可能会被很多其他服务依赖并调用，在这个过程中如果某个服务由于某种原因出错（业务出错、负载过高），可能会导致整个分布式调用链路失败： 上面这个过程最终可能会导致全链路瘫痪（服务雪崩），此时需要一种可以解决上述问题的策略，此策略设计目标为： 在发现有服务调用失败后，及时计算失败率 失败率达到某种阈值时，切断与该服务的所有交互，服务走切断后的自定义逻辑 切断并且不再调用该服务后主动监听被切断的服务是否已经恢复了处理能力，若恢复，则继续让其提供服务 这个策略被放进图1中，就变成了下面这样： 这个过程中，C服务在自己出问题的情况下，并不会像图1里那样仍然有大量流量打进来，也不会影响到上游服务，这个结果让调用链看起来比图1更加的稳定，这个过程就叫熔断。 针对这个过程，可以看到在C不可用时，B走了熔断后的降级逻辑，这个逻辑可以自定义，如果C在整个调用链里属于那种必须要成功的服务，那么这里的逻辑就可以是直接抛错，如果C属于那种失败了也无所谓，不影响整个业务处理，那么降级逻辑里就可以不做处理，例如下面的场景： 类似这种接口，降级策略很适合不做处理，返回空信息即可，这样最坏的情况就是页面少了某个板块的信息，可能会对用户造成不太好的体验，但是不影响其对外服务，被熔断的服务恢复后页面也会重新回归正常。熔断后的降级处理方式是件值得思考的事情，熔断和降级是相互独立的概念，熔断后必然会有降级操作（哪怕直接抛异常也是一种降级策略），这个降级操作是熔断这个动作导致的，所以很多时候会把熔断和降级放在一起说，其实降级还可以由其他动作触发，比如限流后抛出“系统繁忙”，这也是一种降级策略，只不过它是由限流触发的，再比如通过开关埋点在系统负载过高时主动关停一些次要服务来提升核心功能的响应速度，这也是一种降级策略，降级是最终产物，而产生它的方式有很多种。 二、Resilience4j中的熔断器2.1：Resilience4j是什么？它是一个轻量、易用、可组装的高可用框架，支持熔断、高频控制、隔离、限流、限时、重试等多种高可用机制。本篇文章只关注其熔断部分。 2.2：如何使用？通过第一部分的介绍，可以认为一个熔断器必须要具备统计单位请求内的错误率、全熔断、半熔断放量、恢复这几个流程，带着这个流程，下面来介绍下Resilience4j里熔断器的用法。 通过图2里服务B调用服务C的例子，现在利用java类来进行简单模拟下这个流程。 首先定义ServerC类，用于模拟服务C： 代码块1123456789public class ServerC { //使用该方法模拟服务C获取C信息的方法，假设现在服务C的getCInfo方法里有个bug，当输入的id为0时报错，其他情况正常 public String getCInfo(int id) { if (id == 0) { throw new RuntimeException(\"输入0异常\"); } return \"id=\" + id + \"的C信息\"; }} 再定义ServerB类，用于模拟服务B，这里给服务B调用服务C方法那里加上熔断器处理，注意这个类里的注释，会详细说明熔断器的主要配置项以及其使用方法： 代码块2123456789101112131415161718192021222324252627282930313233343536public class ServerB { private CircuitBreakerRegistry breakerRegistry; private ServerC serverC = new ServerC(); //让服务B持有一个服务C的引用，用来表示正常服务间调用里的一个连接引用 ServerB() { //初始化breaker注册器，可以利用该对象生产各种breaker对象（注：凡是用同一个注册器生产出来的breaker，都会继承注册器的配置属性） breakerRegistry = CircuitBreakerRegistry.of(CircuitBreakerConfig.custom() //of方法里面放的就是breaker的配置属性对象 .enableAutomaticTransitionFromOpenToHalfOpen() //开启从全开状态经过下面的waitDurationInOpenState时间后自动切换到半开状态 .failureRateThreshold(50) //熔断器闭合状态下的错误率阈值，50表示50%，如果错误率达到这个阈值，那么熔断器将进入全熔断状态 .ringBufferSizeInClosedState(100) //熔断器闭合状态下，以该值为单位请求数，计算错误率，跟上面错误率阈值综合理解，这个值表示至少有100个请求，且错误50个以上才会触发全熔断 .ringBufferSizeInHalfOpenState(10) //熔断器半熔断状态下，以该值为单位请求数，计算错误率，跟上面错误率阈值综合理解，这个值表示至少有10个请求，且错误5个以上会再次触发全熔断，相比闭合状态，半熔断状态下更容易再次进入全熔断状态 .waitDurationInOpenState(Duration.ofMillis(1000L)) //熔断器全熔断状态持续的时间，全熔断后经过该时间后进入半熔断状态 .build()); } //服务B通过服务C来获取到C的info信息，该方法就是用来干这个的，它会发起对服务C的调用 public String getCInfo(int id) { //breaker对象是按照name划分全局单例的 CircuitBreaker breaker = breakerRegistry.circuitBreaker(\"getCInfo\"); //这里给熔断器取个名，一般情况就是一个服务的path或方法名 try { return breaker.executeCallable(() -&gt; serverC.getCInfo(id)); } catch (CircuitBreakerOpenException e) { //一旦抛出该异常说明已经进入全熔断状态 //被熔断后的降级逻辑 return \"服务C出错，触发服务B的降级逻辑\"; } catch (Exception e) { //熔断关闭或者半熔断状态下，C抛出的错误会被catch到这里 return \"调用服务C出错\"; } } public CircuitBreaker getBreaker() { return breakerRegistry.circuitBreaker(\"getCInfo\"); //为了方便做测试，这里返回对应的breaker对象 }} 上述配置的熔断器解释如下： 在熔断器闭合的情况下（也即是正常情况下），以100个请求为单位窗口计算错误率，一旦错误率达到50%，立刻进入全熔断状态，该状态下服务B不会再发生对服务C的调用，直接走自己的降级逻辑，经过1000ms后恢复为半熔断状态，此时流量开始打进服务C，此时仍然会计算错误率，只是半熔断状态下，是以10个请求为单位窗口计算的错误率，这个可以保证在服务C没有恢复正常的情况下可以更快速的进入全熔断状态。 2.3：测试-熔断器状态切换然后开始编写测试方法，下面会通过测试方法来详细解析该熔断器的状态变迁： 代码块3123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public void testBreak() throws Exception { //按照B服务里熔断器的配置，如果进行100次请求，有50次失败了，则对ServerC的调用进入全熔断状态 //1000ms后恢复为半熔断状态，半熔断状态下进行10次请求，如果有5次依然失败，则再次进入全熔断状态 for (int i = 0; i &lt; 100; i++) { if (i &lt; 50) { serverB.getCInfo(0); //前50次全部报错 } else { serverB.getCInfo(1); //后50次全部成功 } } //断言：此时熔断器为全熔断状态 System.out.println(serverB.getBreaker().getState().equals(CircuitBreaker.State.OPEN)); //全熔断状态下并不会实际调用C，而是会走服务B的降级逻辑，即便我们输入的参数是对的，也一样会被降级 System.out.println(serverB.getCInfo(1)); Thread.sleep(500L); //断言：由于全熔断状态配置的持续时间时1000ms，所以500ms过去后，仍然是全熔断状态 System.out.println(serverB.getBreaker().getState().equals(CircuitBreaker.State.OPEN)); Thread.sleep(500L); //断言：1000ms过后，熔断器处于半熔断状态 System.out.println(serverB.getBreaker().getState().equals(CircuitBreaker.State.HALF_OPEN)); //半熔断状态下会尝试恢复，所以会实际调用C，分别输入正确和错误的参数进行测试 System.out.println(serverB.getCInfo(1)); System.out.println(serverB.getCInfo(0)); //半熔断状态下，只需要请求10次，有5次出错即可再次进入全熔断状态 for (int i = 0; i &lt; 10; i++) { if (i &lt; 4) { //因为上面传过一次0了，所以这里只需要4次便可以重新回到全开状态 serverB.getCInfo(0); //前5次全部报错 } else { serverB.getCInfo(1); //后5次全部成功 } } //断言：此时熔断器为全熔断状态 System.out.println(serverB.getBreaker().getState().equals(CircuitBreaker.State.OPEN)); //同样的，全熔断状态下并不会实际调用C，而是会走服务B的降级逻辑 System.out.println(serverB.getCInfo(1)); //这时静待1000ms，再次进入半熔断状态，我们尝试恢复服务C的调用 Thread.sleep(1000L); //这时我们让其10次请求里有6次成功 for (int i = 0; i &lt; 10; i++) { if (i &lt; 6) { //前6次成功 serverB.getCInfo(1); } else { //后4次失败 serverB.getCInfo(0); } } //由于10次请求里只失败了4次，达不到50%的全开阈值，所以此时会恢复 //断言：此时熔断器为闭合状态 System.out.println(serverB.getBreaker().getState().equals(CircuitBreaker.State.CLOSED)); System.out.println(serverB.getCInfo(1)); //正常输出 System.out.println(serverB.getCInfo(0)); //走普通异常逻辑 } 最终输出如下： 1234567891011true服务C出错，触发服务B的降级逻辑truetrueid=1的C信息调用服务C出错true服务C出错，触发服务B的降级逻辑trueid=1的C信息调用服务C出错 可以看到，单位请求内达到错误率阈值后熔断器会进入全开状态（全熔断），全开状态下走降级逻辑，此时不再会实际请求服务C，一段时间后（全开持续时间），进入半开状态（半熔断），半开时仍然正常打入服务C，只是由于单位请求量相比闭合时更小，若服务还没恢复，计算错误率会更快达到错误率阈值而迅速进入全开状态，以此类推。如果服务已经恢复，那么将会从半开状态进入闭合状态。 2.4：测试-错误率统计方式通过上面的测试用例可以知道触发熔断器状态切换的时机，而且闭合状态下和半熔断状态下统计错误率的单位请求数不相同，那么这个请求数量又是怎么统计的呢？如果一个请求先错误了49次，然后在第101次请求的时候再错误1次是否可以成功触发熔断器全开？如果把这49次失败往后挪一位呢？现在再来按照设想测试下其错误率的统计方式： 代码块412345678910111213141516public void testRate() { //首先闭合状态下单位请求仍然是100，现在让前49次全部失败 for (int i = 0; i &lt; 100; i++) { if (i &lt; 49) { serverB.getCInfo(0); } else { serverB.getCInfo(1); } } //断言：虽然请求了100次，但是错误率并没有达到阈值（50%），所以这里仍然是闭合状态的 System.out.println(serverB.getBreaker().getState().equals(CircuitBreaker.State.CLOSED)); //这里再让其失败一次 serverB.getCInfo(0); //断言：这里应该还是闭合状态的，按照100次单位请求来看，第一次失败的那个请求会被这次失败这个请求顶替掉（这里不理解没关系，下面有图） System.out.println(serverB.getBreaker().getState().equals(CircuitBreaker.State.CLOSED)); } 输出结果为： 12truetrue 然后我们让第一次失败的那次请求和其后面出错的请求后移一位： 代码块512345678910111213141516public void testRate() { //首先闭合状态下单位请求仍然是100，仍然让其错误49次，但现在让第2~50次失败 for (int i = 0; i &lt; 100; i++) { if (i != 0 &amp;&amp; i &lt; 50) { //第2~50次请求失败，总计失败49次 serverB.getCInfo(0); } else { serverB.getCInfo(1); } } //断言：跟上面例子一样，错误率并没有达到阈值，仍然是闭合状态 System.out.println(serverB.getBreaker().getState().equals(CircuitBreaker.State.CLOSED)); //这里再让其失败一次 serverB.getCInfo(0); //断言：这里应该是全开状态，按照100次单位请求来看，第一次成功的那个请求会被这次失败这个请求顶替掉，然后凑够50次失败请求（参考图4） System.out.println(serverB.getBreaker().getState().equals(CircuitBreaker.State.OPEN)); } 输出结果为： 12truetrue 用图来描述下导致这两种情况发生的流程： 所以Resilience4j在计算失败率的时候，是会发生滑动的，错误率是根据当前滑动窗口内的请求进行计算得出的，每次请求都会导致窗口移动，都会重新计算当前失败率，这个在源码解析里会说明这是怎样的一种结构，这里简单了解即可。 三、源码解析3.1：注册器入口通过上面ServerB类里的使用，首先会通过CircuitBreakerRegistry.of生成一个注册器对象，然后利用注册器对象的circuitBreaker方法来生成一个实际的breaker对象，代码如下： 代码块6123456public interface CircuitBreakerRegistry { //静态方法返回了InMemoryCircuitBreakerRegistry的实例 static CircuitBreakerRegistry of(CircuitBreakerConfig circuitBreakerConfig) { return new InMemoryCircuitBreakerRegistry(circuitBreakerConfig); }} InMemoryCircuitBreakerRegistry类代码如下（已简化处理，只展示流程相关代码）： 代码块71234567891011121314151617public final class InMemoryCircuitBreakerRegistry implements CircuitBreakerRegistry { //所有的breaker被存方在这个map里，breaker按照name不同而不同，每个breaker里都有自己的一份错误率统计数据 private final ConcurrentMap&lt;String, CircuitBreaker&gt; circuitBreakers; private final CircuitBreakerConfig defaultCircuitBreakerConfig; //开始的配置对象，闭合状态单位请求量、半开状态单位请求量、错误率阈值等都会放在这里面 public InMemoryCircuitBreakerRegistry(CircuitBreakerConfig defaultCircuitBreakerConfig) { this.defaultCircuitBreakerConfig = Objects.requireNonNull(defaultCircuitBreakerConfig, \"CircuitBreakerConfig must not be null\"); this.circuitBreakers = new ConcurrentHashMap&lt;&gt;(); } @Override public CircuitBreaker circuitBreaker(String name) { //添加一个breaker，若存在，直接返回 return circuitBreakers.computeIfAbsent(Objects.requireNonNull(name, \"Name must not be null\"), (k) -&gt; CircuitBreaker.of(name, defaultCircuitBreakerConfig)); }} 这个流程很简单，就是用一个map来维护所有breaker的，所以需要注意的是，命名breaker的时候，不要携带一些id之类的字段，很容易把map撑爆。 3.2：Breaker实体-CircuitBreaker拿到breaker实体后首先会通过其executeCallable方法执行需要被熔断的逻辑块，之前提到的所有的错误率统计、状态切换都发生在这个实体内。 代码块8123456789101112131415161718192021222324252627public interface CircuitBreaker { default T executeCallable(Callable callable) throws Exception{ return decorateCallable(this, callable).call(); //包装原始的callable } //方法包装，返回一个Callable对象，真正的业务逻辑callable在这里被执行 static Callable decorateCallable(CircuitBreaker circuitBreaker, Callable callable){ return () -&gt; { //全熔断状态下，这里返回false，会抛出CircuitBreakerOpenException类型的异常，ServerB里判定是否走降级逻辑就是通过catch该异常来决定的 if(!circuitBreaker.isCallPermitted()) { throw new CircuitBreakerOpenException(String.format(\"CircuitBreaker '%s' is open\", circuitBreaker.getName())); } //非全熔断状态触发下面的逻辑 long start = System.nanoTime(); try { T returnValue = callable.call(); //执行实际的业务逻辑 long durationInNanos = System.nanoTime() - start; circuitBreaker.onSuccess(durationInNanos); //非常关键的方法，用来累计执行成功的数量，计算错误率 return returnValue; } catch (Throwable throwable) { //执行异常，调用onError累计出错数 long durationInNanos = System.nanoTime() - start; circuitBreaker.onError(durationInNanos, throwable); //非常关键的方法，用来累计执行失败的数量，计算错误率 throw throwable; } }; }} CircuitBreaker是一个接口，CircuitBreakerStateMachine是它的实现类，上述代码里比较关键的isCallPermitted、onSuccess、onError都是在这个CircuitBreakerStateMachine类里实现的。 CircuitBreakerStateMachine类比较复杂，牵扯到整个熔断器的状态切换、错误统计触发等，精简一下该类，只关注核心部分： 代码块912345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394public final class CircuitBreakerStateMachine implements CircuitBreaker { //熔断器的名称 private final String name; /** * 非常非常关键的一个属性，它是一个引用对象，CircuitBreakerState一共有以下子类：ClosedState、HalfOpenState、OpenState、DisabledState、ForcedOpenState * 熔断器每次发生状态切换，都会new出一个新的XXState对象，让下面的引用指向新的状态对象 */ private final AtomicReference stateReference; //开始设置的熔断器配置，通过该对象可以拿到错误率阈值、全熔断持续状态等信息 private final CircuitBreakerConfig circuitBreakerConfig; //&amp;&amp;&amp; 事件处理器，这里不是重点，放到第四部分说，可以先忽略 private final CircuitBreakerEventProcessor eventProcessor; //构造器 public CircuitBreakerStateMachine(String name, CircuitBreakerConfig circuitBreakerConfig) { this.name = name; this.circuitBreakerConfig = circuitBreakerConfig; this.stateReference = new AtomicReference&lt;&gt;(new ClosedState(this)); //初始化的时候，熔断器状态都是闭合状态，所以首先new一个ClosedState并让stateReference指向它 this.eventProcessor = new CircuitBreakerEventProcessor(); } //切换到闭合状态，new ClosedState，可以看到每个XXState对象都持有当前CircuitBreakerStateMachine对象 @Override public void transitionToClosedState() { stateTransition(CLOSED, currentState -&gt; new ClosedState(this, currentState.getMetrics())); } //切换到全熔断状态，new OpenState， @Override public void transitionToOpenState() { stateTransition(OPEN, currentState -&gt; new OpenState(this, currentState.getMetrics())); } //切换到半熔断状态，new HalfOpenState， @Override public void transitionToHalfOpenState() { stateTransition(HALF_OPEN, currentState -&gt; new HalfOpenState(this)); } //状态切换方法（也即是XXState对象切换的地方） private void stateTransition(State newState, Function&lt;CircuitBreakerState, CircuitBreakerState&gt; newStateGenerator) { //引用指向新的XXState对象 CircuitBreakerState previousState = stateReference.getAndUpdate(currentState -&gt; { if (currentState.getState() == newState) { return currentState; } return newStateGenerator.apply(currentState); }); if (previousState.getState() != newState) { //&amp;&amp;&amp; 状态切换事件发布，本部分忽略，参考第四部分 publishStateTransitionEvent(StateTransition.transitionBetween(previousState.getState(), newState)); } } //代码块8里的isCallPermitted方法，这个方法决定了是否抛出\"已熔断\"异常 @Override public boolean isCallPermitted() { //可以看到，这个解决取决于对应XXState里isCallPermitted方法的返回结果 boolean callPermitted = stateReference.get().isCallPermitted(); if (!callPermitted) { //&amp;&amp;&amp; 已熔断异常事件发布，本部分忽略，参考第四部分 publishCallNotPermittedEvent(); } return callPermitted; } //代码块8里的onError方法，业务处理错误后会触发这个方法的调用 @Override public void onError(long durationInNanos, Throwable throwable) { //这个判断是过滤需要忽略的异常处理，一般情况下没配置的话所有异常都会走下面实际的onError逻辑 if (circuitBreakerConfig.getRecordFailurePredicate().test(throwable)) { //&amp;&amp;&amp; 处理错误事件发布，参考第四部分 publishCircuitErrorEvent(name, durationInNanos, throwable); //可以看到，实际上onError也是调用的XXState里的onError方法 stateReference.get().onError(throwable); } else { //&amp;&amp;&amp; 命中了可忽略的异常，忽略错误事件发布，本部分忽略，参考第四部分 publishCircuitIgnoredErrorEvent(name, durationInNanos, throwable); } } //代码块8里的onSuccess方法，业务处理正常会触发这个方法的调用 @Override public void onSuccess(long durationInNanos) { //&amp;&amp;&amp; 处理正常事件发布，本部分忽略，参考第四部分 publishSuccessEvent(durationInNanos); //同样的，onSuccess也是调用的XXState里的onError方法 stateReference.get().onSuccess(); }} 3.3：状态类通过上面的代码可以知道isCallPermitted、onSuccess、onError这三个方法实际上都是调用对应XXState对象里的方法，下面来看下ClosedState、OpenState、HalfOpenState这三个状态对象里有关这三个方法的实现（因为上面的测试用例只涉及这三种状态的互转，实际上这三种状态也是最常用的，所以为了避免混乱，只展示这三种，所有状态类均继承自CircuitBreakerState抽象类） 3.3.1：ClosedState闭合状态时初始状态，中途只会由半熔断状态切换而来，正常情况下都是闭合状态，代码如下： 代码块101234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556final class ClosedState extends CircuitBreakerState { //用来度量错误率的对象 private final CircuitBreakerMetrics circuitBreakerMetrics; //就是配置里的failureRateThreshold属性，闭合状态时的错误率阈值（第二部分的测试用例中是50） private final float failureRateThreshold; //参考代码块9的CircuitBreakerStateMachine构造器中初始化stateReference时，初始态都是闭合状态，最初都是通过该方法完成初始化的 ClosedState(CircuitBreakerStateMachine stateMachine) { this(stateMachine, null); } //这个构造器是状态转换时触发的，参考代码块9里的transitionToClosedState方法 ClosedState(CircuitBreakerStateMachine stateMachine, CircuitBreakerMetrics circuitBreakerMetrics) { super(stateMachine); //拿到熔断器的配置 CircuitBreakerConfig circuitBreakerConfig = stateMachine.getCircuitBreakerConfig(); if(circuitBreakerMetrics == null){ //初始化metrics对象，传进去的是闭合状态时计算错误率的单位请求数（第二部分的测试用例中是100） this.circuitBreakerMetrics = new CircuitBreakerMetrics( circuitBreakerConfig.getRingBufferSizeInClosedState()); }else{ //中途进行状态转换，调用的都是这里的逻辑，利用circuitBreakerMetrics的copy方法，重新赋值给circuitBreakerMetrics属性，暂时忽略，参考第3.4部分 this.circuitBreakerMetrics = circuitBreakerMetrics.copy(circuitBreakerConfig.getRingBufferSizeInClosedState()); } //赋值错误率阈值 this.failureRateThreshold = stateMachine.getCircuitBreakerConfig().getFailureRateThreshold(); } @Override boolean isCallPermitted() { //闭合状态下返回true，不会触发降级逻辑（ps：只有在全熔断状态下才会返回true） return true; } @Override void onError(Throwable throwable) { // 闭合状态下，onerror需要记录错误率，注：circuitBreakerMetrics的onError方法会记录一笔错误的记录，并把当前的错误率返回 checkFailureRate(circuitBreakerMetrics.onError()); } @Override void onSuccess() { // 闭合状态下，onerror需要记录成功数，注：circuitBreakerMetrics的onSuccess方法会记录一笔正确的记录，并把当前的错误率返回 checkFailureRate(circuitBreakerMetrics.onSuccess()); } //根据当前的错误率，决定是否切到半熔断状态 private void checkFailureRate(float currentFailureRate) { if (currentFailureRate &gt;= failureRateThreshold) { //这里判断当前错误率是否超过阈值 // 利用CircuitBreakerStateMachine的transitionToOpenState方法，将状态对象转换成OpenState stateMachine.transitionToOpenState(); } }} 3.3.2：OpenState一般全熔断状态会从闭合或者半熔断状态里切换而来，它的代码如下： 代码块111234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253final class OpenState extends CircuitBreakerState { //根据全熔断持续时间推出的进入半熔断状态的时间 private final Instant retryAfterWaitDuration; //同样是用来度量错误率的对象，该对象就是上一个State对象里的Metrics对象 private final CircuitBreakerMetrics circuitBreakerMetrics; OpenState(CircuitBreakerStateMachine stateMachine, CircuitBreakerMetrics circuitBreakerMetrics) { super(stateMachine); //就是配置里的waitDurationInOpenState属性，全熔断持续时间（第二部分的测试用例中是100ms） final Duration waitDurationInOpenState = stateMachine.getCircuitBreakerConfig().getWaitDurationInOpenState(); //当前时间加上持续时间，就是切换至半熔断状态的时机 this.retryAfterWaitDuration = Instant.now().plus(waitDurationInOpenState); //直接用之前的circuitBreakerMetrics对象 this.circuitBreakerMetrics = circuitBreakerMetrics; //如果配置了自动切换半熔断状态的开关为true，则会发起一个延时任务，用来主动切换状态 if (stateMachine.getCircuitBreakerConfig().isAutomaticTransitionFromOpenToHalfOpenEnabled()) { AutoTransitioner.scheduleAutoTransition(stateMachine::transitionToHalfOpenState, waitDurationInOpenState); } } @Override boolean isCallPermitted() { // 如果全熔断状态持续时间超出目标范围，则认为现在可以切换为半熔断状态，然后返回true if (Instant.now().isAfter(retryAfterWaitDuration)) { stateMachine.transitionToHalfOpenState(); return true; } circuitBreakerMetrics.onCallNotPermitted(); //记录一次NotPermitted（简单的累加） return false; //全熔断状态，直接返回false，表示已被熔断，让调用方抛出CircuitBreakerOpenException异常 } @Override void onError(Throwable throwable) { //理论上处于全熔断状态，isCallPermitted返回false，onError不会被触发（参考代码块8里的decorateCallable方法） //但是存在一种特殊的情况，假设有俩线程，线程1执行的时候还是闭合状态，isCallPermitted返回true，这时线程2里触发了熔断阈值 //线程2把stateReference的指向置为OpenState，这时线程1继续往下执行，触发的onError其实是OpenState里的onError（也即是本例中的这个方法） //全熔断状态下即便是上面这种临界情况发生，这次失败也会被统计上去 circuitBreakerMetrics.onError(); } /** * Should never be called when isCallPermitted returns false. */ @Override void onSuccess() { //跟onError一样，有概率会访问到 circuitBreakerMetrics.onSuccess(); }} 3.3.3：HalfOpenState半熔断状态一定是由全熔断切换出来的，来看下它的代码： 代码块1212345678910111213141516171819202122232425262728293031323334353637383940414243444546474849final class HalfOpenState extends CircuitBreakerState { //同样是用来度量错误率的对象 private CircuitBreakerMetrics circuitBreakerMetrics; //同样是配置里的failureRateThreshold属性 private final float failureRateThreshold; HalfOpenState(CircuitBreakerStateMachine stateMachine) { super(stateMachine); CircuitBreakerConfig circuitBreakerConfig = stateMachine.getCircuitBreakerConfig(); //初始化度量对象，相比闭合状态，这里传入的是ringBufferSizeInHalfOpenState（第二部分的测试用例中是10） this.circuitBreakerMetrics = new CircuitBreakerMetrics( circuitBreakerConfig.getRingBufferSizeInHalfOpenState()); //闭合状态和半开状态共用同一个错误率阈值（第二部分的测试用例中是50） this.failureRateThreshold = stateMachine.getCircuitBreakerConfig().getFailureRateThreshold(); } @Override boolean isCallPermitted() { //跟闭合状态一样，返回true return true; } @Override void onError(Throwable throwable) { // 跟闭合状态一样，要记录和判断当前的错误率（来决定是恢复闭合状态还是进入全熔断状态） checkFailureRate(circuitBreakerMetrics.onError()); } @Override void onSuccess() { // 同上 checkFailureRate(circuitBreakerMetrics.onSuccess()); } //通过该方法，判断错误率，决定是否恢复为闭合状态或者再次进入全熔断状态 private void checkFailureRate(float currentFailureRate) { //Metrics返回-1表示请求量表示还没有达到单位请求量（ringBufferSizeInHalfOpenState） //下面的逻辑可以看出，在半熔断状态下，经过ringBufferSizeInHalfOpenState次请求后根据错误率判断，就可以决定出下一步切换到哪个状态了 if (currentFailureRate != -1) { //当前错误率如果再次超出阈值，则再次进入全熔断状态 if (currentFailureRate &gt;= failureRateThreshold) { stateMachine.transitionToOpenState(); } else { //否则恢复为闭合状态 stateMachine.transitionToClosedState(); } } }} 3.3.4：状态间的切换关系上面三种状态的切换关系如下： 在这些状态中，最初为熔断闭合状态，ServerB的所有请求正常访问ServerC，ServerC报错，错误率累计达到50%后触发熔断全开状态，此时Server对ServerC发出的请求将走ServerB的降级逻辑，不再实际访问ServerC的方法，这个状态会持续waitDurationInOpenState这么久（测试用例中是1000ms），然后进入熔断半开状态，此时跟闭合状态一样，ServerB的所有请求仍会正常访问ServerC，不同的是半开状态下只需要满足ringBufferSizeInHalfOpenState次调用（测试用例中是10次），就可以直接判断错误率是否达到阈值，这点可以在代码块12里的checkFailureRate方法体现，图5中可以看到，如果未达到错误阈值表示ServerC已恢复，则可以关闭熔断，否则再次进入全熔断状态。 3.3.5：度量对象（CircuitBreakerMetrics）的传递这个对象在3.4中会详细说明，目前只需要知道该类用于做错误统计用，错误率计算的核心，核心方法为onError和onSuccess，这俩方法用于错误/正确请求的触发点，用于触发CircuitBreakerMetrics对象对错误率的统计。 通过代码块10、11、12可以看到CircuitBreakerMetrics对象的流向，首先初始化的时候是调用ClosedState第一个构造器触发第二个构造器，第二个构造器会new一个CircuitBreakerMetrics，传过去的size为ringBufferSizeInClosedState，然后由ClosedState切换至OpenState状态时，其CircuitBreakerMetrics会被传递给OpenState对象，根据代码块11可以知道，OpenState利用该对象统计熔断期间被熔断的次数，然后OpenState切换至HalfOpenState时，HalfOpenState没有接受CircuitBreakerMetrics对象的构造器，不管由谁切换到半开状态，CircuitBreakerMetrics对象都是全新的，由代码块12可知，初始化CircuitBreakerMetrics对象时传过去的size就是ringBufferSizeInHalfOpenState。 CircuitBreakerMetrics对象的传递以及传递后在State对象里所做的操作： 图6根据代码块10、11、12画出，简单体现了Metrics对象的生成以及流向，以及这个对象在各State对象里所做的主要操作。通过图6可以看出实际产生新的Metrics对象的地方为闭合态和半开态，因为这俩地方是需要做错误统计的，需要全新的Metric对象，全开态下仅接收前一状态的Metrics对象，在命中熔断后对其内部numberOfNotPermittedCalls（不是很懂这个属性，简单的累加，连用到的地方都没，可能仅仅是做个熔断数统计让业务方获取的吧，做监控可以用），在半开态再次进入闭合态时，其Metrics仍然被传递给了闭合态，由代码块10可知，如果传了Metrics对象，闭合态在产生新的Metrics对象时，会通过copy方法来产生，这个方法在3.4会详细说明，简单来说就是把前一个状态（只可能是半开态）的Metrics里的请求计数同步到它自己的Metrics里，这样做有一个好处，就是新的闭合态不用重新累计错误率了，以单元测试所配的参数试想一下，如果在半开态下，进行了10次请求，发生了4次错误，此时会切回闭合态，闭合态copy了这10次请求的数据，那么只需要再经过90次请求和46次错误便可以再次进入全熔断状态（其实就是保证了状态的平滑切换，不丢失之前已经统计了的数据）。 3.4：错误统计3.4.1：CircuitBreakerMetrics通过3.3的了解，闭合和半开时的请求状态计数都是通过CircuitBreakerMetrics对象来完成的，现在来看下这个类里都干了些什么： 代码块1312345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485class CircuitBreakerMetrics implements CircuitBreaker.Metrics { //通过3.3的代码块可知，该值就是闭合或者半开状态下设置的ringBufferSizeInClosedState和ringBufferSizeInHalfOpenState //表示一次请求窗口的大小，测试用例中就是闭合时的100以及半开时的10，通过图4和下方的getFailureRate方法可以知道， //至少要累计完成一个请求窗口的请求量后才会实际计算错误率 private final int ringBufferSize; //实际用来记录一个请求窗口的请求统计数据的结构，本节不深究，详细参考3.4.2 private final RingBitSet ringBitSet; //全开状态下累计被熔断的请求个数，触发点参考图6以及代码块11 private final LongAdder numberOfNotPermittedCalls; //构造器1，参考图6，在最开始的闭合状态以及后续的半开状态下初始化Metrics对象用的就是该构造器 CircuitBreakerMetrics(int ringBufferSize) { this(ringBufferSize, null); } //参考图6，由半开转到闭合态的时候，是通过该方法进行初始化的 public CircuitBreakerMetrics copy(int targetRingBufferSize) { return new CircuitBreakerMetrics(targetRingBufferSize, this.ringBitSet); //这里会把当前Metrics对象里的ringBitSet传递下去 } //构造器2 CircuitBreakerMetrics(int ringBufferSize, RingBitSet sourceSet) { this.ringBufferSize = ringBufferSize; if(sourceSet != null) { //通过copy初始化会走这里（每次的半开态转闭合态），将原来Metrics对象里的ringBitSet传递下去（用来初始化新的请求窗口） this.ringBitSet = new RingBitSet(this.ringBufferSize, sourceSet); }else{ //非copy新建Metrics对象（每次的半开态和最初的闭合态） this.ringBitSet = new RingBitSet(this.ringBufferSize); } this.numberOfNotPermittedCalls = new LongAdder(); } //onError和onSuccess的触发点参考3.3里的State类 //当请求发生错误时触发该方法，该方法用于记一次失败，然后把当前错误率返回 float onError() { int currentNumberOfFailedCalls = ringBitSet.setNextBit(true); //通过ringBitSet的setNextBit置为true，算作一笔失败的记录 return getFailureRate(currentNumberOfFailedCalls); } //当请求正常时触发该方法，该方法用于记一次成功，然后把当前错误率返回 float onSuccess() { int currentNumberOfFailedCalls = ringBitSet.setNextBit(false); //通过ringBitSet的setNextBit置为false，算作一笔成功的记录 return getFailureRate(currentNumberOfFailedCalls); } //全开状态下累计被熔断的请求个数 void onCallNotPermitted() { numberOfNotPermittedCalls.increment(); } //通过getFailureRate计算错误率并返回 @Override public float getFailureRate() { return getFailureRate(getNumberOfFailedCalls()); } //下方注释中的窗口大小就是ringBufferSize属性 //该方法通过ringBitSet对象返回当前请求窗口内发生请求的总次数，如果达到了ringBufferSize次，则这个值就恒等于ringBufferSize @Override public int getNumberOfBufferedCalls() { return this.ringBitSet.length(); } //该方法通过ringBitSet对象返回当前请求窗口内发生错误的次数 @Override public int getNumberOfFailedCalls() { return this.ringBitSet.cardinality(); } //错误率计算方法 private float getFailureRate(int numberOfFailedCalls) { //若请求还没有完成一个请求窗口，则返回-1 if (getNumberOfBufferedCalls() &lt; ringBufferSize) { return -1.0f; } //完成了一次请求窗口，才会真正计算错误率 return numberOfFailedCalls * 100.0f / ringBufferSize; }} 通过上面的代码可以知道最终统计错误数的是在RingBitSet结构中，下面来仔细了解下这个类~ 3.4.2：位图&amp;BitSetMod了解RingBitSet之前，先来了解一种数据结构-位图，如果已经了解过位图，那么可以直接去看RingBitSet。 RingBitSet持有一个BitSetMod对象，BitSetMod基于位图实现，位图是怎样的一种结构呢？先看下图7，然后再去解析它的源码实现。 通过上图可知，位图就是利用数组内每个元素的bit位存入一个标记，标记只有存在或者不存在（对应二进制的0和1），这样就可以做到用一个long型的数字就可以产生出64个标记信息，非常适合数据量庞大而判断状态少的应用场景，比如判断一个词语是否是屏蔽词，首先屏蔽词状态只有两种：命中or不命中，但是屏蔽词可能是个非常庞大的集合，如果一个个拿来比较，效率完全保证不了，那么就可以利用这个数据结构来解决这类问题，可以首先把所有的屏蔽词放到一个位图结构里，如果有相同的词语，只需要简单的两部运算就可以拿到是否命中结果，构建这个位图结构的过程如下： 通过上图，屏蔽词位图结构就构建好了，如果有个词语需要判定是否命中屏蔽词，只需要让这个词语通过上面的哈希算法计算出哈希值，然后找到对应的数组下标，通过位运算算出其所在位置，将该位置的值取出，如果是0，则认为没有命中，1则认为命中。 以上就是位图结构，通过上面的例子，可以认为同一个值一定命中位图里的同一个位置，那么抽象成熔断器的错误率，错误状态只有0和1，1表示错误，0表示正确，给每次请求编号，当成是图8中的哈希值，相同编号的请求一定会落到同一个位置，现在不理解没关系，这个要结合RingBitSet一起看，目前只需要理解位图特性即可。 Resilience4j里通过BitSetMod简单实现了一个位图结构，来看下代码（注：代码里有大量位运算，过程说明都写在了注释里）： 代码块14123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125/** * 下方为源码注释↓↓ * {@link io.github.resilience4j.circuitbreaker.internal.BitSetMod} is simplified version of {@link java.util.BitSet}. * It has no dynamic allocation, expanding logic, boundary checks * and it's set method returns previous bit state. */public class BitSetMod { /** * 1.此类是一种怎样的数据结构？ * 根据原有注释，可知这是一个简易版的BitSet，即位图结构，可以通过图7更为直观的了解下该结构 * 由图7可知，位图分为x，y轴，y轴就是本类的long型的数组（words），其中内部每一个元素都包括64个bit，因此bit位横向扩展就是x轴（x轴大小恒等于64） * 如果要标记一个数字是否存在于图中，只需要先找到所属的y轴位置（即对应的words下标），然后再计算出它应该出现的x轴long型数字中哪个bit位， * 然后判断该bit位是否已被标记为true，若是，则返回已存在，否则返回不存在。 * &lt;p&gt; * &lt;p&gt; * 2.位运算 * 简单了解下本类中出现的位运算，任意两个数的乘法或除法都可以用&lt;&lt;（左移）或&gt;&gt;（右移）来表示 * 例： * a * b == a &lt;&lt; log2(b) * a / b == a &gt;&gt; log2(b) * 本例中的ADDRESS_BITS_PER_WORD属性，其实就是long型位数以2为底的对数，即log2(64) = 6 * 那么接下来代码中针对ADDRESS_BITS_PER_WORD的位运算就可以简单理解为乘以/除以64了 */ //long类型bit位的对数，即log2(64)＝6，利用该值可以进行简单的乘除法的位运算 private final static int ADDRESS_BITS_PER_WORD = 6; //最终可以存放的总位数，计算方式：words.length * 64（每个long型有64位，利用数组长度乘以位数，就计算出了位图的总位数） //用位运算表示为：words.length &lt;&lt; 6（右移表示乘法，右移6位表示乘以2^6，即words.length * 64） private final int size; //位图数组，long型，64个bit位 private final long[] words; //构造器，传入位图的容量大小 public BitSetMod(final int capacity) { //计算数组大小（即y轴大小），根据上面对位图的基本解释，可以知道，y轴是一个long型数组， //而每次一个数字进来，会首先找到y轴所属的位置，那么这个数组得多大才合适呢？我们知道x轴固定为64个， //也就是说正常情况下，任意数字进来后都会被分到某个y轴对应的long型数字里的某一位，那么y轴大小就很好推算了， //利用给出的容量大小（这个表示任意数最大时为多大），除以64进行平均分组，这样不管传的任意数为多大，始终都可以找到对应的[x,y]，且不会越界 int countOfWordsRequired = wordIndex(capacity - 1) + 1; //上面说过，size就是位图里所有位数，即x * y，也就是words.length * 64，用位运算表示为：words.length &lt;&lt; 6 size = countOfWordsRequired &lt;&lt; ADDRESS_BITS_PER_WORD; //最终((capacity - 1)/64)+1就是y轴数组大小，初始化数组即可 words = new long[countOfWordsRequired]; //到这里，一个位图对象就被我们创建好了，数组（y轴）是它实际的实体，x轴是数组里long型数字的二进制位（64） } private static int wordIndex(int bitIndex) { //下面这个位运算等同于：bitIndex/64 return bitIndex &gt;&gt; ADDRESS_BITS_PER_WORD; } //开始设置数字信息，bitIndex为目标放置位置，value为值（0或1） public int set(int bitIndex, boolean value) { // 利用位置数字除以64，推算出它对应的y轴下标 int wordIndex = wordIndex(bitIndex); // 注：下面的代码都是位运算，开始前先来了解一下如何定位某个数字的二进制第n位上的数字是0还是1 // 将1右移bitIndex位，可以得到一个类似1000000的二进制数字，利用这个数字跟原来的数字本身做位与运算，可以推算出原数第bitIndex位上的数字是1还是0 // 举个例子，我想知道下面这个二进制数字中第5位的数字是0还是1（跟十进制一样，位数是从右往左数，位数最高的在最左边，下标从0开始算起） // 假设该二进制数为λ，设：λ=101010101 // 现在将1右移5位得到bitMask，它用二进制表示为：100000，1的位置正好位于第5位（从右往左，下标从0算起） // 利用λ跟bitMask进行位与运算： // 101010101（λ） // &amp; // 000100000（bitMask） // ------------------ // 000000000（位与结果） // 由这个过程可以发现，λ的第5位如果是0，位与后的结果也是0，如果是1，那么位与运算后的结果肯定是不等于0的，通过这种方式，我们就可以利用1右移的方式， // 知道λ的第n位是0还是1 // // 通过上面的例子，可以知道，任意数与1右移后的数字（bitMask）进行位与运算的结果要么不等于0，要么等于0，因为1右移n位后生成的二进制数在其n位上一定为1， // 其余位置一定为0，0&amp;0、0&amp;1均为0，所以最后的结果要么是000000000，要么还等于1右移后的那个数：000010000，这取决于原始数字里第n位上是否是1， // 如果是1，则相与后的结果值一定不等于0，反之则等于0 // 结合上面所有的描述，这里可以再思考一个问题，为什么位不会相互覆盖？比如我传了一个bitIndex为100，long型1&lt;&lt;100等价于1&lt;&lt;36（以64为模轮回），那么当我传100的时候岂不是会覆盖掉传36时那次做标记？ // 这个问题答案是否定的，因为在最初的时候就已经把bitIndex按照64为单位进行相除计算出下标了，也就是说bitIndex等于100那次，跟bitIndex等于36那次，不在一个下标里（不在一个次元） // 根据这些规则，下面的代码就好理解了。 long bitMask = 1L &lt;&lt; bitIndex; int previous = (words[wordIndex] &amp; bitMask) != 0 ? 1 : 0; //把该位置上当前的值（0或1）赋值给previous（也就是最后返回出去的结果） if (value) { // 重新赋值，注意，这里是原值跟bitMask进行或运算，意味着目标位的值会直接变成1，其余位置的值均不变 words[wordIndex] = words[wordIndex] | bitMask; // 结合例子，参考下面这个过程更容易理解 // 101010101 // | // 000100000 // --------- // 101110101 } else { // value等于false的时候，bitMask取反后跟原值进行与运算，跟上面相反，这是把目标位变成0 words[wordIndex] = words[wordIndex] &amp; ~bitMask; // 结合例子，参考下面这个过程更容易理解 // 101010101 // &amp; // 111011111（bitMask的反码） // --------- // 101010101 } return previous; } int size() { //返回位图里的总位数 return size; } boolean get(int bitIndex) { // 注：如果对下方的右移等操作还不是很了解，请先看set方法里的注释 // 首先还是利用位置数字除以64，推算出它对应的y轴下标 int wordIndex = wordIndex(bitIndex); //如果set里的位运算理解了，下面这个很容易理解，这个流程跟set方法里获取previous一样 long bitMask = 1L &lt;&lt; bitIndex; return (words[wordIndex] &amp; bitMask) != 0; //大于0时返回true，表示目标位是1，否则返回false，目标位是0 }} 上面是Resilience4j针对位图的简单实现，它负责存储单位请求内的错误/成功标志。 3.4.3：RingBitSet之前说过，最终请求被放到了一个环形结构里才对，沿着环执行一周就是一次单位请求，回看下图4，其实第101次请求就是顶替掉第一次请求的结果罢了，现在把图4中以100为请求窗口弯曲成一个环，假如第一次请求是失败的，第101次请求是成功的（绿色背景表示成功的请求，红色背景表示失败的请求）： 如何利用位图结构记录每次请求的错误/成功标记然后再实现图9里的环形结构呢？Resilience4j通过RingBitSet来实现，来看下它的代码： 代码块1512345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697public class RingBitSet { //单位请求数，根据State类初始化RingSet时给的size值，可以确定该值就是各种ringBufferSize private final int size; //真正存放错误率的位图结构 private final BitSetMod bitSet; //在完成一个请求窗口后，该值为false，表示请求已满一次请求窗口 private boolean notFull; //给请求编号，方便位图计算位置 private int index = -1; //请求数量，最终等于size private volatile int length; //当前请求窗口内的错误数，就是利用这个数实时计算错误率的（参考CircuitBreakerMetrics.getNumberOfFailedCalls） private volatile int cardinality = 0; RingBitSet(int bitSetSize) { notFull = true; size = bitSetSize; bitSet = new BitSetMod(bitSetSize); } //携带RingBitSet参数的构造器会把sourceSet里的统计数据赋值给新的RingBitSet（继承其请求数、错误率等） //调用该构造器的触发点在CircuitBreakerMetrics.copy中触发，通过图6可知，每次由半开状态转到闭合状态时，都会调用copy方法， //让新的闭合态继承上次半开态的请求量和错误率，这是合理的，比较平滑无损的过度到闭合态。 RingBitSet(int bitSetSize, RingBitSet sourceSet) { this(bitSetSize); int targetLength = Integer.min(bitSetSize, sourceSet.length); int sourceIndex = sourceSet.index; int forwardIndex = sourceSet.size - sourceIndex; for (int i = 0; i &lt; targetLength; i++) { this.setNextBit(sourceSet.bitSet.get(sourceIndex)); // looping sourceIndex backwards without conditional statements forwardIndex = (forwardIndex + 1) % sourceSet.size; sourceIndex = (sourceSet.size - forwardIndex) % sourceSet.size; } } //非常非常重要的方法，它的触发点在CircuitBreakerMetrics的onError和onSuccess，主要用于记录错误率 public synchronized int setNextBit(boolean value) { increaseLength(); //环形结构依靠这里来实现，index永远在0~size间循环累加，类似：[0,1,2,3...99,0,1,2,3...99] index = (index + 1) % size; //利用位图，将本次的错误/成功标记设置到对应index的位置上， //并且拿到当前index对应上次请求窗口中同样为index位置的请求结果previous，至于为啥要拿到这个值，参考下方的逻辑 int previous = bitSet.set(index, value); //本次请求结果 int current = value ? 1 : 0; //下面这一步就是刷新错误数的，计算方式为：减去同位置上个请求窗口的请求结果，然后加上这次的请求结果 //举个例子，假设单位请求窗口是100，第一个请求窗口的第一次请求错误，index=0的位置被标为1，第101次请求，也就是第二个请求窗口的第一次请求， //意味着index仍然为0，那么第101次请求的结果就会覆盖掉第1次请求的那个结果，以此来完成窗口滚动（参考图9） cardinality = cardinality - previous + current; return cardinality; } //返回当前请求窗口内的错误总量 public int cardinality() { return cardinality; } public int size() { return bitSet.size(); } public int length() { return length; } @Override public String toString() { StringBuilder result = new StringBuilder(); for (int i = 0; i &lt; size; i++) { result.append(bitSet.get(i) ? '1' : '0'); } return result.toString(); } synchronized int getIndex() { return index; } //累加当前请求窗口内的请求量，当完成一次单位请求窗口时，length恒等于单位请求窗口大小（size） private void increaseLength() { if (notFull) { int nextLength = length + 1; if (nextLength &lt; size) { length = nextLength; } else { length = size; notFull = false; } } }} 四、总结Resilience4j通过CircuitBreakerStateMachine来独立出一个熔断器，其内部持有一个CircuitBreakerState对象的引用，在错误率达到某个阈值时，会发生状态切换，CircuitBreakerState的引用会指向新的状态对象。每个状态对象持有一个CircuitBreakerMetrics对象，用于做实时统计和错误率监听使用，CircuitBreakerMetrics对象通过RingBitSet来完成单位请求窗口的错误率统计，这个统计是实时的，每次请求都会触发一次错误率的判断。RingBitSet通过Resilience4j自己实现的一个轻量级的位图结构BitSetMod来标记请求错误/成功，顺便说下，这里通过RingBitSet来保证环形结构，而位图只负责存储请求结果，那么既然这样，我用普通的数组或者其他的可以通过下标获取数值的集合结构也可以实现啊，为什么一定要用位图呢？猜测是位图既可以保证跟数组一样高效，都是O(1)的复杂度，又可以节省存储空间，比如我的单位请求是1w次，如果是数组结构，虽然效率跟位图一样高，但是数组却需要存1w个0或1这样的数组，即便用byte类型的数组，每个数组元素都浪费了7个bit位。其他集合就更不用说了，效率无法保证，其次他们浪费的内存比单纯数组要高，所以，类似这种只有true或false的数据的存储，位图再适合不过了。 感觉有些地方说的不太清晰，待后续改进描述方式。","link":"/2019/12/10/Resilience4j%E7%86%94%E6%96%AD%E5%99%A8-%E4%BD%BF%E7%94%A8%E4%B8%8E%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/"},{"title":"工作中有关分布式缓存的使用和需要注意的问题梳理","text":"目前工作中用到的分布式缓存技术有redis和memcached两种，缓存的目的是为了在高并发系统中有效降低DB的压力，但是在使用的时候可能会因为缓存结构设计不当造成一些问题，这里会把可能遇到的坑整理出来，方便日后查找。 一、常用的两种缓存技术的服务端特点1.1：Memcache服务端Memcache（下面简称mc）服务端是没有集群概念的，所有的存储分发全部交由mc client去做，我这里使用的是xmemcached，这个客户端支持多种哈希策略，默认使用key与实例数取模来进行简单的数据分片，这种分片方式会导致一个问题，那就是新增或者减少节点后会在一瞬间导致大量key失效，最终导致缓存雪崩的发生，给DB带来巨大压力，所以我们的mc client启用了xmemcached的一致性哈希算法来进行数据分片： 代码块11234567XMemcachedClientBuilder builder = new XMemcachedClientBuilder(AddrUtil.getAddresses(servers));builder.setOpTimeout(opTimeout);builder.setConnectTimeout(connectTimeout);builder.setTranscoder(transcoder);builder.setConnectionPoolSize(connectPoolSize);builder.setKeyProvider(keyProvider);builder.setSessionLocator(new KetamaMemcachedSessionLocator()); //启用ketama一致性哈希算法进行数据分片 根据一致性哈希算法的特性，在新增或减少mc的节点只会影响较少一部分的数据。但这种模式下也意味着分配不均匀，新增的节点可能并不能及时达到均摊数据的效果，不过mc采用了虚拟节点的方式来优化原始一致性哈希算法（由ketama算法控制实现），实现了新增物理节点后也可以均摊数据的能力。 最后，mc服务端是多线程处理模式，mc一个value最大只能存储1M的数据，所有的k-v过期后不会自动移除，而是下次访问时与当前时间做对比，过期时间小于当前时间则删除，如果一个k-v产生后就没有再次访问了，那么数据将会一直存在在内存中，直到触发LRU。 1.2：Redis服务端redis服务端有集群模式，key的路由交由redis服务端做处理，除此之外redis有主从配置以达到服务高可用。 redis服务端是单线程处理模式，这意味着如果有一个指令导致redis处理过慢，会阻塞其他指令的响应，所以redis禁止在生产环境使用重量级操作（例如keys，再例如缓存较大的值导致传输过慢） redis服务端并没有采用一致性哈希来做数据分片，而是采用了哈希槽的概念来做数据分片，一个redis cluster整体拥有16384个哈希槽（slot），这些哈希槽按照编号区间的不同，分布在不同节点上，然后一个key进来，通过内部哈希算法（CRC16(key)）计算出槽位置，然后将数据存放进对应的哈希槽对应的空间，redis在新增或者减少节点时，其实就是对这些哈希槽进行重新分配，以新增节点为例，新增节点意味着原先节点上的哈希槽区间会相对缩小，被减去的那些哈希槽里的数据将会顺延至下一个对应节点，这个过程由redis服务端协调完成，过程如下： 迁移过程是以槽为单位，将槽内的key按批次进行迁移的（migrate）。 有关哈希槽和一致性哈希算法的对比参考：一致性哈希和哈希槽对比 二、选型问题2.1：缓存结构化选型mc提供简单的k-v存储，value最大可以存储1M的数据，多线程处理模式，不会出现因为某次处理慢而导致其他请求排队等待的情况，适合存储数据的文本信息。 redis提供丰富的数据结构，服务端是单线程处理模式，虽然处理速度很快，但是如果有一次查询出现瓶颈，那么后续的操作将被阻塞，所以相比k-v这种可能因为数据过大而导致网络交互产生瓶颈的结构来说，它更适合处理一些数据结构的查询、排序、分页等操作，这些操作往往复杂度不高，且耗时极短，因此不太可能会阻塞redis的处理。 使用这两种缓存服务来构建我们的缓存数据，目前提倡所有数据按照标志性字段（例如id）组成自己的信息缓存存储，这个一般由mc的k-v结构来完成存储。而redis提供了很多好用的数据结构，一般构建结构化的缓存数据都使用redis的数据结构来保存数据的基本结构，然后组装数据时根据redis里缓存的标志性字段去mc里查询具体数据，例如一个排行榜接口的获取： 上图redis提供排行榜的结构存储，排行榜里存储的是id和score，通过redis可以获取到结构内所有信息的id，然后利用获得的id可以从mc中查出详细信息，redis在这个过程负责分页、排序，mc则负责存储详细信息。 上面是比较合适的缓存做法，建议每条数据都有一个自己的基本缓存数据，这样便于管理，而不是把一个接口的巨大结构完全缓存到mc或者redis里，这样划分太粗，日积月累下来每个接口或者巨大方法都有一个缓存，key会越来越多，越来越杂。 2.2：Redis的数据结构常用操作和时间复杂度：Redis基础、常用类型介绍、时间复杂度 特殊结构BitMap：Redis中BitMap使用 BitMap适用于实现用户签到、在线状态等功能，空间占有量极低，除此之外还可以用它来实现一个BloomFilter。 在使用redis时，对于时间复杂度非O(1)的操作都需要推算现有的数据量是否可能导致redis阻塞（分页操作一般不会命中这个问题，因为单页个数有限，出问题较多的一般都是全量输出大列表导致的，生产环境应避免全量操作）。 2.3：Redis构造大索引回源问题Redis如果做缓存使用，始终会有过期时间存在，如果到了过期时间，使用redis构建的索引将会消失，这个时候回源，如果存在大批量的数据需要构建redis索引，就会存在回源方法过慢的问题，这里以某个评论系统为例： 评论系统采用有序集合作为评论列表的索引，存储的是评论id，用于排序的score值则按照排序维度拆分，比如发布时间、点赞数等，这也意味着一个资源下的评论列表根据排序维度不同存在着多个redis索引列表，而具体评论内容存mc，正常情况下结构如下： 上面是正常触发一个资源的评论区，每次触发读缓存，都会顺带延长一次缓存的过期时间，这样可以保证较热的内容不会轻易过期，但是如果一个评论区时间过长没人访问过，redis索引就会过期，如果一个评论区有数万条评论数据，长时间没人访问，突然有人过去考古，那么在回源构建redis索引时会很缓慢，如果没有控制措施，还会造成下面缓存穿透的问题，从而导致这种重量级操作反复被多个线程执行，对DB造成巨大压力。 对于上面这种回源构建索引缓慢的问题，处理方式可以是下面这样： 相比直接执行回源方法，这种通过消息队列构造redis索引的方法更加适合，首先仅构建单页或者前面几页的索引数据，然后通过队列通知job（这里可以理解为消费者）进行完整索引构造，当然，这只适合对一致性要求不高的场景。 三、一致性问题一般情况下缓存内的数据要和数据库源数据保持一致性，这就涉及到更新DB后主动失效缓存策略（通俗叫法：清缓存），大部分会经过如下过程： 假如现在有两个服务，服务A和服务B，现在假设服务A会触发某个数据的写操作，而服务B则是只读程序，数据被缓存在一个Cache服务内，现在假如服务A更新了一次数据库，那么结合上图得出以下流程： 服务A触发更新数据库的操作 更新完成后删除数据对应的缓存key 只读服务（服务B）读取缓存时发现缓存miss 服务B读取数据库源信息 写入缓存并返回对应信息 这个过程乍一看是没什么问题的，但是往往多线程运转的程序会导致意想不到的结果，现在来想象下服务A和服务B被多个线程运行着，这个时候重复上述过程，就会存在一致性问题： 3.1：并发读写导致的一致性问题 运行着服务A的线程1首先修改数据，然后删除缓存 运行着服务B的线程3读缓存时发现缓存miss，开始读取DB中的源数据，需要注意的是这次读出来的数据是线程1修改后的那份 这个时候运行着服务A的线程2上线，开始修改数据库，同样的，删除缓存，需要注意的是，这次删除的其实是一个空缓存，没有意义，因为本来线程3那边还没有回源完成 运行着服务B的线程3将读到的由线程1写的那份数据回写进Cache 上述过程完成后，最终结果就是DB里保存的最终数据是线程2写进去的那份，而Cache经过线程3的回源后保存的却是线程1写的那份数据，不一致问题出现。 3.2：主从同步延时导致的一致性问题这种情况要稍微修改下程序的流程图，多出一个从库： 现在读操作走从库，这个时候如果在主库写操作删除缓存后，由于主从同步有可能稍微慢于回源流程触发，回源时读取从库仍然会读到老数据。 3.3：缓存污染导致的一致性问题每次做新需求时更新了原有的缓存结构，或去除几个属性，或新增几个属性，假如新需求是给某个缓存对象O新增一个属性B，如果新逻辑已经在预发或者处于灰度中，就会出现生产环境回源后的缓存数据没有B属性的情况，而预发和灰度时，新逻辑需要使用B属性，就会导致生产&amp;预发缓存污染。过程大致如下： 四、应对缓存一致性问题4.1：binlog+消息队列+消费者del cache 上图是现在常用的清缓存策略，每次表发生变动，通过mysql产生的binlog去给消息队列发送变动消息，这里监听DB变动的服务由canal提供，canal可以简单理解成一个实现了mysql通信协议的从库，通过mysql主从配置完成binlog同步，且它只接收binlog，通过这种机制，就可以很自然的监听数据库表数据变动了，可以保证每次数据库发生的变动，都会被顺序发往消费者去清除对应的缓存key。 4.2：从库binlog+消息队列+消费者del cache上面的过程能保证写库时清缓存的顺序问题，看似并没有什么问题，但是生产环境往往存在主从分离的情况，也就是说上面的图中如果回源时读的是从库（参考问题3.2），那上面的过程仍然是存在一致性问题的： 从库延迟导致的脏读问题，如何解决这类问题呢？只需要将canal监听的数据库设置成从库即可，保证在canal推送过来消息时，所有的从库和主库完全一致，不过这只针对一主一从的情况，如果一主多从，且回源读取的从库有多个，那么上述也是存在一定的风险的（一主多从需要订阅每个从节点的binlog，找出最后发过来的那个节点，然后清缓存，确保所有的从节点全部和主节点一致）。 不过，正常情况下，从库binlog的同步速度都要比canal发消息快，因为canal要接收binlog，然后组装数据变动实体（这一步是有额外开销的），然后通过消息队列推送给各消费者（这一步也是有开销的），所以即便是订阅的master库的表变更，出问题的概率也极小。 ps：目前笔者公司内数据库都是一主一从，canal监听的都是从库的变动。 4.3：更新后key升级针对上面的一致性问题3.3（缓存污染），修改某个缓存结构可能导致在预发或者灰度中状态时和实际生产环境的缓存相互污染，这个时候建议每次更新结构时都进行一次key升级（比如在原有的key名称基础上加上_v2的后缀）。 ⚡⚡⚡binlog是否真的是准确无误的呢？⚡⚡⚡ 并不是，比如上面的情况： 首先线程1走到服务A，写DB，发binlog删除缓存 然后线程3运行的服务B这时cache miss，然后读取DB回源（这时读到的数据是线程1写入的那份数据） 此时线程2再次触发服务A写DB，同样发送binlog删除缓存 最后线程3把读到的数据写入cache，最终导致DB里存储的是线程2写入的数据，但是cache里存储的却是线程1写入的数据，不一致达成 这种情况比较难以触发，因为极少会出现线程3那里写cache的动作会晚于第二次binlog发送的，除非在回源时做了别的带有阻塞性质的操作，所以根据现有的策略，针对问题3.1，没有特别完美的解决方案，只能尽可能保证一致性，但由于实际生产环境就像问题3.1里那样，处于多线程并发读写的环境，即便有binlog做最终的保证，也不能保证最后回源方法写缓存那里的顺序性。除非回源全部交由binlog消费者来做，不过这本就不太现实，这样等于说服务B没有回源方法了。 针对这个问题，出现概率最大的就是那种写并发概率很大的情况，这个时候伴随而来的还有命中率（参考五）问题。 4.4：一致性问题解决总结没有很好的避免问题3.1的方法，从库binlog订阅+清缓存可以保证在清缓存后服务回源读到的一定是最新数据，但是这并不妨碍图11中的问题发生，只是概率极小，在高并发写+读回源时没有什么办法是可以保证完全没有问题的，可以加分布式锁控制回源方法（这样出问题的概率就更加低了）。当然，只有在这种大量写操作的情况下，才会使问题3.1出现的概率提升，一般这种大量写的还会有命中率低的问题，所以这类缓存的更新一般交给binlog消费者直接去做，在单线程处理模式下，一致性问题几乎不会出现。相比问题3.1里的直接delete缓存，binlog模式可以保证清缓存后缓存回源时读到的内容一定是最新的，极大的降低了因为主从配置导致的问题3.1、3.2出现的概率。 五、命中率问题通过前面的流程，抛开特殊因素，已经解决了一致性的问题，但随着清缓存而来的另一个问题就是命中率问题，比如一个数据变更过于频繁，以至于产生过多的binlog消息，这个时候每次都会触发消费者的清缓存操作，这样的话缓存的命中率会瞬间下降，导致大部分用户访问直接访问DB，而且这种频繁变更的数据还会加大问题①出现的概率，所以针对这种频繁变更的数据，不再删除缓存key，而是直接在binlog消费者那里直接回源更新缓存，这样即便表频繁变更，用户访问时每次都是消费者更新好的那份缓存数据，只是这时候消费者要严格按照消息顺序来处理，否则也会有写脏的危险，比如开两个线程同时消费binlog消息，线程1接收到了第一次数据变更的binlog，而线程2接收到了第二次数据变更的binlog，这时线程1读出数据（旧数据），线程2读出数据（新数据）更新缓存，然后线程1再执行更新，这时缓存又会被写脏，所以为了保证消费顺序，必须是单线程处理，如果想要启用多线程均摊压力，可以利用key、id等标识性字段做任务分组，这样同一个id的binlog消息始终会被同一个线程执行。 六、缓存穿透6.1：什么是缓存穿透？正常情况下用户请求一个数据时会携带标记性的参数（比如id），而我们的缓存key则会以这些标记性的参数来划分不同的cache value，然后我们根据这些参数去查缓存，查到就返回，否则回源，然后写入cache服务后返回，详细过程参考上述第三部分。 这个过程看起来也没什么问题，但是某些情况下，根据带进来的参数，在数据库里并不能找到对应的信息，这个时候每次带有这种参数的请求，都会走到数据库回源，这种现象叫做缓存穿透，比较典型的出现这种问题的情况有： ①恶意攻击或者爬虫，携带数据库里本就不存在的数据做参数回源 ②公司内部别的业务方调用我方的接口时，由于沟通不当或其他原因导致的参数大量误传 ③客户端bug导致的参数大量误传 6.2：如何解决缓存穿透问题？目前我们提倡的做法是回源查不到信息时直接缓存空数据（注意：空数据缓存的过期时间要尽可能小，防止无意义内容过多占用Cache内存），这样即便是有参数误传、恶意攻击等情况，也不会每次都打进DB。 但是目前这种做法仍然存在被攻击的风险，如果恶意攻击时携带少量参数还好，这样不存在的空数据缓存仅仅会占用少量内存，但是如果攻击者使用大量穿透攻击，携带的参数千奇百怪，这样就会产生大量无意义的空对象缓存，使得我们的缓存服务器内存暴增，这个时候就需要服务端来进行简单的控制：按照业务内自己的估算，合理的id大致在什么范围内，比如按照用户id做标记的缓存，就直接在获取缓存前判断所传用户id参数是否超过了某个阈值，超过直接返回空。（比如用户总量才几十万或者上百万，结果用户id传过来个几千万甚至几亿明显不合理的情况） 七、缓存击穿7.1：什么是缓存击穿？缓存击穿是指在一个key失效后，大量请求打进回源方法，多线程并发回源的问题。 这种情况在少量访问时不能算作一个问题，但是当一个热点key失效后，就会发生回源时涌进过多流量，全部打在DB上，这样会导致DB在这一时刻压力剧增。 7.2：如何解决缓存击穿？7.2.1：回源方法内追加互斥锁这个可以避免多次回源，但是n台实例群模式下，仍然会存在实例并发回源的情况，这个量级相比之前大量打进，已经大量降低了。 7.2.2：回源方法内追加分布式锁这个可以完全避免上面多实例下并发回源的情况，但是缺点也很明显，那就是又引入了一个新的服务，这意味着发生异常的风险会加大。 八、缓存雪崩8.1：什么是缓存雪崩？缓存雪崩是指缓存数据某一时刻出现大量失效的情况，所有请求全部打进DB，导致短期内DB负载暴增的问题，一般来说造成缓存雪崩有以下几种情况： 8.1.1：缓存服务扩缩容这个是由缓存的数据分片策略的而导致的，如果采用简单的取模运算进行数据分片，那么服务端扩缩容就会导致雪崩的发生。 8.1.2：缓存服务宕机某一时刻缓存服务器出现大量宕机的情况，导致缓存服务不可用，根据现有的实现，是直接打到DB上的。 8.2：如何避免雪崩的发生？8.2.1：缓存服务端的高可用配置*上面mc和redis的分片策略已经说过，所以扩缩容带来的雪崩几率很小，其次redis服务实现了高可用配置：启用cluster模式，一主一从配置。由于对一致性哈希算法的优化，mc宕机、扩缩容对整体影响不大，所以缓存服务器服务端本身目前是可以保证良好的可用性的，尽可能的避免了雪崩的发生（除非大规模宕机，概率很小）。 8.2.2：数据分片策略调整调整缓存服务器的分片策略，比如上面第一部分所讲的，给mc开启一致性哈希算法的分片策略，防止缓存服务端扩缩容后缓存数据大量不可用。 8.2.3：回源限流如果缓存服务真的挂掉了，请求全打在DB上，以至于超出了DB所能承受之重，这个时候建议回源时进行整体限流，被限到的请求紫自动走降级逻辑，或者直接报错。 九、热key问题9.1：什么是热key问题？了解了缓存服务端的实现，可以知道某一个确定的key始终会落到某一台服务器上，如果某个key在生产环境被大量访问，就导致了某个缓存服务节点流量暴增，等访问超出单节点负载，就可能会出现单点故障，单点故障后转移该key的数据到其他节点，单点问题依旧存在，则可能继续会让被转移到的节点也出现故障，最终影响整个缓存服务集群。 9.2：如何解决热key问题？9.2.1：多缓存副本预先感知到发生热点访问的key，生成多个副本key，这样可以保证热点key会被多个缓存服务器持有，然后回源方法公用一个，请求时按照一定的算法随机访问某个副本key： 9.2.2：本地缓存针对热点key外面包一层短存活期的本地缓存，用于缓冲热点服务器的压力。","link":"/2019/10/24/%E5%B7%A5%E4%BD%9C%E4%B8%AD%E6%9C%89%E5%85%B3%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98%E7%9A%84%E4%BD%BF%E7%94%A8%E5%92%8C%E9%9C%80%E8%A6%81%E6%B3%A8%E6%84%8F%E7%9A%84%E9%97%AE%E9%A2%98%E6%A2%B3%E7%90%86/"},{"title":"Druid-类图-属性表","text":"所属文章：池化技术（一）Druid是如何管理数据库连接的？ 本篇为「工具人」文章，建议直接用「ctrl+f」进行查找属性、方法、类名，快速了解其含义和所属类。 主要流程里主要涉及到的类名称、类属性、类方法如下图（淡黄色表示属性，淡蓝色表示方法）： DruidAbstractDataSource抽象类这个类是druid连接池基础类，定义了一些连接池该有的基本属性，以及生成驱动连接对象的方法。 属性表 属性名 说明 username 用户名 password 密码 jdbcUrl 驱动连接 driverClass 驱动class，可以不用配置 initialSize 初始化连接池（主流程2）时需要预先生成的连接对象个数 maxActive 池内最大连接数，也就是说生成连接的线程在当前池内连接数超过这个指标后就不再工作了，参考主流程3 minIdle 池内最小闲置连接数，参考流程4.1 maxWait 在业务线程拿不到可用连接而发生排队时，等待获取到可用连接对象的最大等待时间，参考主流程1和流程1.2 notFullTimeoutRetryCount 获取不到连接时，会尝试重试，这里表示最大重试次数，参考主流程1 testOnBorrow 在取出链接时，是否进行连接可用性测试，默认不开启，参考主流程1 testOnReturn 在回收连接时，是否进行连接可用性测试，默认不开启，参考主流程5 testWhileIdle 在闲置时间超出指定时间（timeBetweenEvictionRunsMillis）后进行连接可用性测试 timeBetweenEvictionRunsMillis 默认60s，一个连接闲置时间超出该值，且设置了testWhileIdle为true时，进行连接可用性测试 inited 是否已被初始化过，参考主流程2 filters（集合） 触发责任链执行时需要执行的所有filter maxWaitThreadCount 默认不开启（-1），表示取不到连接发生等待时阻塞的最大业务线程数，参考流程1.2 removeAbandoned 是否主动回收一些被拿出去使用长久没有归还的连接，默认不开启，参考流程4.2 removeAbandonedTimeoutMillis 表示在removeAbandoned开启的情况下，触发主动归还的时间间隔。 dbType 标记该连接池对象是属于什么数据库类型（根据驱动协议头推算出来） validConnectionChecker druid有多个验证长连接可用性的checker对象，该属性最终会根据数据库类型适配合适的checker对象，参考流程1.3里的init-checker lock 控制连接池线程安全的全局重入锁（参考全部流程里出现的lock） notEmpty 由lock创建的Condition，用于连接不够用时阻塞业务线程，同时唤起主流程3的守护线程追加连接，解释参考主流程2的特别说明第2条 empty 由lock创建的Condition，用于连接足够时（不发生线程等待），阻塞主流程3的守护线程，解释参考主流程2的特别说明第2条 activeConnectionLock 活动连接重入锁，被借出去的连接称为active连接，这类连接在removeAbandoned开启时会被保存进下面的activeConnections里，利用该锁完成对其操作的安全性，参考主流程1和流程4.2 activeConnections（k-v） 解释参考上面的描述，参考主流程1和流程4.2 表1-1 方法表 方法名 说明 testConnectionInternal 测试连接可用性的基本方法，参考流程1.3 createPhysicalConnection 新增真正的数据库物理连接，参考流程2.1 表1-2 DruidDataSource类这个类也是druid连接池基础类，扩展了一些其父类的功能，几乎所有的有关连接池管理的操作都在此类完成。 属性表 属性名 说明 connectCount 一共成功从该池获取了多少次连接（只要获取成功一次，就累加一次） recycleCount 一共成功归还了多少次连接到该池（成功归还一次，累加一次） removeAbandonedCount 在removeAbandoned开启的情况下，被检查后强制归还的连接数 connections（数组） 最终池子里没有被使用的闲置连接存放的地方，类型是DruidConnectionHolder poolingCount pollingCount就是指上面connections的真实数量 activeCount 当前处于借出状态的连接数（也即是被拿出去使用的连接数），poolingCount+activeCount就是当前该池子里一共有多少个连接，不能超过maxActive，参考主流程3 discardCount 被丢弃的连接数，触发丢弃的地方有很多，比如流程1.4、流程4.1 evictConnections（数组） 参考流程4.1 keepAliveConnections（数组） 参考流程4.1 createConnectionThread（线程） 生产连接的守护线程，参考主流程3 destroyConnectionThread（线程） 抛弃连接的守护线程，参考主流程4 logStatsThread（线程） 打印连接池各项监控指标日志的守护线程，参考主流程2，默认关闭 initedLatch（CountDownLatch） 倒计数器，用来保证createConnectionThread和destroyConnectionThread两个守护线程全部开启成功。 enable 连接池对象是否可用，在连接池整体close后，该值为false，表示已关闭的连接池不可用。 keepAlive 参考流程4.1 loadSpifilterSkip 是否启用通过SPI机制加载责任链上的filter，默认开启 表2-1 方法表 方法名 说明 init 初始化整个连接池，参考主流程2 initFromSPIServiceLoader 通过SPI机制加载责任链中的filters initValidConnectionChecker 初始化（适配）检测器，参考流程1.3中的init-checker createAndLogThread 启动上面表2-1里的logStatsThread线程 createAndStartCreatorThread 启动上面表2-1里的createConnectionThread线程 createAndStartDestroyThread 启动上面表2-1里的destroyConnectionThread线程 getConnection 获取连接方法，参考主流程1 getConnectionDirect 获取连接方法（通过getConnection触发），参考主流程1 pollLast 真正从池子里获取连接对象的方法（通过getConnectionDirect触发），参考流程1.2 putLast 真正归还连接进池子的方法（通过recycle触发），参考主流程5 put 新增连接对象放进池子里的方法，通过主流程3触发 close 连接池关闭，不再提供服务，迅速干掉所有连接进入贤者模式。 recycle 连接回收方法，通过下面DruidPooledConnection类的close方法触发，参考主流程5 shrink 连接池瘦身，参考主流程4、流程4.1 removeAbandoned 回收长期未回收的连接，默认关闭不检查，通过表1-1里的removeAbandoned属性控制 emptySignal 触发表1-1里的empty执行signal，用于唤起主流程3新增连接 表2-2 DruidConnectionHolder类最终存放进池子里的基本类型，该类持有驱动产生的真实Connection对象，同时提供一些连接池需要的标记性的属性。 属性表 属性名 说明 dataSource（DruidDataSource） 本身包含一个持有自己实例的连接池对象 conn（Connection） 真实的驱动连接对象 lastActiveTimeMillis 上次活动时间，该值在归还连接时会被刷新一次，参考主流程5，除此之外在一句sql执行结束后，这个值也会被刷新 defaultReadOnly 是否默认为只读模式，默认不是 defaultAutoCommit 是否默认开启AutoCommit，默认开启 discard 当前连接是否已被抛弃 表3-1 方法表 方法名 说明 reset 在连接对象被归还时，由于使用时可能被业务代码人为的改动一些属性（比如autoCommit等）需要把一些属性重新置为默认值，就需要该方法 表3-2 DruidPooledConnection类对外暴露给业务方的连接对象包装类，实际上其内部是包了一层上面的holder对象。 属性表 属性名 说明 conn（Connection） 实际的驱动连接对象，通过下面持有的holder对象获得并赋值 holder（DruidConnectionHolder） 持有的holder对象 disable 标记是否可用 ownerThread 标记最初获取到自己的那个线程（用于决定在close时走下方的close还是syncClose，参考主流程5） closed 标记是否已被关闭 running 标记是否正在运行中，running被置为true的地方，就是执行excute方法时。 abandoned 标记是否已被检查并丢弃，参考流程4.2 表4-1 方法表 方法名 说明 close 关闭该连接，将实际连接归还至连接池，参考主流程5 syncClose 如果是别的线程执行close方法，就得启用该方法去做，该方法与上面close的区别就是加了锁控制，参考流程5 recycle 实际触发datasource.recycle方法的方法，参考流程5 常规操作 有createStatement、commit、rollback等等常规操作，本篇文章不涉及到，仅做连接池的说明，暂时忽略。 表4-2","link":"/2019/09/20/Druid-%E7%B1%BB%E5%9B%BE-%E5%B1%9E%E6%80%A7%E8%A1%A8/"},{"title":"池化技术（二）HikariCP是如何管理数据库连接的？","text":"基于依赖程序的版本信息：&nbsp;&nbsp; 上一篇：Druid是如何管理数据库连接的 零、类图和流程图开始前先来了解下HikariCP获取一个连接时类间的交互流程，方便下面详细流程的阅读。 获取连接时的类间交互： 一、主流程1：获取连接流程HikariCP获取连接时的入口是HikariDataSource里的getConnection方法，现在来看下该方法的具体流程： 上述为HikariCP获取连接时的流程图，由图1可知，每个datasource对象里都会持有一个HikariPool对象，记为pool，初始化后的datasource对象pool是空的，所以第一次getConnection的时候会进行实例化pool属性（参考主流程1），初始化的时候需要将当前datasource里的config属性传过去，用于pool的初始化，最终标记sealed，然后根据pool对象调用getConnection方法（参考流程1.1），获取成功后返回连接对象。 二、主流程2：初始化池对象 该流程用于初始化整个连接池，这个流程会给连接池内所有的属性做初始化的工作，其中比较主要的几个流程上图已经指出，简单概括一下： 利用config初始化各种连接池属性，并且产生一个用于生产物理连接的数据源DriverDataSource 初始化存放连接对象的核心类connectionBag 初始化一个延时任务线程池类型的对象houseKeepingExecutorService，用于后续执行一些延时/定时类任务（比如连接泄漏检查延时任务，参考流程2.2以及主流程4，除此之外maxLifeTime后主动回收关闭连接也是交由该对象来执行的，这个过程可以参考主流程3） 预热连接池，HikariCP会在该流程的checkFailFast里初始化好一个连接对象放进池子内，当然触发该流程得保证initializationTimeout &gt; 0时（默认值1），这个配置属性表示留给预热操作的时间（默认值1在预热失败时不会发生重试）。与Druid通过initialSize控制预热连接对象数不一样的是，HikariCP仅预热进池一个连接对象。 初始化一个线程池对象addConnectionExecutor，用于后续扩充连接对象 初始化一个线程池对象closeConnectionExecutor，用于关闭一些连接对象，怎么触发关闭任务呢？可以参考流程1.1.2 三、流程1.1：通过HikariPool获取连接对象 从最开始的结构图可知，每个HikariPool里都维护一个ConcurrentBag对象，用于存放连接对象，由上图可以看到，实际上HikariPool的getConnection就是从ConcurrentBag里获取连接的（调用其borrow方法获得，对应ConnectionBag主流程），在长连接检查这块，与之前说的Druid不同，这里的长连接判活检查在连接对象没有被标记为“已丢弃”时，只要距离上次使用超过500ms每次取出都会进行检查（500ms是默认值，可通过配置com.zaxxer.hikari.aliveBypassWindowMs的系统参数来控制），emmmm，也就是说HikariCP对长连接的活性检查很频繁，但是其并发性能依旧优于Druid，说明频繁的长连接检查并不是导致连接池性能高低的关键所在。 这个其实是由于HikariCP的无锁实现，在高并发时对CPU的负载没有其他连接池那么高而产生的并发性能差异，后面会说HikariCP的具体做法，即使是Druid，在获取连接、生成连接、归还连接时都进行了锁控制，因为通过上篇解析Druid的文章可以知道，Druid里的连接池资源是多线程共享的，不可避免的会有锁竞争，有锁竞争意味着线程状态的变化会很频繁，线程状态变化频繁意味着CPU上下文切换也将会很频繁。 回到流程1.1，如果拿到的连接为空，直接报错，不为空则进行相应的检查，如果检查通过，则包装成ConnectionProxy对象返回给业务方，不通过则调用closeConnection方法关闭连接（对应流程1.1.2，该流程会触发ConcurrentBag的remove方法丢弃该连接，然后把实际的驱动连接交给closeConnectionExecutor线程池，异步关闭驱动连接）。 四、流程1.1.1：连接判活 承接上面的流程1.1里的判活流程，来看下判活是如何做的，首先说验证方法（注意这里该方法接受的这个connection对象不是poolEntry，而是poolEntry持有的实际驱动的连接对象），在之前介绍Druid的时候就知道，Druid是根据驱动程序里是否存在ping方法来判断是否启用ping的方式判断连接是否存活，但是到了HikariCP则更加简单粗暴，仅根据是否配置了connectionTestQuery觉定是否启用ping： 1this.isUseJdbc4Validation = config.getConnectionTestQuery() == null; 代码块1 所以一般驱动如果不是特别低的版本，不建议配置该项，否则便会走createStatement+excute的方式，相比ping简单发送心跳数据，这种方式显然更低效。 此外，这里在刚进来还会通过驱动的连接对象重新给它设置一遍networkTimeout的值，使之变成validationTimeout，表示一次验证的超时时间，为啥这里要重新设置这个属性呢？因为在使用ping方法校验时，是没办法通过类似statement那样可以setQueryTimeout的，所以只能由网络通信的超时时间来控制，这个时间可以通过jdbc的连接参数socketTimeout来控制： 1jdbc:mysql://127.0.0.1:3306/xxx?socketTimeout=250 这个值最终会被赋值给HikariCP的networkTimeout字段，这就是为什么最后那一步使用这个字段来还原驱动连接超时属性的原因；说到这里，最后那里为啥要再次还原呢？这就很容易理解了，因为验证结束了，连接对象还存活的情况下，它的networkTimeout的值这时仍然等于validationTimeout（不合预期），显然在拿出去用之前，需要恢复成本来的值，也就是HikariCP里的networkTimeout属性。 五、流程1.1.2：关闭连接对象 这个流程简单来说就是把流程1.1.1中验证不通过的死连接，主动关闭的一个流程，首先会把这个连接对象从ConnectionBag里移除，然后把实际的物理连接交给一个线程池去异步执行，这个线程池就是在主流程2里初始化池的时候初始化的线程池closeConnectionExecutor，然后异步任务内开始实际的关连接操作，因为主动关闭了一个连接相当于少了一个连接，所以还会触发一次扩充连接池（参考主流程5）操作。 六、流程2.1：HikariCP监控设置不同于Druid那样监控指标那么多，HikariCP会把我们非常关心的几项指标暴露给我们，比如当前连接池内闲置连接数、总连接数、一个连接被用了多久归还、创建一个物理连接花费多久等，HikariCP的连接池的监控我们这一节专门详细的分解一下，首先找到HikariCP下面的metrics文件夹，这下面放置了一些规范实现的监控接口等，还有一些现成的实现（比如HikariCP自带对prometheus、micrometer、dropwizard的支持，不太了解后面两个，prometheus下文直接称为普罗米修斯）： 下面，来着重看下接口的定义： 123456789101112131415161718//这个接口的实现主要负责收集一些动作的耗时public interface IMetricsTracker extends AutoCloseable{ //这个方法触发点在创建实际的物理连接时（主流程3），用于记录一个实际的物理连接创建所耗费的时间 default void recordConnectionCreatedMillis(long connectionCreatedMillis) {} //这个方法触发点在getConnection时（主流程1），用于记录获取一个连接时实际的耗时 default void recordConnectionAcquiredNanos(final long elapsedAcquiredNanos) {} //这个方法触发点在回收连接时（主流程6），用于记录一个连接从被获取到被回收时所消耗的时间 default void recordConnectionUsageMillis(final long elapsedBorrowedMillis) {} //这个方法触发点也在getConnection时（主流程1），用于记录获取连接超时的次数，每发生一次获取连接超时，就会触发一次该方法的调用 default void recordConnectionTimeout() {} @Override default void close() {}} 代码块2 触发点都了解清楚后，再来看看MetricsTrackerFactory的接口定义： 123456//用于创建IMetricsTracker实例，并且按需记录PoolStats对象里的属性（这个对象里的属性就是类似连接池当前闲置连接数之类的线程池状态类指标）public interface MetricsTrackerFactory{ //返回一个IMetricsTracker对象，并且把PoolStats传了过去 IMetricsTracker create(String poolName, PoolStats poolStats);} 代码块3 上面的接口用法见注释，针对新出现的PoolStats类，我们来看看它做了什么： 123456789101112131415161718192021222324252627282930313233343536373839404142434445public abstract class PoolStats { private final AtomicLong reloadAt; //触发下次刷新的时间（时间戳） private final long timeoutMs; //刷新下面的各项属性值的频率，默认1s，无法改变 // 总连接数 protected volatile int totalConnections; // 闲置连接数 protected volatile int idleConnections; // 活动连接数 protected volatile int activeConnections; // 由于无法获取到可用连接而阻塞的业务线程数 protected volatile int pendingThreads; // 最大连接数 protected volatile int maxConnections; // 最小连接数 protected volatile int minConnections; public PoolStats(final long timeoutMs) { this.timeoutMs = timeoutMs; this.reloadAt = new AtomicLong(); } //这里以获取最大连接数为例，其他的跟这个差不多 public int getMaxConnections() { if (shouldLoad()) { //是否应该刷新 update(); //刷新属性值，注意这个update的实现在HikariPool里，因为这些属性值的直接或间接来源都是HikariPool } return maxConnections; } protected abstract void update(); //实现在↑上面已经说了 private boolean shouldLoad() { //按照更新频率来决定是否刷新属性值 for (; ; ) { final long now = currentTime(); final long reloadTime = reloadAt.get(); if (reloadTime &gt; now) { return false; } else if (reloadAt.compareAndSet(reloadTime, plusMillis(now, timeoutMs))) { return true; } } }} 代码块4 实际上这里就是这些属性获取和触发刷新的地方，那么这个对象是在哪里被生成并且丢给MetricsTrackerFactory的create方法的呢？这就是本节所需要讲述的要点：主流程2里的设置监控器的流程，来看看那里发生了什么事吧： 123456789101112131415161718192021222324252627//监控器设置方法（此方法在HikariPool中，metricsTracker属性就是HikariPool用来触发IMetricsTracker里方法调用的）public void setMetricsTrackerFactory(MetricsTrackerFactory metricsTrackerFactory) { if (metricsTrackerFactory != null) { //MetricsTrackerDelegate是包装类，是HikariPool的一个静态内部类，是实际持有IMetricsTracker对象的类，也是实际触发IMetricsTracker里方法调用的类 //这里首先会触发MetricsTrackerFactory类的create方法拿到IMetricsTracker对象，然后利用getPoolStats初始化PoolStat对象，然后也一并传给MetricsTrackerFactory this.metricsTracker = new MetricsTrackerDelegate(metricsTrackerFactory.create(config.getPoolName(), getPoolStats())); } else { //不启用监控，直接等于一个没有实现方法的空类 this.metricsTracker = new NopMetricsTrackerDelegate(); }}private PoolStats getPoolStats() { //初始化PoolStats对象，并且规定1s触发一次属性值刷新的update方法 return new PoolStats(SECONDS.toMillis(1)) { @Override protected void update() { //实现了PoolStat的update方法，刷新各个属性的值 this.pendingThreads = HikariPool.this.getThreadsAwaitingConnection(); this.idleConnections = HikariPool.this.getIdleConnections(); this.totalConnections = HikariPool.this.getTotalConnections(); this.activeConnections = HikariPool.this.getActiveConnections(); this.maxConnections = config.getMaximumPoolSize(); this.minConnections = config.getMinimumIdle(); } };} 代码块5 到这里HikariCP的监控器就算是注册进去了，所以要想实现自己的监控器拿到上面的指标，要经过如下步骤： 新建一个类实现IMetricsTracker接口，我们这里将该类记为IMetricsTrackerImpl 新建一个类实现MetricsTrackerFactory接口，我们这里将该类记为MetricsTrackerFactoryImpl，并且将上面的IMetricsTrackerImpl在其create方法内实例化 将MetricsTrackerFactoryImpl实例化后调用HikariPool的setMetricsTrackerFactory方法注册到Hikari连接池。 上面没有提到PoolStats里的属性怎么监控，这里来说下，由于create方法是调用一次就没了，create方法只是接收了PoolStats对象的实例，如果不处理，那么随着create调用的结束，这个实例针对监控模块来说就失去持有了，所以这里如果想要拿到PoolStats里的属性，就需要开启一个守护线程，让其持有PoolStats对象实例，并且定时获取其内部属性值，然后push给监控系统，如果是普罗米修斯等使用pull方式获取监控数据的监控系统，可以效仿HikariCP原生普罗米修斯监控的实现，自定义一个Collector对象来接收PoolStats实例，这样普罗米修斯就可以定期拉取了，比如HikariCP根据普罗米修斯监控系统自己定义的MetricsTrackerFactory实现（对应图2里的PrometheusMetricsTrackerFactory类）： 12345678910111213@Overridepublic IMetricsTracker create(String poolName, PoolStats poolStats) { getCollector().add(poolName, poolStats); //将接收到的PoolStats对象直接交给Collector，这样普罗米修斯服务端每触发一次采集接口的调用，PoolStats都会跟着执行一遍内部属性获取流程 return new PrometheusMetricsTracker(poolName, this.collectorRegistry); //返回IMetricsTracker接口的实现类}//自定义的Collectorprivate HikariCPCollector getCollector() { if (collector == null) { //注册到普罗米修斯收集中心 collector = new HikariCPCollector().register(this.collectorRegistry); } return collector; 代码块6 通过上面的解释可以知道在HikariCP中如何自定义一个自己的监控器，以及相比Druid的监控，有什么区别。工作中很多时候都是需要自定义的，我司虽然也是用的普罗米修斯监控，但是因为HikariCP原生的普罗米修斯收集器里面对监控指标的命名并不符合我司的规范，所以就自定义了一个，有类似问题的不妨也试一试。 🍁 这一节没有画图，纯代码，因为画图不太好解释这部分的东西，这部分内容与连接池整体流程关系也不大，充其量获取了连接池本身的一些属性，在连接池里的触发点也在上面代码段的注释里说清楚了，看代码定义可能更好理解一些。 七、流程2.2：连接泄漏的检测与告警本节对应主流程2里的子流程2.2，在初始化池对象时，初始化了一个叫做leakTaskFactory的属性，本节来看下它具体是用来做什么的。 7.1：它是做什么的？一个连接被拿出去使用时间超过leakDetectionThreshold（可配置，默认0）未归还的，会触发一个连接泄漏警告，通知业务方目前存在连接泄漏的问题。 7.2：过程详解该属性是ProxyLeakTaskFactory类型对象，且它还会持有houseKeepingExecutorService这个线程池对象，用于生产ProxyLeakTask对象，然后利用上面的houseKeepingExecutorService延时运行该对象里的run方法。该流程的触发点在上面的流程1.1最后包装成ProxyConnection对象的那一步，来看看具体的流程图： 每次在流程1.1那里生成ProxyConnection对象时，都会触发上面的流程，由流程图可以知道，ProxyConnection对象持有PoolEntry和ProxyLeakTask的对象，其中初始化ProxyLeakTask对象时就用到了leakTaskFactory对象，通过其schedule方法可以进行ProxyLeakTask的初始化，并将其实例传递给ProxyConnection进行初始化赋值（ps：由图知ProxyConnection在触发回收事件时，会主动取消这个泄漏检查任务，这也是ProxyConnection需要持有ProxyLeakTask对象的原因）。 在上面的流程图中可以知道，只有在leakDetectionThreshold不等于0的时候才会生成一个带有实际延时任务的ProxyLeakTask对象，否则返回无实际意义的空对象。所以要想启用连接泄漏检查，首先要把leakDetectionThreshold配置设置上，这个属性表示经过该时间后借出去的连接仍未归还，则触发连接泄漏告警。 ProxyConnection之所以要持有ProxyLeakTask对象，是因为它可以监听到连接是否触发归还操作，如果触发，则调用cancel方法取消延时任务，防止误告。 由此流程可以知道，跟Druid一样，HikariCP也有连接对象泄漏检查，与Druid主动回收连接相比，HikariCP实现更加简单，仅仅是在触发时打印警告日志，不会采取具体的强制回收的措施。 与Druid一样，默认也是关闭这个流程的，因为实际开发中一般使用第三方框架，框架本身会保证及时的close连接，防止连接对象泄漏，开启与否还是取决于业务是否需要，如果一定要开启，如何设置leakDetectionThreshold的大小也是需要考虑的一件事。 八、主流程3：生成连接对象本节来讲下主流程2里的createEntry方法，这个方法利用PoolBase里的DriverDataSource对象生成一个实际的连接对象（如果忘记DriverDatasource是哪里初始化的了，可以看下主流程2里PoolBase的initializeDataSource方法的作用），然后用PoolEntry类包装成PoolEntry对象，现在来看下这个包装类有哪些主要属性： 123456789101112131415161718192021final class PoolEntry implements IConcurrentBagEntry { private static final Logger LOGGER = LoggerFactory.getLogger(PoolEntry.class); //通过cas来修改state属性 private static final AtomicIntegerFieldUpdater stateUpdater; Connection connection; //实际的物理连接对象 long lastAccessed; //触发回收时刷新该时间，表示“最近一次使用时间” long lastBorrowed; //getConnection里borrow成功后刷新该时间，表示“最近一次借出的时间” @SuppressWarnings(\"FieldCanBeLocal\") private volatile int state = 0; //连接状态，枚举值：IN_USE（使用中）、NOT_IN_USE（闲置中）、REMOVED（已移除）、RESERVED（标记为保留中） private volatile boolean evict; //是否被标记为废弃，很多地方用到（比如流程1.1靠这个判断连接是否已被废弃，再比如主流程4里时钟回拨时触发的直接废弃逻辑） private volatile ScheduledFuture&lt;?&gt; endOfLife; //用于在超过连接生命周期（maxLifeTime）时废弃连接的延时任务，这里poolEntry要持有该对象，主要是因为在对象主动被关闭时（意味着不需要在超过maxLifeTime时主动失效了），需要cancel掉该任务 private final FastList openStatements; //当前该连接对象上生成的所有的statement对象，用于在回收连接时主动关闭这些对象，防止存在漏关的statement private final HikariPool hikariPool; //持有pool对象 private final boolean isReadOnly; //是否为只读 private final boolean isAutoCommit; //是否存在事务} 代码块7 上面就是整个PoolEntry对象里所有的属性，这里再说下endOfLife对象，它是一个利用houseKeepingExecutorService这个线程池对象做的延时任务，这个延时任务一般在创建好连接对象后maxLifeTime左右的时间触发，具体来看下createEntry代码： 123456789101112131415161718192021222324252627282930313233private PoolEntry createPoolEntry() { final PoolEntry poolEntry = newPoolEntry(); //生成实际的连接对象 final long maxLifetime = config.getMaxLifetime(); //拿到配置好的maxLifetime if (maxLifetime &gt; 0) { //&lt;=0的时候不启用主动过期策略 // 计算需要减去的随机数 // 源注释：variance up to 2.5% of the maxlifetime final long variance = maxLifetime &gt; 10_000 ? ThreadLocalRandom.current().nextLong(maxLifetime / 40) : 0; final long lifetime = maxLifetime - variance; //生成实际的延时时间 poolEntry.setFutureEol(houseKeepingExecutorService.schedule( () -&gt; { //实际的延时任务，这里直接触发softEvictConnection，而softEvictConnection内则会标记该连接对象为废弃状态，然后尝试修改其状态为STATE_RESERVED，若成功，则触发closeConnection（对应流程1.1.2） if (softEvictConnection(poolEntry, \"(connection has passed maxLifetime)\", false /* not owner */)) { addBagItem(connectionBag.getWaitingThreadCount()); //回收完毕后，连接池内少了一个连接，就会尝试新增一个连接对象 } }, lifetime, MILLISECONDS)); //给endOfLife赋值，并且提交延时任务，lifetime后触发 } return poolEntry; } //触发新增连接任务 public void addBagItem(final int waiting) { //前排提示：addConnectionQueue和addConnectionExecutor的关系和初始化参考主流程2 //当添加连接的队列里已提交的任务超过那些因为获取不到连接而发生阻塞的线程个数时，就进行提交连接新增连接的任务 final boolean shouldAdd = waiting - addConnectionQueue.size() &gt;= 0; // Yes, &gt;= is intentional. if (shouldAdd) { //提交任务给addConnectionExecutor这个线程池，PoolEntryCreator是一个实现了Callable接口的类，下面将通过流程图的方式介绍该类的call方法 addConnectionExecutor.submit(poolEntryCreator); } } 代码块8 通过上面的流程，可以知道，HikariCP一般通过createEntry方法来新增一个连接入池，每个连接被包装成PoolEntry对象，在创建好对象时，同时会提交一个延时任务来关闭废弃该连接，这个时间就是我们配置的maxLifeTime，为了保证不在同一时间失效，HikariCP还会利用maxLifeTime减去一个随机数作为最终的延时任务延迟时间，然后在触发废弃任务时，还会触发addBagItem，进行连接添加任务（因为废弃了一个连接，需要往池子里补充一个），该任务则交给由主流程2里定义好的addConnectionExecutor线程池执行，那么，现在来看下这个异步添加连接对象的任务流程： 这个流程就是往连接池里加连接用的，跟createEntry结合起来说是因为这俩流程是紧密相关的，除此之外，主流程5（fillPool，扩充连接池）也会触发该任务。 九、主流程4：连接池缩容HikariCP会按照minIdle定时清理闲置过久的连接，这个定时任务在主流程2初始化连接池对象时被启用，跟上面的流程一样，也是利用houseKeepingExecutorService这个线程池对象做该定时任务的执行器。 来看下主流程2里是怎么启用该任务的： 12//housekeepingPeriodMs的默认值是30s，所以定时任务的间隔为30sthis.houseKeeperTask = houseKeepingExecutorService.scheduleWithFixedDelay(new HouseKeeper(), 100L, housekeepingPeriodMs, MILLISECONDS); 代码块9 那么本节主要来说下HouseKeeper这个类，该类实现了Runnable接口，回收逻辑主要在其run方法内，来看看run方法的逻辑流程图： 上面的流程就是HouseKeeper的run方法里具体做的事情，由于系统时间回拨会导致该定时任务回收一些连接时产生误差，因此存在如下判断： 12345//now就是当前系统时间，previous就是上次触发该任务时的时间，housekeepingPeriodMs就是隔多久触发该任务一次//也就是说plusMillis(previous, housekeepingPeriodMs)表示当前时间//如果系统时间没被回拨，那么plusMillis(now, 128)一定是大于当前时间的，如果被系统时间被回拨//回拨的时间超过128ms，那么下面的判断就成立，否则永远不会成立if (plusMillis(now, 128) &lt; plusMillis(previous, housekeepingPeriodMs)) 代码块10 这是hikariCP在解决系统时钟被回拨时做出的一种措施，通过流程图可以看到，它是直接把池子里所有的连接对象取出来挨个儿的标记成废弃，并且尝试把状态值修改为STATE_RESERVED（后面会说明这些状态，这里先不深究）。如果系统时钟没有发生改变（绝大多数情况会命中这一块的逻辑），由图知，会把当前池内所有处于闲置状态（STATE_NOT_IN_USE）的连接拿出来，然后计算需要检查的范围，然后循环着修改连接的状态： 123456789101112//拿到所有处于闲置状态的连接final List notInUse = connectionBag.values(STATE_NOT_IN_USE);//计算出需要被检查闲置时间的数量，简单来说，池内需要保证最小minIdle个连接活着，所以需要计算出超出这个范围的闲置对象进行检查int toRemove = notInUse.size() - config.getMinIdle();for (PoolEntry entry : notInUse) { //在检查范围内，且闲置时间超出idleTimeout，然后尝试将连接对象状态由STATE_NOT_IN_USE变为STATE_RESERVED成功 if (toRemove &gt; 0 &amp;&amp; elapsedMillis(entry.lastAccessed, now) &gt; idleTimeout &amp;&amp; connectionBag.reserve(entry)) { closeConnection(entry, \"(connection has passed idleTimeout)\"); //满足上述条件，进行连接关闭 toRemove--; }}fillPool(); //因为可能回收了一些连接，所以要再次触发连接池扩充流程检查下是否需要新增连接。 代码块11 上面的代码就是流程图里对应的没有回拨系统时间时的流程逻辑。该流程在idleTimeout大于0（默认等于0）并且minIdle小于maxPoolSize的时候才会启用，默认是不启用的，若需要启用，可以按照条件来配置。 十、主流程5：扩充连接池这个流程主要依附HikariPool里的fillPool方法，这个方法已经在上面很多流程里出现过了，它的作用就是在触发连接废弃、连接池连接不够用时，发起扩充连接数的操作，这是个很简单的过程，下面看下源码（为了使代码结构更加清晰，对源码做了细微改动）： 123456789101112131415161718// PoolEntryCreator关于call方法的实现流程在主流程3里已经看过了，但是这里却有俩PoolEntryCreator对象，// 这是个较细节的地方，用于打日志用，不再说这部分，为了便于理解，只需要知道这俩对象执行的是同一块call方法即可private final PoolEntryCreator poolEntryCreator = new PoolEntryCreator(null);private final PoolEntryCreator postFillPoolEntryCreator = new PoolEntryCreator(\"After adding \");private synchronized void fillPool() { // 这个判断就是根据当前池子里相关数据，推算出需要扩充的连接数， // 判断方式就是利用最大连接数跟当前连接总数的差值，与最小连接数与当前池内闲置的连接数的差值，取其最小的那一个得到 int needAdd = Math.min(maxPoolSize - connectionBag.size(), minIdle - connectionBag.getCount(STATE_NOT_IN_USE)); //减去当前排队的任务，就是最终需要新增的连接数 final int connectionsToAdd = needAdd - addConnectionQueue.size(); for (int i = 0; i &lt; connectionsToAdd; i++) { //一般循环的最后一次会命中postFillPoolEntryCreator任务，其实就是在最后一次会打印一次日志而已（可以忽略该干扰逻辑） addConnectionExecutor.submit((i &lt; connectionsToAdd - 1) ? poolEntryCreator : postFillPoolEntryCreator); }} 代码块12 由该过程可以知道，最终这个新增连接的任务也是交由addConnectionExecutor线程池来处理的，而任务的主题也是PoolEntryCreator，这个流程可以参考主流程3. 然后needAdd的推算： 1Math.min(最大连接数 - 池内当前连接总数, 最小连接数 - 池内闲置的连接数) 根据这个方式判断，可以保证池内的连接数永远不会超过maxPoolSize，也永远不会低于minIdle。在连接吃紧的时候，可以保证每次触发都以minIdle的数量扩容。因此如果在maxPoolSize跟minIdle配置的值一样的话，在池内连接吃紧的时候，就不会发生任何扩容了。 十一、主流程6：连接回收最开始说过，最终真实的物理连接对象会被包装成PoolEntry对象，存放进ConcurrentBag，然后获取时，PoolEntry对象又会被再次包装成ProxyConnection对象暴露给使用方的，那么触发连接回收，实际上就是触发ProxyConnection里的close方法： 12345678910111213141516public final void close() throws SQLException { // 原注释：Closing statements can cause connection eviction, so this must run before the conditional below closeStatements(); //此连接对象在业务方使用过程中产生的所有statement对象，进行统一close，防止漏close的情况 if (delegate != ClosedConnection.CLOSED_CONNECTION) { leakTask.cancel(); //取消连接泄漏检查任务，参考流程2.2 try { if (isCommitStateDirty &amp;&amp; !isAutoCommit) { //在存在执行语句后并且还打开了事务，调用close时需要主动回滚事务 delegate.rollback(); //回滚 lastAccess = currentTime(); //刷新\"最后一次使用时间\" } } finally { delegate = ClosedConnection.CLOSED_CONNECTION; poolEntry.recycle(lastAccess); //触发回收 } }} 代码块13 这个就是ProxyConnection里的close方法，可以看到它最终会调用PoolEntry的recycle方法进行回收，除此之外，连接对象的最后一次使用时间也是在这个时候刷新的，该时间是个很重要的属性，可以用来判断一个连接对象的闲置时间，来看下PoolEntry的recycle方法： 123456void recycle(final long lastAccessed) { if (connection != null) { this.lastAccessed = lastAccessed; //刷新最后使用时间 hikariPool.recycle(this); //触发HikariPool的回收方法，把自己传过去 }} 代码块14 之前有说过，每个PoolEntry对象都持有HikariPool的对象，方便触发连接池的一些操作，由上述代码可以看到，最终还是会触发HikariPool里的recycle方法，再来看下HikariPool的recycle方法： 1234void recycle(final PoolEntry poolEntry) { metricsTracker.recordConnectionUsage(poolEntry); //监控指标相关，忽略 connectionBag.requite(poolEntry); //最终触发connectionBag的requite方法归还连接，该流程参考ConnectionBag主流程里的requite方法部分} 代码块15 以上就是连接回收部分的逻辑，相比其他流程，还是比较简洁的。 十二、ConcurrentBag主流程这个类用来存放最终的PoolEntry类型的连接对象，提供了基本的增删查的功能，被HikariPool持有，上面那么多的操作，几乎都是在HikariPool中完成的，HikariPool用来管理实际的连接生产动作和回收动作，实际操作的却是ConcurrentBag类，梳理下上面所有流程的触发点： 主流程2：初始化HikariPool时初始化ConcurrentBag（构造方法），预热时通过createEntry拿到连接对象，调用ConcurrentBag.add添加连接到ConcurrentBag。 流程1.1：通过HikariPool获取连接时，通过调用ConcurrentBag.borrow拿到一个连接对象。 主流程6：通过ConcurrentBag.requite归还一个连接。 流程1.1.2：触发关闭连接时，会通过ConcurrentBag.remove移除连接对象，由前面的流程可知关闭连接触发点为：连接超过最大生命周期maxLifeTime主动废弃、健康检查不通过主动废弃、连接池缩容。 主流程3：通过异步添加连接时，通过调用ConcurrentBag.add添加连接到ConcurrentBag，由前面的流程可知添加连接触发点为：连接超过最大生命周期maxLifeTime主动废弃连接后、连接池扩容。 主流程4：连接池缩容任务，通过调用ConcurrentBag.values筛选出需要的做操作的连接对象，然后再通过ConcurrentBag.reserve完成对连接对象状态的修改，然后会通过流程1.1.2触发关闭和移除连接操作。 通过触发点整理，可以知道该结构里的主要方法，就是上面触发点里标记为标签色的部分，然后来具体看下该类的基本定义和主要方法： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public class ConcurrentBag&lt;T extends IConcurrentBagEntry&gt; implements AutoCloseable { private final CopyOnWriteArrayList&lt;T&gt; sharedList; //最终存放PoolEntry对象的地方，它是一个CopyOnWriteArrayList private final boolean weakThreadLocals; //默认false，为true时可以让一个连接对象在下方threadList里的list内处于弱引用状态，防止内存泄漏（参见备注1） private final ThreadLocal&lt;List&lt;Object&gt;&gt; threadList; //线程级的缓存，从sharedList拿到的连接对象，会被缓存进当前线程内，borrow时会先从缓存中拿，从而达到池内无锁实现 private final IBagStateListener listener; //内部接口，HikariPool实现了该接口，主要用于ConcurrentBag主动通知HikariPool触发添加连接对象的异步操作（也就是主流程3里的addConnectionExecutor所触发的流程） private final AtomicInteger waiters; //当前因为获取不到连接而发生阻塞的业务线程数，这个在之前的流程里也出现过，比如主流程3里addBagItem就会根据该指标进行判断是否需要新增连接 private volatile boolean closed; //标记当前ConcurrentBag是否已被关闭 private final SynchronousQueue&lt;T&gt; handoffQueue; //这是个即产即销的队列，用于在连接不够用时，及时获取到add方法里新创建的连接对象，详情可以参考下面borrow和add的代码 //内部接口，PoolEntry类实现了该接口 public interface IConcurrentBagEntry { //连接对象的状态，前面的流程很多地方都已经涉及到了，比如主流程4的缩容 int STATE_NOT_IN_USE = 0; //闲置 int STATE_IN_USE = 1; //使用中 int STATE_REMOVED = -1; //已废弃 int STATE_RESERVED = -2; //标记保留，介于闲置和废弃之间的中间状态，主要由缩容那里触发修改 boolean compareAndSet(int expectState, int newState); //尝试利用cas修改连接对象的状态值 void setState(int newState); //设置状态值 int getState(); //获取状态值 } //参考上面listener属性的解释 public interface IBagStateListener { void addBagItem(int waiting); } //获取连接方法 public T borrow(long timeout, final TimeUnit timeUnit) { // 省略... } //回收连接方法 public void requite(final T bagEntry) { //省略... } //添加连接方法 public void add(final T bagEntry) { //省略... } //移除连接方法 public boolean remove(final T bagEntry) { //省略... } //根据连接状态值获取当前池子内所有符合条件的连接集合 public List values(final int state) { //省略... } //获取当前池子内所有的连接 public List values() { //省略... } //利用cas把传入的连接对象的state从 STATE_NOT_IN_USE 变为 STATE_RESERVED public boolean reserve(final T bagEntry) { //省略... } //获取当前池子内符合传入状态值的连接数量 public int getCount(final int state) { //省略... }} 代码块16 从这个基本结构就可以稍微看出HikariCP是如何优化传统连接池实现的了，相比Druid来说，HikariCP更加偏向无锁实现，尽量避免锁竞争的发生。 12.1：borrow这个方法用来获取一个可用的连接对象，触发点为流程1.1，HikariPool就是利用该方法获取连接的，下面来看下该方法做了什么： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public T borrow(long timeout, final TimeUnit timeUnit) throws InterruptedException { // 源注释：Try the thread-local list first final List&lt;Object&gt; list = threadList.get(); //首先从当前线程的缓存里拿到之前被缓存进来的连接对象集合 for (int i = list.size() - 1; i &gt;= 0; i--) { final Object entry = list.remove(i); //先移除，回收方法那里会再次add进来 final T bagEntry = weakThreadLocals ? ((WeakReference&lt;T&gt;) entry).get() : (T) entry; //默认不启用弱引用 // 获取到对象后，通过cas尝试把其状态从STATE_NOT_IN_USE 变为 STATE_IN_USE，注意，这里如果其他线程也在使用这个连接对象， // 并且成功修改属性，那么当前线程的cas会失败，那么就会继续循环尝试获取下一个连接对象 if (bagEntry != null &amp;&amp; bagEntry.compareAndSet(STATE_NOT_IN_USE, STATE_IN_USE)) { return bagEntry; //cas设置成功后，表示当前线程绕过其他线程干扰，成功获取到该连接对象，直接返回 } } // 源注释：Otherwise, scan the shared list ... then poll the handoff queue final int waiting = waiters.incrementAndGet(); //如果缓存内找不到一个可用的连接对象，则认为需要“回源”，waiters+1 try { for (T bagEntry : sharedList) { //循环sharedList，尝试把连接状态值从STATE_NOT_IN_USE 变为 STATE_IN_USE if (bagEntry.compareAndSet(STATE_NOT_IN_USE, STATE_IN_USE)) { // 源注释：If we may have stolen another waiter's connection, request another bag add. if (waiting &gt; 1) { //阻塞线程数大于1时，需要触发HikariPool的addBagItem方法来进行添加连接入池，这个方法的实现参考主流程3 listener.addBagItem(waiting - 1); } return bagEntry; //cas设置成功，跟上面的逻辑一样，表示当前线程绕过其他线程干扰，成功获取到该连接对象，直接返回 } } //走到这里说明不光线程缓存里的列表竞争不到连接对象，连sharedList里也找不到可用的连接，这时则认为需要通知HikariPool，该触发添加连接操作了 listener.addBagItem(waiting); timeout = timeUnit.toNanos(timeout); //这时候开始利用timeout控制获取时间 do { final long start = currentTime(); //尝试从handoffQueue队列里获取最新被加进来的连接对象（一般新入的连接对象除了加进sharedList之外，还会被offer进该队列） final T bagEntry = handoffQueue.poll(timeout, NANOSECONDS); //如果超出指定时间后仍然没有获取到可用的连接对象，或者获取到对象后通过cas设置成功，这两种情况都不需要重试，直接返回对象 if (bagEntry == null || bagEntry.compareAndSet(STATE_NOT_IN_USE, STATE_IN_USE)) { return bagEntry; } //走到这里说明从队列内获取到了连接对象，但是cas设置失败，说明又该对象又被其他线程率先拿去用了，若时间还够，则再次尝试获取 timeout -= elapsedNanos(start); //timeout减去消耗的时间，表示下次循环可用时间 } while (timeout &gt; 10_000); //剩余时间大于10s时才继续进行，一般情况下，这个循环只会走一次，因为timeout很少会配的比10s还大 return null; //超时，仍然返回null } finally { waiters.decrementAndGet(); //这一步出去后，HikariPool收到borrow的结果，算是走出阻塞，所以waiters-1 }} 代码块17 仔细看下注释，该过程大致分成三个主要步骤： 从线程缓存获取连接 获取不到再从sharedList里获取 都获取不到则触发添加连接逻辑，并尝试从队列里获取新生成的连接对象 12.2：add这个流程会添加一个连接对象进入bag，通常由主流程3里的addBagItem方法通过addConnectionExecutor异步任务触发添加操作，该方法主流程如下： 12345678910public void add(final T bagEntry) { sharedList.add(bagEntry); //直接加到sharedList里去 // 源注释：spin until a thread takes it or none are waiting // 参考borrow流程，当存在线程等待获取可用连接，并且当前新入的这个连接状态仍然是闲置状态，且队列里无消费者等待获取时，发起一次线程调度 while (waiters.get() &gt; 0 &amp;&amp; bagEntry.getState() == STATE_NOT_IN_USE &amp;&amp; !handoffQueue.offer(bagEntry)) { //注意这里会offer一个连接对象入队列 yield(); }} 代码块18 结合borrow来理解的话，这里在存在等待线程时会添加一个连接对象入队列，可以让borrow里发生等待的地方更容易poll到这个连接对象。 12.3：requite这个流程会回收一个连接，该方法的触发点在主流程6，具体代码如下： 1234567891011121314151617181920public void requite(final T bagEntry) { bagEntry.setState(STATE_NOT_IN_USE); //回收意味着使用完毕，更改state为STATE_NOT_IN_USE状态 for (int i = 0; waiters.get() &gt; 0; i++) { //如果存在等待线程的话，尝试传给队列，让borrow获取 if (bagEntry.getState() != STATE_NOT_IN_USE || handoffQueue.offer(bagEntry)) { return; } else if ((i &amp; 0xff) == 0xff) { parkNanos(MICROSECONDS.toNanos(10)); } else { yield(); } } final List&lt;Object&gt; threadLocalList = threadList.get(); if (threadLocalList.size() &lt; 50) { //线程内连接集合的缓存最多50个，这里回收连接时会再次加进当前线程的缓存里，方便下次borrow获取 threadLocalList.add(weakThreadLocals ? new WeakReference&lt;&gt;(bagEntry) : bagEntry); //默认不启用弱引用，若启用的话，则缓存集合里的连接对象没有内存泄露的风险 }} 代码块19 12.4：remove这个负责从池子里移除一个连接对象，触发点在流程1.1.2，代码如下： 123456789101112131415public boolean remove(final T bagEntry) { // 下面两个cas操作，都是从其他状态变为移除状态，任意一个成功，都不会走到下面的warn log if (!bagEntry.compareAndSet(STATE_IN_USE, STATE_REMOVED) &amp;&amp; !bagEntry.compareAndSet(STATE_RESERVED, STATE_REMOVED) &amp;&amp; !closed) { LOGGER.warn(\"Attempt to remove an object from the bag that was not borrowed or reserved: {}\", bagEntry); return false; } // 直接从sharedList移除掉 final boolean removed = sharedList.remove(bagEntry); if (!removed &amp;&amp; !closed) { LOGGER.warn(\"Attempt to remove an object from the bag that does not exist: {}\", bagEntry); } return removed;} 代码块20 这里需要注意的是，移除时仅仅移除了sharedList里的对象，各个线程内缓存的那一份集合里对应的对象并没有被移除，这个时候会不会存在该连接再次从缓存里拿到呢？会的，但是不会返回出去，而是直接remove掉了，仔细看borrow的代码发现状态不是闲置状态的时候，取出来时就会remove掉，然后也拿不出去，自然也不会触发回收方法。 12.5：values该方法存在重载方法，用于返回当前池子内连接对象的集合，触发点在主流程4，代码如下： 1234567891011public List values(final int state) { //过滤出来符合状态值的对象集合逆序后返回出去 final List list = sharedList.stream().filter(e -&gt; e.getState() == state).collect(Collectors.toList()); Collections.reverse(list); return list;}public List values() { //返回全部连接对象（注意下方clone为浅拷贝） return (List) sharedList.clone();} 代码块21 12.6：reserve该方法单纯将连接对象的状态值由STATE_NOT_IN_USE修改为STATE_RESERVED，触发点仍然是主流程4，缩容时使用，代码如下： 123public boolean reserve(final T bagEntry){ return bagEntry.compareAndSet(STATE_NOT_IN_USE, STATE_RESERVED);} 代码块22 12.7：getCount该方法用于返回池内符合某个状态值的连接的总数量，触发点为主流程5，扩充连接池时用于获取闲置连接总数，代码如下： 123456789public int getCount(final int state){ int count = 0; for (IConcurrentBagEntry e : sharedList) { if (e.getState() == state) { count++; } } return count;} 代码块23 以上就是ConcurrentBag的主要方法和处理连接对象的主要流程。 十三、总结到这里基本上一个连接的生产到获取到回收到废弃一整个生命周期在HikariCP内是如何管理的就说完了，相比之前的Druid的实现，有很大的不同，主要是HikariCP的无锁获取连接，本篇没有涉及FastList的说明，因为从连接管理这个角度确实很少用到该结构，用到FastList的地方主要在存储连接对象生成的statement对象以及用于存储线程内缓存起来的连接对象； 除此之外HikariCP还利用javassist技术编译期生成了ProxyConnection的初始化，这里也没有相关说明，网上有关HikariCP的优化有很多文章，大多数都提到了字节码优化、fastList、concurrentBag的实现，本篇主要通过深入解析HikariPool和ConcurrentBag的实现，来说明HikariCP相比Druid具体做了哪些不一样的操作。","link":"/2019/08/28/%E6%B1%A0%E5%8C%96%E6%8A%80%E6%9C%AF%EF%BC%88%E4%BA%8C%EF%BC%89HikariCP%E6%98%AF%E5%A6%82%E4%BD%95%E7%AE%A1%E7%90%86%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%E7%9A%84%EF%BC%9F/"},{"title":"池化技术（一）Druid是如何管理数据库连接的？","text":"基于依赖程序的版本信息：&nbsp;&nbsp; 下一篇：HikariCP是如何管理数据库连接的 零、类图&amp;流程预览下方流程中涉及到的类、属性、方法名均列在这里：Druid-类图-属性表 ←该表格用来辅助理解下面的流程图和代码，不用细看，混乱时可用来理清关系。 本文会通过getConnection作为入口，探索在druid里，一个连接的生命周期。大体流程被划分成了以下几个主流程： 一、主流程1：获取连接流程首先从入口来看看它在获取连接时做了哪些操作： 上述流程对应源代码如下（请展开）： 代码段1-1 &gt;folded123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104public DruidPooledConnection getConnection(long maxWaitMillis) throws SQLException { init(); //初始化，即主流程2 if (filters.size() &gt; 0) { FilterChainImpl filterChain = new FilterChainImpl(this); //责任链，内部也是触发下面的getConnectionDirect方法，只是要走一遍责任链上每个filter的逻辑，这里不做描述，后续放到流程1.1里体现 return filterChain.dataSource_connect(this, maxWaitMillis); } else { return getConnectionDirect(maxWaitMillis); //触发getConnectionDirect } } public DruidPooledConnection getConnectionDirect(long maxWaitMillis) throws SQLException { int notFullTimeoutRetryCnt = 0; for (;;) { //死循环 /** * 真正返回出去的连接对象，注意这里是被druid包装成了DruidPooledConnection类型， * 实际上池子里存放的连接类型是DruidConnectionHolder，DruidPooledConnection类本身持有一个holder属性， * 用于保存真正的连接对象，而DruidConnectionHolder才是真正保存驱动连接对象的类。 */ DruidPooledConnection poolableConnection; try { poolableConnection = getConnectionInternal(maxWaitMillis); //从池子里获取连接，这一个后续放到流程1.2体现 } catch (GetConnectionTimeoutException ex) { if (notFullTimeoutRetryCnt &lt;= this.notFullTimeoutRetryCount &amp;&amp; !isFull()) { //出现了超时异常，在连接池没满且重试次数未超过上限的情况下，重试一次（notFullTimeoutRetryCount默认是0，所以至少可以重试一次）。 notFullTimeoutRetryCnt++; //重试次数+1 if (LOG.isWarnEnabled()) { LOG.warn(\"get connection timeout retry : \" + notFullTimeoutRetryCnt); } continue; } throw ex; //超过重试次数或者池子已满仍然获取失败，则直接抛出异常 } if (testOnBorrow) { //testOnBorrow开启时，每次都进行检测连接可用性 boolean validate = testConnectionInternal(poolableConnection.holder, poolableConnection.conn); if (!validate) { if (LOG.isDebugEnabled()) { LOG.debug(\"skip not validate connection.\"); } Connection realConnection = poolableConnection.conn; //获取真正驱动的连接对象 discardConnection(realConnection); //若连接不可用，则触发discard，这个方法具体放到流程1.4体现 continue; } } else { Connection realConnection = poolableConnection.conn; if (poolableConnection.conn.isClosed()) { discardConnection(null); // 传入null，避免重复关闭 continue; } if (testWhileIdle) { //不启用testOnBorrow的情况下，才会判断是否启用testWhileIdle final DruidConnectionHolder holder = poolableConnection.holder; long currentTimeMillis = System.currentTimeMillis(); long lastActiveTimeMillis = holder.lastActiveTimeMillis; //上次被使用的时间 long lastKeepTimeMillis = holder.lastKeepTimeMillis; if (lastKeepTimeMillis &gt; lastActiveTimeMillis) { lastActiveTimeMillis = lastKeepTimeMillis; } long idleMillis = currentTimeMillis - lastActiveTimeMillis; //计算出闲置时间 long timeBetweenEvictionRunsMillis = this.timeBetweenEvictionRunsMillis; if (timeBetweenEvictionRunsMillis &lt;= 0) { timeBetweenEvictionRunsMillis = DEFAULT_TIME_BETWEEN_EVICTION_RUNS_MILLIS; } if (idleMillis &gt;= timeBetweenEvictionRunsMillis || idleMillis &lt; 0) { //当闲置时间超出timeBetweenEvictionRunsMillis（默认60s）时，则触发检查逻辑 boolean validate = testConnectionInternal(poolableConnection.holder, poolableConnection.conn); if (!validate) { if (LOG.isDebugEnabled()) { LOG.debug(\"skip not validate connection.\"); } discardConnection(realConnection); //连接不可用，同样触发discard continue; } } } } if (removeAbandoned) { //若开启removeAbandoned，则把当前拿到的连接放到activeConnections里，方便后续检查（后面流程4.2体现） StackTraceElement[] stackTrace = Thread.currentThread().getStackTrace(); poolableConnection.connectStackTrace = stackTrace; poolableConnection.setConnectedTimeNano(); //设置连接获取时间为当前时间 poolableConnection.traceEnable = true; //这个设置为true，则在归还该连接时会在activeConnections里清除掉该连接对象 activeConnectionLock.lock(); try { activeConnections.put(poolableConnection, PRESENT); } finally { activeConnectionLock.unlock(); } } if (!this.defaultAutoCommit) { //默认是不开事务的，所以这里是true，不会触发下面的逻辑；这个不建议手动设置默认值，一般开启事务的工作自己做或者交给第三方框架（如spring）做比较好 poolableConnection.setAutoCommit(false); } return poolableConnection; //最终返回可用连接 } } 上述为获取连接时的流程图，首先会调用init进行连接池的初始化，然后运行责任链上的每一个filter，最终执行getConnectionDirect获取真正的连接对象，如果开启了testOnBorrow，则每次都会去测试连接是否可用 这也是官方不建议设置testOnBorrow为true的原因，影响性能，这里的测试是指测试mysql服务端的长连接是否断开，一般mysql服务端长连保活时间是8h，被使用一次则刷新一次使用时间，若一个连接距离上次被使用超过了保活时间，那么再次使用时将无法与mysql服务端通信 如果testOnBorrow没有被置为true，则会进行testWhileIdle的检查（这一项官方建议设置为true，缺省值也是true），检查时会判断当前连接对象距离上次被使用的时间是否超过规定检查的时间，若超过，则进行检查一次，这个检查时间通过timeBetweenEvictionRunsMillis来控制，默认60s，每个连接对象会记录下上次被使用的时间，用当前时间减去上一次的使用时间得出闲置时间，闲置时间再跟timeBetweenEvictionRunsMillis比较，超过这个时间就做一次连接可用性检查，这个相比testOnBorrow每次都检查来说，性能会提升很多，用的时候无需关注该值，因为缺省值是true，经测试如果将该值设置为false，testOnBorrow也设置为false，数据库服务端长连保活时间改为60s，60s内不使用连接，超过60s后使用将会报连接错误。若使用testConnectionInternal方法测试长连接结果为false，则证明该连接已被服务端断开或者有其他的网络原因导致该连接不可用，则会触发discardConnection进行连接回收（对应流程1.4，因为丢弃了一个连接，因此该方法会唤醒主流程3进行检查是否需要新建连接）。整个流程运行在一个死循环内，直到取到可用连接或者超过重试上限报错退出（在连接没有超过连接池上限的话，最多重试一次（重试次数默认重试1次，可以通过notFullTimeoutRetryCount属性来控制），所以取连接这里一旦发生等待，在连接池没有满的情况下，最大等待2 × maxWait的时间 ←这个有待验证）。 特别说明项 为了保证性能，不建议将testOnBorrow设置为true，或者说牵扯到长连接可用检测的那几项配置使用druid默认的配置就可以保证性能是最好的，如上所说，默认长连接检查是60s一次，所以不启用testOnBorrow的情况下要想保证万无一失，自己要确认下所连的那个mysql服务端的长连接保活时间（虽然默认是8h，但是dba可能给测试环境设置的时间远小于这个时间，所以如果这个时间小于60s，就需要手动设置timeBetweenEvictionRunsMillis了，如果mysql服务端长连接时间是8h或者更长，则用默认值即可。 为了防止不必要的扩容，在mysql服务端长连接够用的情况下，对于一些qps较高的服务、网关业务，建议把池子的最小闲置连接数minIdle和最大连接数maxActive设置成一样的，且按照需要调大，且开启keepAlive进行连接活性检查（参考流程4.1），这样就不会后期发生动态新建连接的情况（建连还是个比较重的操作，所以不如一开始就申请好所有需要的连接，个人意见，仅供参考），但是像管理后台这种，长期qps非常低，但是有的时候需要用管理后台做一些巨大的操作（比如导数据什么的）导致需要的连接暴增，且管理后台不会特别要求性能，就适合将minIdle的值设置的比maxActive小，这样不会造成不必要的连接浪费，也不会在需要暴增连接的时候无法动态扩增连接。 二、主流程2：初始化连接池通过上面的流程图可以看到，在获取一个连接的时候首先会检查连接池是否已经初始化完毕（通过inited来控制，bool类型，未初始化为flase，初始化完毕为true，这个判断过程在init方法内完成），若没有初始化，则调用init进行初始化（图主流程1中的紫色部分），下面来看看init方法里又做了哪些操作： 上述流程对应源代码如下（请展开）： 代码段2-1 >folded123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295public void init() throws SQLException { if (inited) { return; //如果已经被初始化过，则终止该方法 } // bug fixed for dead lock, for issue #2980 DruidDriver.getInstance(); final ReentrantLock lock = this.lock; //获取重入锁 try { lock.lockInterruptibly(); } catch (InterruptedException e) { throw new SQLException(\"interrupt\", e); } boolean init = false; try { if (inited) { //双重检查 return; } initStackTrace = Utils.toString(Thread.currentThread().getStackTrace()); this.id = DruidDriver.createDataSourceId(); //生成连接池id if (this.id &gt; 1) { //生成其他对象的id，比如连接对象的id、statement对象的id long delta = (this.id - 1) * 100000; this.connectionIdSeedUpdater.addAndGet(this, delta); this.statementIdSeedUpdater.addAndGet(this, delta); this.resultSetIdSeedUpdater.addAndGet(this, delta); this.transactionIdSeedUpdater.addAndGet(this, delta); } if (this.jdbcUrl != null) { this.jdbcUrl = this.jdbcUrl.trim(); initFromWrapDriverUrl(); //jdbc url的头必须是jdbc:wrap-jdbc才会触发该方法里的逻辑（这个头貌似是oracle的？本篇文章仅针对mysql） } for (Filter filter : filters) { filter.init(this); //通过池对象初始化filters（因为filter里面可能会用到一些池属性） } if (this.dbType == null || this.dbType.length() == 0) { this.dbType = JdbcUtils.getDbType(jdbcUrl, null); //根据jdbc协议头分析出当前数据库的类型（本文默认mysql） } if (JdbcConstants.MYSQL.equals(this.dbType) || JdbcConstants.MARIADB.equals(this.dbType) || JdbcConstants.ALIYUN_ADS.equals(this.dbType)) { boolean cacheServerConfigurationSet = false; if (this.connectProperties.containsKey(\"cacheServerConfiguration\")) { cacheServerConfigurationSet = true; } else if (this.jdbcUrl.indexOf(\"cacheServerConfiguration\") != -1) { cacheServerConfigurationSet = true; } if (cacheServerConfigurationSet) { this.connectProperties.put(\"cacheServerConfiguration\", \"true\"); } } //下面就是对设置的这些属性合理性的判断，不符合要求的将直接抛异常 if (maxActive &lt;= 0) { throw new IllegalArgumentException(\"illegal maxActive \" + maxActive); } if (maxActive &lt; minIdle) { throw new IllegalArgumentException(\"illegal maxActive \" + maxActive); } if (getInitialSize() &gt; maxActive) { throw new IllegalArgumentException(\"illegal initialSize \" + this.initialSize + \", maxActive \" + maxActive); } if (timeBetweenLogStatsMillis &gt; 0 &amp;&amp; useGlobalDataSourceStat) { throw new IllegalArgumentException(\"timeBetweenLogStatsMillis not support useGlobalDataSourceStat=true\"); } if (maxEvictableIdleTimeMillis &lt; minEvictableIdleTimeMillis) { throw new SQLException(\"maxEvictableIdleTimeMillis must be grater than minEvictableIdleTimeMillis\"); } if (this.driverClass != null) { this.driverClass = driverClass.trim(); } //通过SPI机制加载责任链上需要执行的filter，方法详情在下面 initFromSPIServiceLoader(); //如果driver为空，加载驱动，最终将加载到的驱动注册到DriverManager上去 if (this.driver == null) { if (this.driverClass == null || this.driverClass.isEmpty()) { this.driverClass = JdbcUtils.getDriverClassName(this.jdbcUrl); //在driverClass不配置的情况下，druid会通过url来判定属于哪个driverClass } if (MockDriver.class.getName().equals(driverClass)) { //忽略 driver = MockDriver.instance; } else { if (jdbcUrl == null &amp;&amp; (driverClass == null || driverClass.length() == 0)) { throw new SQLException(\"url not set\"); } driver = JdbcUtils.createDriver(driverClassLoader, driverClass); //driverClass不为空的情况下直接触发驱动加载 } } else { //除非手动设置驱动，否则不会走这里的逻辑 if (this.driverClass == null) { this.driverClass = driver.getClass().getName(); } } initCheck(); //根据dbType的不同，来初始化一些标记字段（比如isMySql） initExceptionSorter(); //异常处理器初始化 initValidConnectionChecker(); //初始化长连接检测时所需要用到的checker的适配类型，具体实现在下面 validationQueryCheck(); //简单的检测validationQuery参数是否填写了，若没填写会打印一个错误日志，不影响主流程 if (isUseGlobalDataSourceStat()) { //默认不开启，忽略 dataSourceStat = JdbcDataSourceStat.getGlobal(); if (dataSourceStat == null) { dataSourceStat = new JdbcDataSourceStat(\"Global\", \"Global\", this.dbType); JdbcDataSourceStat.setGlobal(dataSourceStat); } if (dataSourceStat.getDbType() == null) { dataSourceStat.setDbType(this.dbType); } } else { dataSourceStat = new JdbcDataSourceStat(this.name, this.jdbcUrl, this.dbType, this.connectProperties); } dataSourceStat.setResetStatEnable(this.resetStatEnable); //下面三个数组都跟池子本身有关系，所以容量为maxActive connections = new DruidConnectionHolder[maxActive]; //初始化连接池本体 evictConnections = new DruidConnectionHolder[maxActive]; //初始化丢弃连接数组（流程4.1需要用到） keepAliveConnections = new DruidConnectionHolder[maxActive]; //初始化需要检测可用性连接数组（流程4.1要用） SQLException connectError = null; if (createScheduler != null &amp;&amp; asyncInit) { //另外一种通过线程池管理连接池的方式，默认不启用，忽略 for (int i = 0; i &lt; initialSize; ++i) { createTaskCount++; CreateConnectionTask task = new CreateConnectionTask(true); this.createSchedulerFuture = createScheduler.submit(task); } } else if (!asyncInit) { // init connections while (poolingCount &lt; initialSize) { //当池子里的连接数少于需要初始化的个数时，则需要不断新增连接填充连接池，直到等于初始化连接数 try { PhysicalConnectionInfo pyConnectInfo = createPhysicalConnection(); //直接通过驱动程序创建连接对象，参考流程2.1 DruidConnectionHolder holder = new DruidConnectionHolder(this, pyConnectInfo); //拿着驱动连接包装成holder对象 connections[poolingCount++] = holder; //生成好的连接直接往后排 } catch (SQLException ex) { LOG.error(\"init datasource error, url: \" + this.getUrl(), ex); if (initExceptionThrow) { connectError = ex; break; } else { Thread.sleep(3000); //异常报错后会休眠3s来进行下次的添加 } } } if (poolingCount &gt; 0) { poolingPeak = poolingCount; poolingPeakTime = System.currentTimeMillis(); } } createAndLogThread(); //开启打印log日志的守护线程 createAndStartCreatorThread(); //开启负责新增连接的守护线程（主流程3） createAndStartDestroyThread(); //开启负责丢弃连接的守护线程（主流程4） initedLatch.await(); //倒计数器，用来保证上面的主流程3和4两个守护线程全部开启完毕后才进行接下来的操作 init = true; initedTime = new Date(); registerMbean(); if (connectError != null &amp;&amp; poolingCount == 0) { throw connectError; } if (keepAlive) { // async fill to minIdle if (createScheduler != null) { //默认不启用该模式，忽略 for (int i = 0; i &lt; minIdle; ++i) { createTaskCount++; CreateConnectionTask task = new CreateConnectionTask(true); this.createSchedulerFuture = createScheduler.submit(task); } } else { this.emptySignal(); //keepAlive=true，主动唤起主流程3一次 } } } catch (SQLException e) { LOG.error(\"{dataSource-\" + this.getID() + \"} init error\", e); throw e; } catch (InterruptedException e) { throw new SQLException(e.getMessage(), e); } catch (RuntimeException e){ LOG.error(\"{dataSource-\" + this.getID() + \"} init error\", e); throw e; } catch (Error e){ LOG.error(\"{dataSource-\" + this.getID() + \"} init error\", e); throw e; } finally { inited = true; //初始化完成后置为true lock.unlock(); //释放锁 if (init &amp;&amp; LOG.isInfoEnabled()) { String msg = \"{dataSource-\" + this.getID(); if (this.name != null &amp;&amp; !this.name.isEmpty()) { msg += \",\"; msg += this.name; } msg += \"} inited\"; LOG.info(msg); } } } private void initFromSPIServiceLoader() { if (loadSpifilterSkip) { //默认不跳过SPI加载 return; } if (autoFilters == null) { List filters = new ArrayList(); ServiceLoader autoFilterLoader = ServiceLoader.load(Filter.class); //加载Filter的实现 for (Filter filter : autoFilterLoader) { AutoLoad autoLoad = filter.getClass().getAnnotation(AutoLoad.class); if (autoLoad != null &amp;&amp; autoLoad.value()) { filters.add(filter); } } autoFilters = filters; } for (Filter filter : autoFilters) { if (LOG.isInfoEnabled()) { LOG.info(\"load filter from spi :\" + filter.getClass().getName()); } addFilter(filter); //把通过SPI机制加载到的filter放到池子的filters里，用于后续责任链触发 } } private void initValidConnectionChecker() { //初始化checker if (this.validConnectionChecker != null) { return; } String realDriverClassName = driver.getClass().getName(); //根据驱动的class名称，来适配具体的checker实现 if (JdbcUtils.isMySqlDriver(realDriverClassName)) { this.validConnectionChecker = new MySqlValidConnectionChecker(); //假设是mysql类型的驱动，那么适配到mysql的checker，MySqlValidConnectionChecker的构造器参考下面的方法 } else if (realDriverClassName.equals(JdbcConstants.ORACLE_DRIVER) || realDriverClassName.equals(JdbcConstants.ORACLE_DRIVER2)) { this.validConnectionChecker = new OracleValidConnectionChecker(); } else if (realDriverClassName.equals(JdbcConstants.SQL_SERVER_DRIVER) || realDriverClassName.equals(JdbcConstants.SQL_SERVER_DRIVER_SQLJDBC4) || realDriverClassName.equals(JdbcConstants.SQL_SERVER_DRIVER_JTDS)) { this.validConnectionChecker = new MSSQLValidConnectionChecker(); } else if (realDriverClassName.equals(JdbcConstants.POSTGRESQL_DRIVER) || realDriverClassName.equals(JdbcConstants.ENTERPRISEDB_DRIVER)) { this.validConnectionChecker = new PGValidConnectionChecker(); } } //Mysql对应的checker构造器 public MySqlValidConnectionChecker(){ try { clazz = Utils.loadClass(\"com.mysql.jdbc.MySQLConnection\"); if (clazz == null) { clazz = Utils.loadClass(\"com.mysql.cj.jdbc.ConnectionImpl\"); } if (clazz != null) { //如果驱动程序本身有ping方法，则下面的usePingMethod设置为true，后续连接保活测试就会采用ping.invoke的方式触发。 ping = clazz.getMethod(\"pingInternal\", boolean.class, int.class); } if (ping != null) { usePingMethod = true; } } catch (Exception e) { LOG.warn(\"Cannot resolve com.mysql.jdbc.Connection.ping method. Will use 'SELECT 1' instead.\", e); } configFromProperties(System.getProperties()); } 可以看到，实例化的时候会初始化全局的重入锁lock，在初始化过程中包括后续的连接池操作都会利用该锁保证线程安全，初始化连接池的时候首先会进行双重检查是否已经初始化过，若没有，则进行连接池的初始化，这时候还会通过SPI机制额外加载责任链上的filter，但是这类filter需要在类上加上@AutoLoad注解。然后初始化了三个数组，容积都为maxActive，首先connections就是用来存放池子里连接对象的，evictConnections用来存放每次检查需要抛弃的连接（结合流程4.1理解），keepAliveConnections用于存放需要连接检查的存活连接（同样结合流程4.1理解），然后生成初始化数（initialSize）个连接，放进connections，然后生成两个必须的守护线程，用来添加连接进池以及从池子里摘除不需要的连接，这俩过程较复杂，因此拆出来单说（主流程3和主流程4）。 特别说明项 从流程上看如果一开始实例化的时候不对连接池进行初始化（这个初始化是指对池子本身的初始化，并非单纯的指druid对象属性的初始化），那么在第一次调用getConnection时就会走上图那么多逻辑，尤其是耗时较久的建立连接操作，被重复执行了很多次，导致第一次getConnection时耗时过久，如果你的程序并发量很大，那么第一次获取连接时就会因为初始化流程而发生排队，所以建议在实例化连接池后对其进行预热，通过调用init方法或者getConnection方法都可以。 在构建全局重入锁的时候，利用lock对象生成了俩Condition，对这俩Condition解释如下： 当连接池连接够用时，利用empty阻塞添加连接的守护线程（主流程3），当连接池连接不够用时，获取连接的那个线程（这里记为业务线程A）就会阻塞在notEmpty上，且唤起阻塞在empty上的添加连接的守护线程，走完添加连接的流程，走完后会重新唤起阻塞在notEmpty上的业务线程A，业务线程A就会继续尝试获取连接。 三、流程1.1：责任链 ⚠️ 这块东西结合源码看更容易理解 上述流程对应源代码如下（请展开）： 代码段-1-2 >folded1234567891011121314151617181920212223242526272829303132333435363738394041424344454647//DruidDataSource类里的方法：获取连接 public DruidPooledConnection getConnection(long maxWaitMillis) throws SQLException { init(); if (filters.size() &gt; 0) { //责任链上的filter存在 FilterChainImpl filterChain = new FilterChainImpl(this); //该类是执行整个责任链的执行者 return filterChain.dataSource_connect(this, maxWaitMillis); //每个需要执行责任链的方法，在filterChain里都可以找到映射方法，比如本方法getConnection，就对应filterChain.dataSource_connect（参考流程1.1） } else { return getConnectionDirect(maxWaitMillis); } } //FilterChainImpl类里的方法：获取连接映射方法 @Override public DruidPooledConnection dataSource_connect(DruidDataSource dataSource, long maxWaitMillis) throws SQLException { if (this.pos &lt; filterSize) { //除了FilterChainImpl里面包含一些datasource的映射方法，需要执行的filter里面也包括，比如下面的dataSource_getConnection方法 DruidPooledConnection conn = nextFilter().dataSource_getConnection(this, dataSource, maxWaitMillis); //根据下标，获取下一个filter，触发目标方法 return conn; } return dataSource.getConnectionDirect(maxWaitMillis); //执行到最后一个filter时，触发datasource，返回真正的连接 } //FilterChainImpl类里的方法：获取下一个需要执行的filter private Filter nextFilter() { return getFilters() .get(pos++); //根据游标计算 } //随便找了一个filter里的目标方法 //LogFilter类里的方法：dataSource_getConnection @Override public DruidPooledConnection dataSource_getConnection(FilterChain chain, DruidDataSource dataSource, long maxWaitMillis) throws SQLException { DruidPooledConnection conn = chain.dataSource_connect(dataSource, maxWaitMillis); //这里又会利用FilterChainImpl触发映射方法 //下面就是自己内部的一些特有逻辑，忽略 ConnectionProxy connection = (ConnectionProxy) conn.getConnectionHolder().getConnection(); if (connectionConnectAfterLogEnable &amp;&amp; isConnectionLogEnabled()) { connectionLog(\"{conn-\" + connection.getId() + \"} pool-connect\"); } return conn; //返回 } 这里对应流程1里获取连接时需要执行的责任链，每个DruidAbstractDataSource里都包含filters属性，filters是对Druid里Filters接口的实现，里面有很多对应着连接池里的映射方法，比如例子中dataSource的getConnection方法在触发的时候就会利用FilterChain把每个filter里的dataSource_getConnection给执行一遍，这里也要说明下FilterChain，通过流程1.1可以看出来，datasource是利用FilterChain来触发各个filter的执行的，FilterChain里也有一堆datasource里的映射方法，比如上图里的dataSource_connect，这个方法会把datasource里的filters全部执行一遍直到nextFilter取不到值，才会触发dataSource.getConnectionDirect，这个结合代码会比较容易理解。 四、流程1.2：从池中获取连接的流程 上述流程对应源代码如下（请展开）： 代码段1-3 >folded123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215private DruidPooledConnection getConnectionInternal(long maxWait) throws SQLException { //可用性判断 if (closed) { connectErrorCountUpdater.incrementAndGet(this); throw new DataSourceClosedException(\"dataSource already closed at \" + new Date(closeTimeMillis)); } if (!enable) { connectErrorCountUpdater.incrementAndGet(this); throw new DataSourceDisableException(); } final long nanos = TimeUnit.MILLISECONDS.toNanos(maxWait); //纳秒 final int maxWaitThreadCount = this.maxWaitThreadCount; //目前因为拿不到连接而发生阻塞的业务线程数 DruidConnectionHolder holder; for (boolean createDirect = false;;) { if (createDirect) { //模式未启用，恒等false，下面的逻辑不会触发，所以为了方便阅读，隐藏这部分代码 //代码隐藏 } try { lock.lockInterruptibly(); //锁获取 } catch (InterruptedException e) { connectErrorCountUpdater.incrementAndGet(this); throw new SQLException(\"interrupt\", e); } try { if (maxWaitThreadCount &gt; 0 &amp;&amp; notEmptyWaitThreadCount &gt;= maxWaitThreadCount) { //如果因为拿不到连接而阻塞的业务线程数达到阈值，则直接抛异常 connectErrorCountUpdater.incrementAndGet(this); throw new SQLException(\"maxWaitThreadCount \" + maxWaitThreadCount + \", current wait Thread count \" + lock.getQueueLength()); } if (onFatalError &amp;&amp; onFatalErrorMaxActive &gt; 0 &amp;&amp; activeCount &gt;= onFatalErrorMaxActive) { connectErrorCountUpdater.incrementAndGet(this); StringBuilder errorMsg = new StringBuilder(); errorMsg.append(\"onFatalError, activeCount \") .append(activeCount) .append(\", onFatalErrorMaxActive \") .append(onFatalErrorMaxActive); if (lastFatalErrorTimeMillis &gt; 0) { errorMsg.append(\", time '\") .append(StringUtils.formatDateTime19( lastFatalErrorTimeMillis, TimeZone.getDefault())) .append(\"'\"); } if (lastFatalErrorSql != null) { errorMsg.append(\", sql \\n\") .append(lastFatalErrorSql); } throw new SQLException( errorMsg.toString(), lastFatalError); } connectCount++; //连接数累加 if (createScheduler != null &amp;&amp; poolingCount == 0 &amp;&amp; activeCount &lt; maxActive &amp;&amp; creatingCountUpdater.get(this) == 0 &amp;&amp; createScheduler instanceof ScheduledThreadPoolExecutor) { ScheduledThreadPoolExecutor executor = (ScheduledThreadPoolExecutor) createScheduler; if (executor.getQueue().size() &gt; 0) { createDirect = true; //createScheduler这种异步添加模式不开启（默认不开启，本文也不是基于该模式的），createDirect永远不等于true，所以上面createDirect==true的代码不会被触发 continue; } } if (maxWait &gt; 0) { holder = pollLast(nanos); //尝试从池子里获取连接 } else { holder = takeLast(); } if (holder != null) { activeCount++; //拿到连接，activeCount累加 if (activeCount &gt; activePeak) { activePeak = activeCount; activePeakTime = System.currentTimeMillis(); } } } catch (InterruptedException e) { connectErrorCountUpdater.incrementAndGet(this); throw new SQLException(e.getMessage(), e); } catch (SQLException e) { connectErrorCountUpdater.incrementAndGet(this); throw e; } finally { lock.unlock(); } break; } if (holder == null) { //没有获取到连接，整理错误信息，抛出错误 long waitNanos = waitNanosLocal.get(); StringBuilder buf = new StringBuilder(128); buf.append(\"wait millis \")// .append(waitNanos / (1000 * 1000))// .append(\", active \").append(activeCount)// .append(\", maxActive \").append(maxActive)// .append(\", creating \").append(creatingCount)// ; if (creatingCount &gt; 0 &amp;&amp; createStartNanos &gt; 0) { long createElapseMillis = (System.nanoTime() - createStartNanos) / (1000 * 1000); if (createElapseMillis &gt; 0) { buf.append(\", createElapseMillis \").append(createElapseMillis); } } if (createErrorCount &gt; 0) { buf.append(\", createErrorCount \").append(createErrorCount); } List sqlList = this.getDataSourceStat().getRuningSqlList(); for (int i = 0; i &lt; sqlList.size(); ++i) { if (i != 0) { buf.append('\\n'); } else { buf.append(\", \"); } JdbcSqlStatValue sql = sqlList.get(i); buf.append(\"runningSqlCount \").append(sql.getRunningCount()); buf.append(\" : \"); buf.append(sql.getSql()); } String errorMessage = buf.toString(); if (this.createError != null) { throw new GetConnectionTimeoutException(errorMessage, createError); } else { throw new GetConnectionTimeoutException(errorMessage); } } holder.incrementUseCount(); DruidPooledConnection poolalbeConnection = new DruidPooledConnection(holder); //包装成目标对象 return poolalbeConnection; //返回 } //尝试从池子里获取连接 private DruidConnectionHolder pollLast(long nanos) throws InterruptedException, SQLException { long estimate = nanos; for (;;) { if (poolingCount == 0) { //池子里的空闲连接为0，说明需要通知主流程3新增连接了 emptySignal(); // empty.signal，唤起主流程3新增连接 if (failFast &amp;&amp; isFailContinuous()) { //如果置为快速结束，则不阻塞业务线程，直接抛出异常 throw new DataSourceNotAvailableException(createError); } if (estimate &lt;= 0) { waitNanosLocal.set(nanos - estimate); return null; } notEmptyWaitThreadCount++; //因为获取不到连接而陷入阻塞状态的业务线程数+1 if (notEmptyWaitThreadCount &gt; notEmptyWaitThreadPeak) { notEmptyWaitThreadPeak = notEmptyWaitThreadCount; } try { long startEstimate = estimate; estimate = notEmpty.awaitNanos(estimate); // 阻塞（挂起）estimate这么长的世界，期间如果被唤醒，则estimate就会被刷新成剩余等待时间 // recycle or // creator notEmptyWaitCount++; notEmptyWaitNanos += (startEstimate - estimate); if (!enable) { connectErrorCountUpdater.incrementAndGet(this); throw new DataSourceDisableException(); } } catch (InterruptedException ie) { notEmpty.signal(); // 期间线程被中断，则唤起一次其他处于阻塞状态的业务线程 notEmptySignalCount++; throw ie; } finally { notEmptyWaitThreadCount--; } if (poolingCount == 0) { //依然没有竞争到 if (estimate &gt; 0) { //如果目标阻塞时间（maxWait）还没有用完，则继续尝试获取 continue; } waitNanosLocal.set(nanos - estimate); return null; } } decrementPoolingCount(); //poolingCount-- DruidConnectionHolder last = connections[poolingCount]; //直接获取 connections[poolingCount] = null; //获取后意味着连接已被借出，原有位置置空 long waitNanos = nanos - estimate; //标记这次获取连接花了多长时间，连接够用时便为0 last.setLastNotEmptyWaitNanos(waitNanos); return last; //返回 } } 通过getConnectionInternal方法从池子里获取真正的连接对象，druid支持两种方式新增连接，一种是通过开启不同的守护线程通过await、signal通信实现（本文启用的方式，也是默认的方式），另一种是直接通过线程池异步新增，这个方式通过在初始化druid时传入asyncInit=true，再把一个线程池对象赋值给createScheduler，就成功启用了这种模式，没仔细研究这种方式，所以本文的流程图和代码块都会规避这个模式。 上面的流程很简单，连接足够时就直接poolingCount-1，数组取值，返回，activeCount+1，整体复杂度为O(1)，关键还是看取不到连接时的做法，取不到连接时，druid会先唤起新增连接的守护线程新增连接，然后陷入等待状态，然后唤醒该等待的点有两处，一个是用完了连接recycle（主流程5）进池子后触发，另外一个就是新增连接的守护线程成功新增了一个连接后触发，await被唤起后继续加入锁竞争，然后往下走如果发现池子里的连接数仍然是0（说明在唤醒后参与锁竞争里刚被放进来的连接又被别的线程拿去了），则继续下一次的await，这里采用的是awaitNanos方法，初始值是maxWait，然后下次被刷新后就是maxWait减去上次阻塞花费的实际时间，每次await的时间会逐步减少，直到归零，整体时间是约等于maxWait的，但实际比maxActive要大，因为程序本身存在耗时以及被唤醒后又要参与锁竞争导致也存在一定的耗时。 如果最终都没办法拿到连接则返回null出去，紧接着触发主流程1中的重试逻辑。 druid如何防止在获取不到连接时阻塞过多的业务线程？ 通过上面的流程图和流程描述，如果非常极端的情况，池子里的连接完全不够用时，会阻塞过多的业务线程，甚至会阻塞超过maxWait这么久，有没有一种措施是可以在连接不够用的时候控制阻塞线程的个数，超过这个限制后直接报错，而不是陷入等待呢？ druid其实支持这种策略的，在maxWaitThreadCount属性为默认值（-1）的情况下不启用，如果maxWaitThreadCount配置大于0，表示启用，这是druid做的一种丢弃措施，如果你不希望在池子里的连接完全不够用导阻塞的业务线程过多，就可以考虑配置该项，这个属性的意思是说在连接不够用时最多让多少个业务线程发生阻塞，流程1.2的图里没有体现这个开关的用途，可以在代码里查看，每次在pollLast方法里陷入等待前会把属性notEmptyWaitThreadCount进行累加，阻塞结束后会递减，由此可见notEmptyWaitThreadCount就是表示当前等待可用连接时阻塞的业务线程的总个数，而getConnectionInternal在每次调用pollLast前都会判断这样一段代码： 代码块112345if (maxWaitThreadCount &gt; 0 &amp;&amp; notEmptyWaitThreadCount &gt;= maxWaitThreadCount) { connectErrorCountUpdater.incrementAndGet(this); throw new SQLException(\"maxWaitThreadCount \" + maxWaitThreadCount + \", current wait Thread count \" + lock.getQueueLength()); //直接抛异常，而不是陷入等待状态阻塞业务线程 } 可以看到，如果配置了maxWaitThreadCount所限制的等待线程个数，那么会直接判断当前陷入等待的业务线程是否超过了maxWaitThreadCount，一旦超过甚至不触发pollLast的调用（防止新增等待线程），直接抛错。 一般情况下不需要启用该项，一定要启用建议考虑好maxWaitThreadCount的取值，一般来说发生大量等待说明代码里存在不合理的地方：比如典型的连接池基本配置不合理，高qps的系统里maxActive配置过小；比如借出去的连接没有及时close归还；比如存在慢查询或者慢事务导致连接借出时间过久。这些要比配置maxWaitThreadCount更值得优先考虑，当然配置这个做一个极限保护也是没问题的，只是要结合实际情况考虑好取值。 五、流程1.3：连接可用性测试①init-checker讲这块的东西之前，先来了解下如何初始化检测连接用的checker，整个流程参考下图： ps：上述流程对应源代码位置：代码段2-1中的initValidConnectionChecker方法与MySqlValidConnectionChecker构造器 初始化checker发生在init阶段（限于篇幅，没有在主流程2（init阶段）里体现出来，只需要记住初始化checker也是发生在init阶段就好），druid支持多种数据库的连接源，所以checker针对不同的驱动程序都做了适配，所以才看到图中checker有不同的实现，我们根据加载到的驱动类名匹配不同的数据库checker，上图匹配至mysql的checker，checker的初始化里做了一件事情，就是判断驱动内是否有ping方法（jdbc4开始支持，mysql-connector-java早在3.x的版本就有ping方法的实现了），如果有，则把usePingMethod置为true，用于后续启用checker时做判断用（下面会讲，这里置为true，则通过反射的方式调用驱动程序的ping方法，如果为false，则触发普通的SELECT 1查询检测，SELECT 1就是我们非常熟悉的那个东西啦，新建statement，然后执行SELECT 1，然后再判断连接是否可用）。 ②testConnectionInternal然后回到本节探讨的方法：流程1.3对应的testConnectionInternal 上述流程对应源代码如下（请展开）： 代码段1-4 >folded123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137//数据库连接可用性测试 protected boolean testConnectionInternal(DruidConnectionHolder holder, Connection conn) { String sqlFile = JdbcSqlStat.getContextSqlFile(); String sqlName = JdbcSqlStat.getContextSqlName(); if (sqlFile != null) { JdbcSqlStat.setContextSqlFile(null); } if (sqlName != null) { JdbcSqlStat.setContextSqlName(null); } try { if (validConnectionChecker != null) { //checker不为空 //checker是init（主流程2）里通过驱动进行适配的检测者，因为本篇文章基于mysql，所以假设这里适配到的checker是MySqlValidConnectionChecker类型的 boolean valid = validConnectionChecker.isValidConnection(conn, validationQuery, validationQueryTimeout); long currentTimeMillis = System.currentTimeMillis(); if (holder != null) { holder.lastValidTimeMillis = currentTimeMillis; } if (valid &amp;&amp; isMySql) { //这里在现有驱动版本的情况下拿到的lastPacketReceivedTimeMs始终小于0，因为找不到com.mysql.jdbc.MySQLConnection long lastPacketReceivedTimeMs = MySqlUtils.getLastPacketReceivedTimeMs(conn); if (lastPacketReceivedTimeMs &gt; 0) { long mysqlIdleMillis = currentTimeMillis - lastPacketReceivedTimeMs; if (lastPacketReceivedTimeMs &gt; 0 // &amp;&amp; mysqlIdleMillis &gt;= timeBetweenEvictionRunsMillis) { discardConnection(conn); String errorMsg = \"discard long time none received connection. \" + \", jdbcUrl : \" + jdbcUrl + \", jdbcUrl : \" + jdbcUrl + \", lastPacketReceivedIdleMillis : \" + mysqlIdleMillis; LOG.error(errorMsg); return false; } } } return valid; //返回验证结果 } if (conn.isClosed()) { return false; } //checker为空时，就直接利用validationQuery进行常规测试 if (null == validationQuery) { return true; //validationQuery为空就单纯返回true } Statement stmt = null; ResultSet rset = null; try { stmt = conn.createStatement(); if (getValidationQueryTimeout() &gt; 0) { stmt.setQueryTimeout(validationQueryTimeout); } rset = stmt.executeQuery(validationQuery); if (!rset.next()) { return false; //执行检测语句失败，返回false } } finally { //关闭资源 JdbcUtils.close(rset); JdbcUtils.close(stmt); } return true; //验证通过返回true } catch (Throwable ex) { // skip return false; } finally { if (sqlFile != null) { JdbcSqlStat.setContextSqlFile(sqlFile); } if (sqlName != null) { JdbcSqlStat.setContextSqlName(sqlName); } } } //MySqlValidConnectionChecker类里的验证方法 public boolean isValidConnection(Connection conn, String validateQuery, int validationQueryTimeout) throws Exception { if (conn.isClosed()) { return false; } if (usePingMethod) { //是否启用ping方法（如果驱动程序有该方法，则这里为true，一般情况下都是true） if (conn instanceof DruidPooledConnection) { conn = ((DruidPooledConnection) conn).getConnection(); } if (conn instanceof ConnectionProxy) { conn = ((ConnectionProxy) conn).getRawObject(); } if (clazz.isAssignableFrom(conn.getClass())) { if (validationQueryTimeout &lt; 0) { validationQueryTimeout = DEFAULT_VALIDATION_QUERY_TIMEOUT; } try { //ping对象是初始化时拿到驱动程序的一个Method对象，这里通过invoke触发调用 ping.invoke(conn, true, validationQueryTimeout * 1000); } catch (InvocationTargetException e) { Throwable cause = e.getCause(); if (cause instanceof SQLException) { throw (SQLException) cause; } throw e; //ping出错抛异常 } return true; //通过则返回true } } //如果不支持ping方式检测，则触发SELECT 1的方式进行检测（一般情况下不会触发，都是上面ping方式） String query = validateQuery; if (validateQuery == null || validateQuery.isEmpty()) { query = DEFAULT_VALIDATION_QUERY; } Statement stmt = null; ResultSet rs = null; try { stmt = conn.createStatement(); if (validationQueryTimeout &gt; 0) { stmt.setQueryTimeout(validationQueryTimeout); } rs = stmt.executeQuery(query); return true; } finally { JdbcUtils.close(rs); JdbcUtils.close(stmt); } } 这个方法会利用主流程2（init阶段）里初始化好的checker对象（流程参考init-checker）里的isValidConnection方法，如果启用ping，则该方法会利用invoke触发驱动程序里的ping方法，如果不启用ping，就采用SELECT 1方式（从init-checker里可以看出启不启用取决于加载到的驱动程序里是否存在相应的方法）。 六、流程1.4：抛弃连接 上述流程对应源代码如下（请展开）： 代码段1-5 >folded12345678910111213141516//丢弃连接 public void discardConnection(Connection realConnection) { JdbcUtils.close(realConnection); //close掉真正的连接对象，一般调用该方法传入的connection对象都是最原始的驱动连接对象，所以这里并不会触发recycle lock.lock(); try { activeCount--; //活跃连接数-1 discardCount++; //丢弃连接数+1 if (activeCount &lt;= minIdle) { emptySignal(); //唤起一次主流程3新增连接 } } finally { lock.unlock(); } } 经过流程1.3返回的测试结果，如果发现连接不可用，则直接触发抛弃连接逻辑，这个过程非常简单，如上图所示，由流程1.2获取到该连接时累加上去的activeCount，在本流程里会再次减一，表示被取出来的连接不可用，并不能active状态。其次这里的close是拿着驱动那个连接对象进行close，正常情况下一个连接对象会被druid封装成DruidPooledConnection对象，内部持有的conn就是真正的驱动Connection对象，上图中的关闭连接就是获取的该对象进行close，如果使用包装类DruidPooledConnection进行close，则代表回收连接对象（recycle，参考主流程5）。 七、主流程3：添加连接的守护线程 上述流程对应源代码如下（请展开）： 代码段3-1 >folded123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182//DruidDataSource的内部类，对应主流程3，用来补充连接 public class CreateConnectionThread extends Thread { public CreateConnectionThread(String name){ super(name); //重置线程名称 this.setDaemon(true); //标记为守护线程 } //run方法 public void run() { initedLatch.countDown(); //通知init（主流程2）自己已经启动成功 long lastDiscardCount = 0; int errorCount = 0; for (;;) { //死循环 // addLast try { lock.lockInterruptibly(); //锁获取 } catch (InterruptedException e2) { break; } long discardCount = DruidDataSource.this.discardCount; //当前丢弃连接数与最后一次丢弃连接数的差值大于0，说明又发生了丢弃连接的现象，该条件会促进连接的创建 boolean discardChanged = discardCount - lastDiscardCount &gt; 0; lastDiscardCount = discardCount; try { boolean emptyWait = true; if (createError != null &amp;&amp; poolingCount == 0 &amp;&amp; !discardChanged) { emptyWait = false; } if (emptyWait &amp;&amp; asyncInit &amp;&amp; createCount &lt; initialSize) { emptyWait = false; } if (emptyWait) { // 必须存在线程等待，才创建连接，否则不创建 if (poolingCount &gt;= notEmptyWaitThreadCount &amp;&amp; (!(keepAlive &amp;&amp; activeCount + poolingCount &lt; minIdle)) &amp;&amp; !isFailContinuous() ) { empty.await(); //不需要创建连接时，阻塞（挂起） } // 防止创建超过maxActive数量的连接 if (activeCount + poolingCount &gt;= maxActive) { empty.await(); //超出限制依然挂起，不再新增连接 continue; } } } catch (InterruptedException e) { lastCreateError = e; lastErrorTimeMillis = System.currentTimeMillis(); if (!closing) { LOG.error(\"create connection Thread Interrupted, url: \" + jdbcUrl, e); } break; } finally { lock.unlock(); //锁释放 } //从上面的程序走到这里，说明该线程被成功唤起，则进行新建连接 PhysicalConnectionInfo connection = null; try { connection = createPhysicalConnection(); //利用驱动程序新建物理连接 } catch (SQLException e) { LOG.error(\"create connection SQLException, url: \" + jdbcUrl + \", errorCode \" + e.getErrorCode() + \", state \" + e.getSQLState(), e); errorCount++; if (errorCount &gt; connectionErrorRetryAttempts &amp;&amp; timeBetweenConnectErrorMillis &gt; 0) { // fail over retry attempts setFailContinuous(true); if (failFast) { lock.lock(); try { notEmpty.signalAll(); } finally { lock.unlock(); } } if (breakAfterAcquireFailure) { break; } try { Thread.sleep(timeBetweenConnectErrorMillis); } catch (InterruptedException interruptEx) { break; } } } catch (RuntimeException e) { LOG.error(\"create connection RuntimeException\", e); setFailContinuous(true); continue; } catch (Error e) { LOG.error(\"create connection Error\", e); setFailContinuous(true); break; } if (connection == null) { continue; //新建失败后再次尝试 } boolean result = put(connection); //尝试放入池子 if (!result) { JdbcUtils.close(connection.getPhysicalConnection()); LOG.info(\"put physical connection to pool failed.\"); } errorCount = 0; // reset errorCount } } } //这一个put方法是上面触发接收PhysicalConnectionInfo类型连接用的，之前说过，最终保存在池子里的连接对象都是DruidConnectionHolder类型，所以这里时进行一次包装，然后真正put进去的是更下面的put方法 protected boolean put(PhysicalConnectionInfo physicalConnectionInfo) { DruidConnectionHolder holder = null; try { //包装成holder类型 holder = new DruidConnectionHolder(DruidDataSource.this, physicalConnectionInfo); } catch (SQLException ex) { lock.lock(); try { if (createScheduler != null) { createTaskCount--; } } finally { lock.unlock(); } LOG.error(\"create connection holder error\", ex); return false; } return put(holder); //真正放入池子 } //真正将连接对象放入池子 private boolean put(DruidConnectionHolder holder) { lock.lock(); try { if (poolingCount &gt;= maxActive) { return false; //如果此时发现当前池子里的闲置连接数已经超过了maxActive，那么就不再往里面加了 } connections[poolingCount] = holder; //加在数组尾部 incrementPoolingCount(); //poolingCount++ if (poolingCount &gt; poolingPeak) { poolingPeak = poolingCount; poolingPeakTime = System.currentTimeMillis(); } notEmpty.signal(); //唤起一个因为拿不到连接对象而发生阻塞的业务线程，让其再次进入运行状态，进行获取连接竞争 notEmptySignalCount++; if (createScheduler != null) { //模式未启用 createTaskCount--; if (poolingCount + createTaskCount &lt; notEmptyWaitThreadCount // &amp;&amp; activeCount + poolingCount + createTaskCount &lt; maxActive) { emptySignal(); } } } finally { lock.unlock(); } return true; } 在主流程2（init初始化阶段）时就开启了该流程，该流程独立运行，大部分时间处于等待状态，不会抢占cpu，但是当连接不够用时，就会被唤起追加连接，成功创建连接后将会唤醒其他正在等待获取可用连接的线程，比如： 结合流程1.2来看，当连接不够用时，会通过empty.signal唤醒该线程进行补充连接（阻塞在empty上的线程只有主流程3的单线程），然后通过notEmpty阻塞自己，当该线程补充连接成功后，又会对阻塞在notEmpty上的线程进行唤醒，让其进入锁竞争状态，简单理解就是一个生产-消费模型。这里有一些细节，比如池子里的连接使用中（activeCount）加上池子里剩余连接数（poolingCount）就是指当前一共生成了多少个连接，这个数不能比maxActive还大，如果比maxActive还大，则再次陷入等待。而在往池子里put连接时，则判断poolingCount是否大于maxActive来决定最终是否入池。 八、主流程4：抛弃连接的守护线程 上述流程对应源代码如下（请展开）： 代码段4-1 >folded123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354//连接池瘦身，参考主流程4 public class DestroyConnectionThread extends Thread { public DestroyConnectionThread(String name){ super(name); //给线程重命名 this.setDaemon(true); //标记为守护线程 } //run方法 public void run() { initedLatch.countDown(); //通知init（主流程2）自己已经启动成功 for (;;) { //死循环 // 从前面开始删除 try { if (closed) { break; } if (timeBetweenEvictionRunsMillis &gt; 0) { //检查时间间隔，不启用（小于等于0时）则默认1s，事实上，druid对于该参数的缺省值是60s Thread.sleep(timeBetweenEvictionRunsMillis); } else { Thread.sleep(1000); //默认1s } if (Thread.interrupted()) { break; } destroyTask.run(); //启动destroy的run方法（在下方） } catch (InterruptedException e) { break; } } } } //DruidDataSource内部类 public class DestroyTask implements Runnable { public DestroyTask() { } @Override public void run() { shrink(true, keepAlive); //连接池的检查&amp;瘦身 if (isRemoveAbandoned()) { //如果开启该属性，则进行强制回收检查 removeAbandoned(); } } } 流程4.1：连接池瘦身，检查连接是否可用以及丢弃多余连接整个过程如下： 上述流程对应源代码如下（请展开）： 代码段-4-2 >folded123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164//连接池瘦身 public void shrink(boolean checkTime, boolean keepAlive) { try { lock.lockInterruptibly(); } catch (InterruptedException e) { return; } int evictCount = 0; int keepAliveCount = 0; try { if (!inited) { return; } final int checkCount = poolingCount - minIdle; //根据poolingCount和minIdle计算出evictCheck的范围 final long currentTimeMillis = System.currentTimeMillis(); for (int i = 0; i &lt; poolingCount; ++i) { //开始遍历连接池里闲置的连接 DruidConnectionHolder connection = connections[i]; if (checkTime) { //除非手动调用，不然经过主流程4触发，一般为true if (phyTimeoutMillis &gt; 0) { //默认不启用，忽略 long phyConnectTimeMillis = currentTimeMillis - connection.connectTimeMillis; if (phyConnectTimeMillis &gt; phyTimeoutMillis) { evictConnections[evictCount++] = connection; continue; } } //计算闲置时间 long idleMillis = currentTimeMillis - connection.lastActiveTimeMillis; if (idleMillis &lt; minEvictableIdleTimeMillis &amp;&amp; idleMillis &lt; keepAliveBetweenTimeMillis ) { //如果闲置时间达不到检测&amp;瘦身的阈值，则不处理 break; } if (idleMillis &gt;= minEvictableIdleTimeMillis) { if (checkTime &amp;&amp; i &lt; checkCount) { //达到需要丢弃的阈值时，则判断连接下标是否在evictCheck范围，若在，则视为“可以丢弃的对象”放入evictConnections数组 evictConnections[evictCount++] = connection; continue; } else if (idleMillis &gt; maxEvictableIdleTimeMillis) { //达到必须要丢弃的阈值时，则不管是不是在evictCheck范围内，都直接放入“可以丢弃的对象”的evictConnections数组 evictConnections[evictCount++] = connection; continue; } } //如果上面的条件均没有命中，如果keepAlive为true，则判断是不是超过了闲置连接检查其活性的频次阈值（即由keepAliveBetweenTimeMillis控制） if (keepAlive &amp;&amp; idleMillis &gt;= keepAliveBetweenTimeMillis) { keepAliveConnections[keepAliveCount++] = connection; //满足条件则视为“需要检测活性的对象”，放入keepAliveConnections数组 } } else { if (i &lt; checkCount) { evictConnections[evictCount++] = connection; } else { break; } } } int removeCount = evictCount + keepAliveCount; //这一批需要移除特殊处理的连接总数 if (removeCount &gt; 0) { System.arraycopy(connections, removeCount, connections, 0, poolingCount - removeCount); //根据当前移除的元素，把剩余的元素移动至数组首部（参考流程4.1） Arrays.fill(connections, poolingCount - removeCount, poolingCount, null); //剩余位置清空 poolingCount -= removeCount; } keepAliveCheckCount += keepAliveCount; } finally { lock.unlock(); } if (evictCount &gt; 0) { //如果需要丢弃的连接数量大于0 for (int i = 0; i &lt; evictCount; ++i) { DruidConnectionHolder item = evictConnections[i]; Connection connection = item.getConnection(); JdbcUtils.close(connection); //直接关闭连接（这里是直接关闭驱动连接，不再放回池子） destroyCountUpdater.incrementAndGet(this); } Arrays.fill(evictConnections, null); //将evictConnections数组重新置空（方便下次使用） } if (keepAliveCount &gt; 0) { //检测那些需要判活的连接数 // keep order for (int i = keepAliveCount - 1; i &gt;= 0; --i) { DruidConnectionHolder holer = keepAliveConnections[i]; Connection connection = holer.getConnection(); holer.incrementKeepAliveCheckCount(); boolean validate = false; try { this.validateConnection(connection); //检测其活性 validate = true; } catch (Throwable error) { if (LOG.isDebugEnabled()) { LOG.debug(\"keepAliveErr\", error); } // skip } boolean discard = !validate; if (validate) { //检测通过 holer.lastKeepTimeMillis = System.currentTimeMillis(); boolean putOk = put(holer); //检测通过后，再次放入池子 if (!putOk) { //放不进去池子（说明已经达到连接池最大连接数阈值maxActive），则视为可以“直接抛弃”的连接 discard = true; } } if (discard) { try { connection.close(); //如果可以抛弃，则直接关闭连接（直接调用驱动的close） } catch (Exception e) { // skip } lock.lock(); try { discardCount++; //抛弃连接数累加 if (activeCount &lt;= minIdle) { emptySignal(); //唤起主流程3追加连接对象 } } finally { lock.unlock(); } } } this.getDataSourceStat().addKeepAliveCheckCount(keepAliveCount); Arrays.fill(keepAliveConnections, null); //将keepAliveConnections数组重新置空（方便下次使用） } } //上面检测通过，再次通过该方法重新把连接放入池子 private boolean put(DruidConnectionHolder holder) { lock.lock(); try { if (poolingCount &gt;= maxActive) { return false; //若池子内闲置连接数超过maxActive，则无法继续添加新的连接进来，返回false } connections[poolingCount] = holder; //否则直接把此连接对象放入连接池队尾 incrementPoolingCount(); //poolingCount++ if (poolingCount &gt; poolingPeak) { poolingPeak = poolingCount; poolingPeakTime = System.currentTimeMillis(); } notEmpty.signal(); //唤起那些因获取不到可用连接而陷入阻塞状态的业务线程一次 notEmptySignalCount++; if (createScheduler != null) { //不启用该模式，忽略 createTaskCount--; if (poolingCount + createTaskCount &lt; notEmptyWaitThreadCount // &amp;&amp; activeCount + poolingCount + createTaskCount &lt; maxActive) { emptySignal(); } } } finally { lock.unlock(); } return true; } 整个流程分成图中主要的几步，首先利用poolingCount减去minIdle计算出需要做丢弃检查的连接对象区间，意味着这个区间的对象有被丢弃的可能，具体要不要放进丢弃队列evictConnections，要判断两个属性： minEvictableIdleTimeMillis：最小检查间隙，缺省值30min，官方解释：一个连接在池中最小生存的时间（结合检查区间来看，闲置时间超过这个时间，才会被丢弃）。 maxEvictableIdleTimeMillis：最大检查间隙，缺省值7h，官方解释：一个连接在池中最大生存的时间（无视检查区间，只要闲置时间超过这个时间，就一定会被丢弃）。 如果当前连接对象闲置时间超过minEvictableIdleTimeMillis且下标在evictCheck区间内，则加入丢弃队列evictConnections，如果闲置时间超过maxEvictableIdleTimeMillis，则直接放入evictConnections（一般情况下会命中第一个判断条件，除非一个连接不在检查区间，且闲置时间超过maxEvictableIdleTimeMillis）。 如果连接对象不在evictCheck区间内，且keepAlive属性为true，则判断该对象闲置时间是否超出keepAliveBetweenTimeMillis（缺省值60s），若超出，则意味着该连接需要进行连接可用性检查，则将该对象放入keepAliveConnections队列。 两个队列赋值完成后，则池子会进行一次压缩，没有涉及到的连接对象会被压缩到队首。 然后就是处理evictConnections和keepAliveConnections两个队列了，evictConnections里的对象会被close最后释放掉，keepAliveConnections里面的对象将会其进行检测（流程参考流程1.3的isValidConnection），碰到不可用的连接会调用discard（流程1.4）抛弃掉，可用的连接会再次被放进连接池。 整个流程可以看出，连接闲置后，也并非一下子就减少到minIdle的，如果之前产生一堆的连接（不超过maxActive），突然闲置了下来，则至少需要花minEvictableIdleTimeMillis的时间才可以被移出连接池，如果一个连接闲置时间超过maxEvictableIdleTimeMillis则必定被回收，所以极端情况下（比如一个连接池从初始化后就没有再被使用过），连接池里并不会一直保持minIdle个连接，而是一个都没有，生产环境下这是非常不常见的，默认的maxEvictableIdleTimeMillis都有7h除非是极度冷门的系统才会出现这种情况，而开启keepAlive也不会推翻这个规则，keepAlive的优先级是低于maxEvictableIdleTimeMillis的，keepAlive只是保证了那些检查中不需要被移出连接池的连接在指定检测时间内去检测其连接活性，从而决定是否放入池子或者直接discard。 流程4.2：主动回收连接，防止内存泄漏过程如下： 上述流程对应源代码如下（请展开）： 代码段4-3 >folded1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283//回收长期未归还的连接（再次说明：该方法仅在removeAbandoned设置为true的情况下触发） public int removeAbandoned() { int removeCount = 0; long currrentNanos = System.nanoTime(); //这个列表用于存放满足条件的真正需要强制回收的连接 List abandonedList = new ArrayList(); activeConnectionLock.lock(); try { //在removeAbandoned设置为true的情况下，所有被借出去的连接，都会被保存进activeConnections（参考主流程1），所以要进行“长期未归还”的检查，就是从activeConnections开始的 Iterator iter = activeConnections.keySet().iterator(); for (; iter.hasNext();) { DruidPooledConnection pooledConnection = iter.next(); if (pooledConnection.isRunning()) { continue; //如果当前连接正在使用中（指的是正在execute），则不处理 } //利用当前时间和连接被借出去时的时间，计算出连接被借出去的时间有多久 long timeMillis = (currrentNanos - pooledConnection.getConnectedTimeNano()) / (1000 * 1000); if (timeMillis &gt;= removeAbandonedTimeoutMillis) { //如果连接被借出去的时间超过removeAbandonedTimeoutMillis这个阈值，将会命中“主动归还”的逻辑检查 iter.remove(); //先从activeConnections移除 pooledConnection.setTraceEnable(false); //标记为false，防止回收时重复removeactiveConnections，可以参考主流程5 abandonedList.add(pooledConnection); //放入“强制回收”队列 } } } finally { activeConnectionLock.unlock(); } if (abandonedList.size() &gt; 0) { //如果“强制回收”队列大于0，说明有需要回收的连接 for (DruidPooledConnection pooledConnection : abandonedList) { //循环这些连接 final ReentrantLock lock = pooledConnection.lock; lock.lock(); //拿到连接的锁 try { if (pooledConnection.isDisable()) { continue; //已经被回收的，则不管 } } finally { lock.unlock(); } //触发回收连接对象（pooledConnection）里的holcder（注意这里其实是把pooledConnection对象里的holder给回收至连接池了，pooledConnection对象本身会被销毁） JdbcUtils.close(pooledConnection); //这里触发的close，是DruidPooledConnection的close，也就是会触发recycle方法的close pooledConnection.abandond(); //标记为 removeAbandonedCount++; removeCount++; if (isLogAbandoned()) { //日志打印，忽略 StringBuilder buf = new StringBuilder(); buf.append(\"abandon connection, owner thread: \"); buf.append(pooledConnection.getOwnerThread().getName()); buf.append(\", connected at : \"); buf.append(pooledConnection.getConnectedTimeMillis()); buf.append(\", open stackTrace\\n\"); StackTraceElement[] trace = pooledConnection.getConnectStackTrace(); for (int i = 0; i &lt; trace.length; i++) { buf.append(\"\\tat \"); buf.append(trace[i].toString()); buf.append(\"\\n\"); } buf.append(\"ownerThread current state is \" + pooledConnection.getOwnerThread().getState() + \", current stackTrace\\n\"); trace = pooledConnection.getOwnerThread().getStackTrace(); for (int i = 0; i &lt; trace.length; i++) { buf.append(\"\\tat \"); buf.append(trace[i].toString()); buf.append(\"\\n\"); } LOG.error(buf.toString()); } } } return removeCount; //返回本次被强制回收的连接个数 } 这个流程在removeAbandoned设置为true的情况下才会触发，用于回收那些拿出去的使用长期未归还（归还：调用close方法触发主流程5）的连接。 先来看看activeConnections是什么，activeConnections用来保存当前从池子里被借出去的连接，这个可以通过主流程1看出来，每次调用getConnection时，如果开启removeAbandoned，则会把连接对象放到activeConnections，然后如果长期不调用close，那么这个被借出去的连接将永远无法被重新放回池子，这是一件很麻烦的事情，这将存在内存泄漏的风险，因为不close，意味着池子会不断产生新的连接放进connections，不符合连接池预期（连接池出发点是尽可能少的创建连接），然后之前被借出去的连接对象还有一直无法被回收的风险，存在内存泄漏的风险，因此为了解决这个问题，就有了这个流程，流程整体很简单，就是将现在借出去还没有归还的连接，做一次判断，符合条件的将会被放进abandonedList进行连接回收（这个list里的连接对象里的abandoned将会被置为true，标记已被该流程处理过，防止主流程5再次处理，具体可以参考代码段5-1）。 这个如果在实践中能保证每次都可以正常close，完全不用设置removeAbandoned=true，目前如果使用了类似mybatis、spring等开源框架，框架内部是一定会close的，所以此项是不建议设置的，视情况而定。 九、主流程5：回收连接这个流程通常是靠连接包装类DruidPooledConnection的close方法触发的，目标方法为recycle，流程图如下： 上述流程对应源代码如下（请展开）： 代码段5-1 >folded123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280//DruidPooledConnection类的close方法 @Override public void close() throws SQLException { if (this.disable) { //检查，因为该连接对象是抛出去给别的业务线程使用，也就是说并不受连接池本身管控，所以很可能存在多线程同时close的操作，因此这里需要做一层检查，包括下方的syncClose里的检查也是一个意思 return; } DruidConnectionHolder holder = this.holder; //拿到对应的holder对象（之前说过，这个对象才是最后放进连接池的对象） if (holder == null) { if (dupCloseLogEnable) { LOG.error(\"dup close\"); } return; } DruidAbstractDataSource dataSource = holder.getDataSource(); //拿到对应的连接池对象 boolean isSameThread = this.getOwnerThread() == Thread.currentThread(); if (!isSameThread) { //关闭该连接与获取该连接的线程并非同一个的时候，则触发下面的syncClose dataSource.setAsyncCloseConnectionEnable(true); } if (dataSource.isAsyncCloseConnectionEnable()) { syncClose(); //参考上面的解释，该方法详情在下方 return; } //一些事件监听器的触发，忽略 for (ConnectionEventListener listener : holder.getConnectionEventListeners()) { listener.connectionClosed(new ConnectionEvent(this)); } //责任链的执行，参考流程1.1与代码段1-2，运行方式是一样的，找到映射方法，整个触发一遍责任链上的filters List filters = dataSource.getProxyFilters(); if (filters.size() &gt; 0) { FilterChainImpl filterChain = new FilterChainImpl(dataSource); filterChain.dataSource_recycle(this); } else { recycle(); //触发目标方法recycle } this.disable = true; //标记该连接已失效，无法再次提供服务 } //上面逻辑走syncClose的情况，该方法与上面大体相同，但由于不是同一个线程做的操作，所以这里需要锁控制 public void syncClose() throws SQLException { lock.lock(); //获取锁，这个锁是当前连接对象上的锁，为了解决同一个连接对象在不同的线程里被同时close多次而造成的线程安全问题 try { if (this.disable) { return; } DruidConnectionHolder holder = this.holder; //同样的，拿到需要归还的holder对象 if (holder == null) { if (dupCloseLogEnable) { LOG.error(\"dup close\"); } return; } //同样是一些事件监听器的触发，忽略 for (ConnectionEventListener listener : holder.getConnectionEventListeners()) { listener.connectionClosed(new ConnectionEvent(this)); } //同样的责任链的执行，参考上面的解释 DruidAbstractDataSource dataSource = holder.getDataSource(); List filters = dataSource.getProxyFilters(); if (filters.size() &gt; 0) { FilterChainImpl filterChain = new FilterChainImpl(dataSource); filterChain.dataSource_recycle(this); } else { recycle(); //触发目标方法recycle，方法详情在下方 } this.disable = true; //标记该连接已失效，无法再次提供服务 } finally { lock.unlock(); //解锁 } } //DruidPooledConnection类的recycle方法，由上面俩方法直接触发 public void recycle() throws SQLException { if (this.disable) { return; } DruidConnectionHolder holder = this.holder; //拿到真正需要归还的连接对象 if (holder == null) { if (dupCloseLogEnable) { LOG.error(\"dup close\"); } return; } if (!this.abandoned) { //如果期间已经被流程4.2处理过了（abandoned==true），则不触发下方逻辑 DruidAbstractDataSource dataSource = holder.getDataSource(); dataSource.recycle(this); //真正触发连接池的回收方法，方法详情在下方 } //连接对象一旦被回收处理，则会把所有与连接相关的属性置空（不持有），closed标记为true this.holder = null; conn = null; transactionInfo = null; closed = true; } //DruidDataSource类里的recycle方法，真正回收连接的方法，由上面DruidPooledConnection类的recycle触发 protected void recycle(DruidPooledConnection pooledConnection) throws SQLException { final DruidConnectionHolder holder = pooledConnection.holder; if (holder == null) { LOG.warn(\"connectionHolder is null\"); return; } if (logDifferentThread // &amp;&amp; (!isAsyncCloseConnectionEnable()) // &amp;&amp; pooledConnection.ownerThread != Thread.currentThread()// ) { LOG.warn(\"get/close not same thread\"); } final Connection physicalConnection = holder.conn; //拿到真正的驱动连接对象 if (pooledConnection.traceEnable) { //如果traceEnable为true（满足该属性为true，必须要removeAbandoned设置为true，这样在主流程1那里才会被放进activeConnections，才会置为true），流程4.2处理过后，会把该属性重新置为false，其他情况均为true Object oldInfo = null; activeConnectionLock.lock(); try { if (pooledConnection.traceEnable) { //双重检查 oldInfo = activeConnections.remove(pooledConnection); //从activeConnections移除，防止流程4.2的重复检查 pooledConnection.traceEnable = false; //置为false } } finally { activeConnectionLock.unlock(); } if (oldInfo == null) { if (LOG.isWarnEnabled()) { LOG.warn(\"remove abandonded failed. activeConnections.size \" + activeConnections.size()); } } } final boolean isAutoCommit = holder.underlyingAutoCommit; final boolean isReadOnly = holder.underlyingReadOnly; final boolean testOnReturn = this.testOnReturn; try { // 如果在归还至连接池时发现此连接对象还有未处理完的事务，则直接回滚 if ((!isAutoCommit) &amp;&amp; (!isReadOnly)) { pooledConnection.rollback(); } // reset holder, restore default settings, clear warnings boolean isSameThread = pooledConnection.ownerThread == Thread.currentThread(); if (!isSameThread) { //同样判断线程，为了保证安全性 final ReentrantLock lock = pooledConnection.lock; lock.lock(); try { holder.reset(); //连接被借出去后，可能被业务方改动了一些属性（典型的比如autoCommit），现在利用reset方法还原为默认值 } finally { lock.unlock(); } } else { holder.reset(); //同上，这里认为获取和关闭连接的是同一个线程，不存在线程安全问题，因此不用去竞争锁 } //连接已被抛弃，则不作任何处理（不再归还） if (holder.discard) { return; } //忽略 if (phyMaxUseCount &gt; 0 &amp;&amp; holder.useCount &gt;= phyMaxUseCount) { discardConnection(holder.conn); return; } //如果驱动连接本身被人为关闭了，除一些监控值之外，也不做处理 if (physicalConnection.isClosed()) { lock.lock(); try { activeCount--; closeCount++; } finally { lock.unlock(); } return; } //参考testOnBorrow，这里testOnReturn就是指每次回收连接都要做连接可用性检查，同样官方不建议开启，影响性能，缺省值也是不开启的 if (testOnReturn) { //流程忽略 boolean validate = testConnectionInternal(holder, physicalConnection); if (!validate) { JdbcUtils.close(physicalConnection); destroyCountUpdater.incrementAndGet(this); lock.lock(); try { activeCount--; closeCount++; } finally { lock.unlock(); } return; } } if (!enable) { //中途发现连接又被置为不可用，则直接触发抛弃方法，参考流程1.4和代码段1-5 discardConnection(holder.conn); return; } boolean result; final long currentTimeMillis = System.currentTimeMillis(); if (phyTimeoutMillis &gt; 0) { long phyConnectTimeMillis = currentTimeMillis - holder.connectTimeMillis; if (phyConnectTimeMillis &gt; phyTimeoutMillis) { discardConnection(holder.conn); return; } } lock.lock(); try { activeCount--; closeCount++; //最终放入池子，方法详情在下方 result = putLast(holder, currentTimeMillis); recycleCount++; } finally { lock.unlock(); } if (!result) { //如果加不进去，则直接关闭驱动连接，然后不处理（此时holder已经失去强引用，不久便会被回收） JdbcUtils.close(holder.conn); LOG.info(\"connection recyle failed.\"); } } catch (Throwable e) { holder.clearStatementCache(); if (!holder.discard) { this.discardConnection(physicalConnection); holder.discard = true; } LOG.error(\"recyle error\", e); recycleErrorCountUpdater.incrementAndGet(this); } } //DruidDataSource类里的putLast方法，由上方的recycle方法触发 boolean putLast(DruidConnectionHolder e, long lastActiveTimeMillis) { if (poolingCount &gt;= maxActive) { //池子已满，不加 return false; } e.lastActiveTimeMillis = lastActiveTimeMillis; //刷新上次活跃时间，该时间很重要，直接影响连接检查的触发 connections[poolingCount] = e; //放进连接池数组尾部 incrementPoolingCount(); //poolingCount++ if (poolingCount &gt; poolingPeak) { poolingPeak = poolingCount; poolingPeakTime = lastActiveTimeMillis; } notEmpty.signal(); //因为成功回收了一个连接，那就唤起一次所有因为获取不到连接而被阻塞的业务线程吧~（参考流程1.2） notEmptySignalCount++; return true; } 这也是非常重要的一个流程，连接用完要归还，就是利用该流程完成归还的动作，利用druid对外包装的Connecion包装类DruidPooledConnection的close方法触发，该方法会通过自己内部的close或者syncClose方法来间接触发dataSource对象的recycle方法，从而达到回收的目的。 最终的recycle方法： 如果removeAbandoned被设置为true，则通过traceEnable判断是否需要从activeConnections移除该连接对象，防止流程4.2再次检测到该连接对象，当然如果是流程4.2主动触发的该流程，那么意味着流程4.2里已经remove过该对象了，traceEnable会被置为false，本流程就不再触发remove了（这个流程都是在removeAbandoned=true的情况下进行的，在主流程1里连接被放进activeConnections时traceEnable被置为true，而在removeAbandoned=false的情况下traceEnable恒等于false）。 如果回收过程中发现存在有未处理完的事务，则触发回滚（比较有可能触发这一条的是流程4.2里强制归还连接，也有可能是单纯使用连接，开启事务却没有提交事务就直接close的情况），然后利用holder.reset进行恢复连接对象里一些属性的默认值，除此之外，holder对象还会把由它产生的statement对象放到自己的一个arraylist里面，reset方法会循环着关闭内部未关闭的statement对象，最后清空list，当然，statement对象自己也会记录下其产生的所有的resultSet对象，然后关闭statement时同样也会循环关闭内部未关闭的resultSet对象，这是连接池做的一种保护措施，防止用户拿着连接对象做完一些操作没有对打开的资源关闭。 判断是否开启testOnReturn，这个跟testOnBorrow一样，官方默认不开启，也不建议开启，影响性能，理由参考主流程1里针对testOnBorrow的解释。 直接放回池子（当前connections的尾部），然后需要注意的是putLast方法和put方法的不同之处，putLast会把lastActiveTimeMillis置为当前时间，也就是说不管一个连接被借出去过久，只要归还了，最后活跃时间就是当前时间，这就会有造成某种特殊异常情况的发生（非常极端，几乎不会触发，可以选择不看）： 如果不开启testOnBorrow和testOnReturn，并且keepAlive设置为false，那么长连接可用测试的间隔依据就是利用当前时间减去上次活跃时间（lastActiveTimeMillis）得出闲置时间，然后再利用闲置时间跟timeBetweenEvictionRunsMillis（默认60s）进行对比，超过才进行长连接可用测试。 那么如果一个mysql服务端的长连接保活时间被人为调整为60s，然后timeBetweenEvictionRunsMillis被设置为59s，这个设置是非常合理的，保证了测试间隔小于长连接实际保活时间，然后如果这时一个连接被拿出去后一直过了61s才被close回收，该连接对象的lastActiveTimeMillis被刷为当前时间，如果在59s内再次拿到该连接对象，就会绕过连接检查直接报连接不可用的错误。 十、尾声到这里针对druid连接池的初始化以及其内部一个连接从生产到消亡的整个流程就已经整理完了，主要是列出其运行流程以及一些主要的监控数据都是如何产生的，没有涉及到的是一个sql的执行，因为这个基本上就跟使用原生驱动程序差不多，只是druid又包装了一层Statement等，用于完成一些自己的操作。 对于druid，处理连接只是很小的一块内容，却是很核心的一块内容。 Druid地址：https://github.com/alibaba/druid","link":"/2019/08/28/%E6%B1%A0%E5%8C%96%E6%8A%80%E6%9C%AF%EF%BC%88%E4%B8%80%EF%BC%89Druid%E6%98%AF%E5%A6%82%E4%BD%95%E7%AE%A1%E7%90%86%E6%95%B0%E6%8D%AE%E5%BA%93%E8%BF%9E%E6%8E%A5%E7%9A%84%EF%BC%9F/"},{"title":"利用Spring的BeanPostProcessor来修改bean属性","text":"一、关于BeanPostProcessor1.1：它是什么？首先它是一个接口，定义了两个方法： 1234567891011public interface BeanPostProcessor { @Nullable //所有bean初始化之前触发该方法 default Object postProcessBeforeInitialization(Object bean, String beanName) throws BeansException { return bean; } @Nullable //所有bean初始化之后触发该方法 default Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { return bean; }} 代码块1 它定义了两个方法，分别是： postProcessBeforeInitialization：bean初始化前置处理 postProcessAfterInitialization：bean初始化后置处理 ⚙️ 注：这里的初始化是指一个被实例化后的bean的完成其一些初始化方法的调用（最基本的就是通过@PostConstruct预设的初始化方法），上面两个方法的before和after就是针对这个状态来区分触发时机的。 我们可以定义一个实现了该接口的bean，来达到对其他bean做一些初始化前后要做的事情。 1.2：什么时候触发？首先看下spring beans的生命周期（图片来源于网络）： 上图中标红的位置就是BeanPostProcessor两个方法的触发点，可以看到这些方法的触发是在初始化阶段。 那么，如何定义一个类似的bean的初始化阶段的后置处理器呢？很简单，让一个bean实现BeanPostProcessor接口并重写其before、after方法即可，可以搞很多个这样的bean，触发过程就是，容器里的任何bean在实例化后初始化前，都会触发一次所有实现了BeanPostProcessor接口的bean的before方法，初始化以后都会触发一次所有实现了BeanPostProcessor接口的bean的after方法，也就是说，spring在启动时，会预先加载实现了该接口的对象（通过registerBeanPostProcessors方法注册这类bean），这样，其他任何bean在初始化时，都可以通过之前已经加载好的逻辑，逐个触发一遍（当然如果想要保证实现顺序，还可以通过实现Order接口，来定义触发顺序）。 1.3：可以用来做什么？了解了它的触发时机，那么它通常可以用来做哪些事情呢？一般来说，可以利用其做一些通用性的bean属性注入，下面通过一个实例来说下其应用方式和场景。 二、使用方式实战一下，给目前项目内所有的SqlSessionFactory对象都加一个拦截器。 2.1：定义一个Mybatis拦截器现在来定义一个Mybatis里的拦截器，它的作用就是简单拿到sql，然后打印出该sql执行耗时： 12345678910111213141516171819202122232425262728293031323334@Slf4j@Intercepts({@Signature(type = StatementHandler.class, method = \"query\", args = {Statement.class, ResultHandler.class}), @Signature(type = StatementHandler.class, method = \"update\", args = {Statement.class}), @Signature(type = StatementHandler.class, method = \"batch\", args = {Statement.class})})public class SqlInterceptor implements Interceptor { @Override public Object intercept(Invocation invocation) throws Throwable { //拦截每次的sql执行 Object target = invocation.getTarget(); StatementHandler statementHandler = (StatementHandler) target; BoundSql boundSql = statementHandler.getBoundSql(); String sql = boundSql.getSql(); //获取sql long start = System.currentTimeMillis(); try { return invocation.proceed(); //sql运行 } catch (Throwable t) { System.out.println(String.format(\"错误SQL=%s\", sql)); throw t; } finally { System.out.println(String.format(\"耗时%s ms, SQL=%s\", (System.currentTimeMillis() - start), sql)); } } @Override public Object plugin(Object target) { return Plugin.wrap(target, this); } @Override public void setProperties(Properties properties) { }} 代码块2 Mybatis的拦截器需要预先往SqlSessionFactory设置： 12345678@Bean(name = \"sqlSession\") public SqlSessionFactory sqlSession(@Qualifier(\"dataSource\") DataSource dataSource) throws Exception { SqlSessionFactoryBean bean = new SqlSessionFactoryBean(); bean.setDataSource(dataSource); bean.setVfs(SpringBootVFS.class); bean.getObject().getConfiguration().addInterceptor(new SqlInterceptor()); //手动加入 return bean.getObject(); } 代码块3 2.2：借助BeanPostProcessor操作相关Bean这时项目模块如果很多，但是这个拦截器又要求对所有项目所有的SqlSessionFactory都生效，一个个去改每个项目里的SqlSessionFactory类型的bean太过繁琐，这个时候就可以在公共模块里定义一个BeanPostProcessor去干这件事，比如可以定义成下面这样： 123456789101112131415@Slf4jpublic class SqlSessionFactoryBeanPostProcessor implements BeanPostProcessor { @Override public Object postProcessAfterInitialization(Object bean, String beanName) throws BeansException { if (bean instanceof SqlSessionFactory) { //所有bean初始化之后都会进入这个方法，这个时候需要滤出需要的类型，比如这次就只需要拿到SqlSessionFactory类型的对象对其设置拦截器就行了 SqlSessionFactory nowBean = (SqlSessionFactory) bean; nowBean.getConfiguration().addInterceptor(new SqlInterceptor(nowBean //设置拦截器 .getConfiguration() .getEnvironment() .getDataSource())); } return bean; //完成后返回出去，可能直接进入容器，也可能会去执行其他的BeanPostProcessor }} 代码块4 然后再把它也定义成一个bean，其本身也是一个bean，才能被spring扫到去装载，否则只是实现BeanPostProcessor接口spring是没办法察觉做管理的： 1234567@ConditionalOnClass({SqlSessionFactory.class}) //存在SqlSessionFactory类型时，才会触发下面bean的装载public class MysqlAutoConfiguration { @Bean public SqlSessionFactoryBeanPostProcessor sqlSessionFactoryBeanPostProcessor() { return new SqlSessionFactoryBeanPostProcessor(); }} 代码块5 这样写完，就不用去一个个的改SqlSessionFactory对象了，只要引入该公共模块，那么在bean初始化完成后，就会走这段逻辑，然后滤出自己需要的类型，对其进行修改就好，这样，所有SqlSessionFactory就在不修改别的地方初始化SqlSessionFactory代码的情况下，全局生效了。","link":"/2019/07/16/%E5%88%A9%E7%94%A8Spring%E7%9A%84BeanPostProcessor%E6%9D%A5%E4%BF%AE%E6%94%B9bean%E5%B1%9E%E6%80%A7/"},{"title":"InfluxDB（一）初探时序数据库","text":"最近公司有个需求需要借助InfluxDB实现（或者更准确的说，使用该数据库可以更容易的实现），因此稍微看了下这个数据库，把比较重要的一些东西先简单记录一下，日后如果踩坑，也会继续在下面补充。 零、下载&amp;安装官方地址：https://portal.influxdata.com/downloads/ 一、什么是时序数据库，它可以用来做什么？简单来说时序数据库就是存储带有时间戳且包含随时间发生变化的数据，InfluxDB属于一种时序数据库。这类数据具体指什么数据呢？这里举几个例子： 监控，比如某一时段CPU占用率、服务QPS、服务耗时等监控数据 某飞机在运行过程中各时刻的高度、速度、机舱内温度、湿度 某地区每时每刻的室外温度记录 上述的数据都满足某一个具体的事物（监控指标、飞机、地区）随着时间的变化而需要记录的一些数据（监控数据、速度、温度等），这些数据被称为时序性数据，这些数据可以被用来做大数据分析、人工智能等。 ❓ 疑问点：通过上述的例子，这些无非是一些指标数据嘛，为什么非得用时序性数据库记录？不可以用Mysql这种关系型数据库来做吗？ 不可以，首先对于这种数据，量级上是巨大的，比如每时每刻都要记录某飞机的坐标变化，类似这种指标，量级很庞大，关系型数据库设计之初并不是为了处理这种大数据量级的，即便是后来的分表分库，也是为了优化数据量和访问量大了以后查询和写入性能问题的，所以不适合用来存储数量如此庞大的数据，所以这并不是能不能实现的问题，而是适不适合的问题，而时序型数据库的写入速度非常快，其次内部还会对存储数据进行压缩，同时提供了更高的可用性，比如数据保留策略、数据聚合函数、连续查询等。数据保留策略可以保证超过指定期限的数据被回收，释放磁盘空间。 二、InfluxDB基本概念2.1：建表不需要专门建表，一般insert执行过去时，如果发现表不存在，就自动创建。 2.2：基本概念与mysql中概念的对应关系 概念名称 概念解释 MySQL类比 database 数据库 类似mysql里的库 measurement 表 类似mysql里的表 point 一行数据记录 类似mysql里的一行数据 表1 2.3：point详解point的概念就类似于mysql里一行数据（data point，直译为数据点，可以理解成一行数据），point的构成部分有三个： point属性 属性解释 备注 time 时间戳 influxdb自带字段，单位：纳秒 tags 有各种索引的属性 可设置多个，逗号隔开 fields 没有索引的属性 可设置多个，逗号隔开 表2 tags和fileds的区别： 121. field无索引，一般表示会随着时间戳而发生改变的属性，比如温度、经纬度等，类型可以是 string, float, int, bool2. tag有索引，一般表示不会随着时间戳而发生改变的属性，比如城市、编号等，类型只可为 string 比较值得注意的是：在InfluxDB中，tags决定了series的数量，而series的数量越大，对于系统CPU和InfluxDB的性能影响越大，series估算方法下面会说。 2.4：sql语句示范建库： 1create database \"库名\" 一般来说，新建完库，还要对该库设置自己的时效策略（RP），下面会详细说明RP的设置方法，一般建好库后都会有一个默认的RP策略 插入数据： 12345insert results,hostname=index2,name=index2333 value=2,value2=4 -------- ---------------------------- ------------------------- 表名 tags的设置 fields的设置 上面这句sql的意思就是说在名为result的表里，插入两个分别命名为hostname和name取值分别为index2和index2333的tags，以及两个分别命名为value和value2取值分别为2和4的fields的数据。 ⚠️ 注意：这里的指令直接输入整数，InfluxDB是按照浮点型处理的，如果一定要让上面的value=2和value2=4中的2和4是整型数据，那么需要在后面加上修饰词：value=2i，i就代表整型。 检索数据： 1select * from results 打印结果为： 1234name: results2time hostname name value value2---- -------- ---- ----- ------1560928150794920700 index2 index2333 2 4 分页检索： 1SELECT * FROM 表 WHERE 条件 LIMIT rows OFFSET (page - 1)*rows rows代表每页展示行数，page表示页码 删除表： 1drop measurement \"表名\" 三、InfluxDB的时效设置很遗憾，InfluxDB是不支持删除和修改的，删除有专门的操作，但是性能很低，不建议使用。 InfluxDB支持定期清除数据策略。 查看当前库对应策略配置的命令为： 1show retention policies on \"库名\" 结果： 123name duration shardGroupDuration replicaN default------- --------- ------------------ --------- -------autogen 0s 168h0m0s 1 true name：过期策略名 duration：保留多长时间的数据，比如3w，意思就是把三周前的数据清除掉，只保留三周内的，支持h（小时），d（天），w（星期）这几种配法。 shardGroupDuration：存储数据的分组结构，比如设置为1d，表示的是每组存储1d的数据量，也就是说数据将按照天为单位划分存储分组，然后根据每条数据的时间戳决定把它放到哪个分组里，因此这个概念还会影响到过期策略，因为InfluxDB在清除过期数据时不可能逐条清理，而是通过清除整个ShardGroup的方式进行，因为通过跟当前时间对比就可以知道哪些分组里的数据一定是过期的，从而进行整组清理，效率往往更高。 replicaN：副本数量，一般为1个（这个大概就是备份的意思？这个配置在单实例模式下不起作用） default：是否是默认配置，设置为true表示默认的意思，主要用于查询时是否指定策略，如果不指定，则这个查询就是针对default=true的策略进行的，注意，本文章里的sql都没有指明保留策略的名字，如果有需要，那么请指定。一个库允许有多套策略配置（每套策略里都可以有自己的一份数据，比如同样一张表的数据在策略A和策略B的情况下是不同的，可以理解为一个库对应N个策略，每个策略里有自己的N多张表，相互独立），在不指定策略名称的情况下写的sql，默认使用default=true的策略。 当然，策略也支持修改，指令如下： 1alter retention policy \"autogen\" on \"test\" duration 30d default 上面这条指令就会把上面策略明为autogen的策略有效保存时间改为30天。 四、InfluxDB的存储结构4.1：结构层级了解完策略，结合上面提到的series、tags、fields等概念，画一下单个InfluxDB库的存储结构： 图里没有体现出tags、fields这些数据层面的东西，它们最终被存放在了Shard里，之前说过tags会影响series的大小，其实series就相当于一个唯一化分类，eries估算方式为： series的个数 = RP × measurement × tags（tags去重后的个数） 比如一个database中有一个measurement，叫test，有两个RP（7d, 30d），tag有host，server。host的值有hostA、hostB，server为server1，server2。那么这个database的series值为2RP x 4tags = 8 4.2：LSM-Tree回归图1前，先来了解下LSM-Tree，几乎所有的k-v存储的写密集型数据库都采用该数据结构实现，该数据结构的结构如下（注意下面说的层级并非树的层级，而是合并逻辑发生时的层级）： 如图2，该结构写入流程如下： 写入的数据首先加到0层，0层的数据存储在内存中，短期内查询的数据一般在0层，由于是内存操作，因此效率会非常高，当0层的数据达到一定大小时，此时会把0层 和位于它下面的1层进行合并，然后合并出来的新的数据（0层+1层的数据）会顺序写磁盘（这里由于是顺序写入磁盘，因此写性能会非常好），然后替换掉原来老的1层数据，当1层达到一定大小的时候，将继续和它的下层合并，以此类推，一级一级的往下递，除此之外还可以将合并之后旧文件全部删掉，留下最新的。 LSM-Tree参考文章：LSM-tree 基本原理及应用 然后回归图1，最终的数据会被存储进Shard，Shard里存在几个区域，就是最终存放数据的地方，下面针对这几个区域说明下它们的作用： 4.3：Cache相当于上面说的LSM-Tree的0层，存放于内存中，写入的数据时首先被该模块接收并存储，该模块在内存中表现为一个map结构，k = seriesKey + 分隔符 + fieldsName，v = 具体的filed对应的值的数组，具体结构体如下（参考网上资料）： 123456789101112131415161718192021type Cache struct { commit sync.Mutex mu sync.RWMutex store map[string]*entry size uint64 // 当前使用内存的大小 maxSize uint64 // 缓存最大值 // snapshots are the cache objects that are currently being written to tsm files // they're kept in memory while flushing so they can be queried along with the cache. // they are read only and should never be modified // memtable 快照，用于写入 tsm 文件，只读 snapshot *Cache snapshotSize uint64 snapshotting bool // This number is the number of pending or failed WriteSnaphot attempts since the last successful one. snapshotAttempts int stats *CacheStatistics lastSnapshot time.Time} 代码块1 基于前面说的LSM-Tree，可以知道这里的cache不是持续增长的，而是达到一定值就会进行跟下层存储在磁盘上的数据（1~n层）进行合并，然后清空cache，在InfluxDB中，这一部分存储在磁盘上的数据，就是指TSM File模块，下面会介绍。 4.4：WAL在上面对于Cache的描述中，Cache是基于内存做的写入数据接收方，那么如果中途机器宕掉，那么就会造成Cache里数据丢失的问题，为了解决这个问题，InfluxDB就设计了WAL模块，实际在写入一个数据时，不仅会先写进Cache，还会写入WAL，可以简单理解WAL就是对Cache里数据的备份，防止数据丢失，在Cache做完一次合并清除掉自身时，旧的WAL文件也会随之删除，然后新建一个WAL，迎接新一轮的写入。同样的，在InfluxDB启动时，也会先去读取WAL文件初始化Cache模块。 4.5：TSM File用于组成TSM-Tree结构的主要磁盘文件（可以对应图2的1~n层），内部做了很多存储以及压缩优化，单个TSM File的最大大小为2GB。 4.6：Compactor在后台持续运行的一个task（频率为1s），主要做以下事情： 在Cache达到阈值时，进行快照，然后将数据合并并保存在TSM File中 合并TSM File，将多个小型TSM File进行合并，使得每个文件的大小尽可能达到单个文件最大大小（也就是上面说到的2GB） 检查并删除一些已关闭的WAL文件 五、具体案例，以及实际的存储目录结合图1的结构以及图2的数据结构，再加上对shard内部各组件的介绍，下面通过一个实际的例子来探索下InfluxDB实际的存储目录。 现在创建出来一个叫style_rank的库，且设置默认RP为7d，保存周期为7天，默认分组策略为24h分一次，如图： 图中名为7d的rp策略被设置为默认策略，其中duration被设置为168h，shardGroupDuration被设置为了24h，意味着该策略里的表数据将以7天为一个周期过期，期间以1天为单位分组。 5.1：InfluxDB配置文件下载下来一个InfluxDB后，通过配置文件influxdb.conf可以配置各个文件存放的目录： 这几个配置决定了你将产生的数据存放在哪里，建议自定义这个，默认的目录很迷=_= 5.2：具体的数据目录配置完上面的文件，启动，然后录入数据，然后再去具体的目录下查找具体的数据文件： 元数据层：用来存放当前库的属性，比如RP、默认RP、分组策略等。 wal层：用来存放刚写入数据的信息，会先写入内存，然后异步写入wal文件，wal文件用来重启InfluxDB时恢复内存数据用，当wal文件达到一定大小时，会压入data层，wal层的结构和data层的几乎一致： 上面进入wal目录后，需要选择正确的库，这里选择style_rank，进入库后，会显示出该库的所有RP，这里选择7d，然后对应图1，你现在来到了ShardGroup层，图6中的编号，就是生成的ShardGroup，随便进入一个，则可以看到最终的wal文件（事实上只有当天建的group内的wal才有数据）。 data层：用来最终存放数据的目录，所有wal文件内的数据达到一定大小后，均会被压缩进data层，现在进入data层，然后进入style_rank： 可以看到下方除了有两个RP策略外，还有_series目录，这个目录就是存放索引（tags字段所产生的索引数据）的地方，用于服务重启时恢复索引数据用。 进入7d，可以看到跟wal层差不多的ShardGroup分层，你现在又来到了图1的ShardGroup层： 然后再次进入一个分组内： 最终的tsm文件就存在该目录下，fields.idx存放的是表里的fields元数据，用于写入数据时做效验用。 六、执行计划然后继续由上面的style_rank库，在7d这个RP下，新建两张表： 12insert season_views,date=\"201907\" season_id=666,views=56789insert season_views_v2,season_id=666 views=56789 目的很简单，只是为了记录下season_id=xxx的数据在某一时刻对应的views值，俩表的区别在于，season_view仅仅以日期为tag（作为索引字段，可以忽略不计），season_views_v2则以season_id作为tag，某一时刻，两张表的数据量均达到了5kw且数据一致，下面，让我们以最简单的方式，查询下这两张表： ℹ️ ps：sql前加explain关键字可以查看其执行计划，加explain analyze可以看到具体每一步执行的耗时。 同样的查询，season_id加了索引和不加索引的区别： NUMBER OF SERIES（扫描系列数，加了索引后根据series的哈希算法，同一个season_id都被分在了同一个Shard里，这里之所以为8，是因为ShardGroup不同）由不加索引的64个，减少到了8个 NUMBER OF FILES（扫描文件数）由不加索引的54个减少到11个 NUMBER OF BLOCKS（扫描的数据块）由不加索引的152058减少到了184 七、总结 在使用时，尽可能按照某具体时间段进行过滤，如果过滤条件筛选出的数据量过大，则会严重影响查询效率。 适度使用函数，若使用，请保证筛选条件筛选出的数据量，如果过大，效率会极低，比如在season_views_v2中启用TOP函数，查出前三名，查询耗时超过10s，加上season_id条件后，则迅速返回。 加索引时需要注意，尽量避免大数据量的属性做tag，否则会产生大量series，生成大量索引数据，占用过多内存，比如用户级别的tag（这里的用户级别是指大型网站用户数量级，一般1亿以上的情况）。 免费版的InfluxDB不支持集群，主从需要借助influx-proxy实现，也可以自己通过监听wal文件写入，通过消息队列的方式实现主从同步，wal类似mysql里的binlog。 建议使用的时候，先把自己的库建好，然后再把RP建好，如果嫌麻烦，可以直接把自己新建的RP定义成默认RP，如果RP设置为默认，只会对查询产生影响（当然写的时候也需要指定），在sql中不指定RP的情况下执行，则认为就是走的默认RP。 分库，建议按照日期分。","link":"/2019/06/21/InfluxDB%EF%BC%88%E4%B8%80%EF%BC%89%E5%88%9D%E6%8E%A2%E6%97%B6%E5%BA%8F%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"title":"链路追踪（二）-分布式链路追踪系统数据采集","text":"本篇文章基于上一篇，只针对数据采集做介绍，会提供一个SDK的实现和使用，会做实现方案的介绍，具体详细介绍下面边框加粗的部分： 一、数据采集接着拿上一篇里的例子来说，把例子里的图贴过来： 简单回顾下上图，一次API调用，完成上面各个业务服务的调用，然后聚合所有服务的信息，然后Redis_02的调用发生瓶颈，继而影响到E、D、C三个服务，现在需要直观的展示这条链路上的瓶颈点，于是需要一个链路系统，展示成如下图的效果： 要想展示成上图中的效果，则必须要进行数据的采集和上报，那么这就牵扯到两个概念，Span和Tracer，抽象成数据库的设计层面，可以理解成Tracer对Span等于一对多的关系，而一个Span可能包含多个子Span，一个Tracer表示一次调用所经过的整个系统链路，里面包含N多Span，每个Span表示一次事件的触发（也就是调用），那么就用图2来解释下这种关系： 所以上报数据最关键的地方就是要做到如下几点： 在调用之处（比如例子中API调用开始的地方），创建Tracer，生成唯一Trace ID； 在需要追踪的地方（比如例子中发生服务调用的地方），创建Span，指定Trace ID，并生成唯一Span ID，然后按需建立父子关系，追踪结束时（比如例子中调用完成时）释放Span（即置为finished，此时计时已完成）； 跨系统追踪时做好协议约定，每次跨系统调用时可以在协议头传输发起调用系统的TraceID，以便链路可以做到跨系统顺利传输。 最终主链路执行完毕（例子中就是指API调用结束）时，推送此链路产生的所有Span到链路系统，链路系统负责落库、数据分析和展示。 以上便是链路追踪业务SDK需要参与做到的事情。 Tracer是个虚拟概念，负责聚合Span使用，实际上报的数据全是Span，下面来看下Span的结构定义（JSON）： 12345678910111213{ \"spanId\": 123456, \"traceId\": 1234, \"parentId\": 123455, \"title\": \"getSomeThing\", \"project\": \"project.tree.group.project_name\", \"startTime\": 1555731560000, \"endTime\": 1555731570000, \"tags\": { \"component\": \"rpc\", \"span.kind\": \"client\" }} 这是一个span的基本结构定义，startTime和endTime可以推算出本次Span耗时（交给链路系统前端时可以用来展示时间轴的长短），title表示的是Span本身的描述，一般是一个method的名字，project是当前所处项目的全称，项目的全称可以交给链路系统前端用来搜索出该项目的所有链路信息。spanId、traceId、parentId结合上面的图理解即可，tags表示的是一些描述信息，这里有一些标准化的东西：标准的Span tag 和 log field 二、数据采集基于Java语言的实现一般基于io.opentracing标准实现上报SDK，下面来逐步实现一个最简单的数据收集器，首先在项目中引入io.opentracing的jar包，然后追加两个基本类SimpleTracer和SimpleSpan，这里只贴出关键代码。 SimpleTracer定义： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475// 追踪器，实现Tracer接口public class SimpleTracer implements Tracer { private final List finishedSpans = new ArrayList&lt;&gt;(); //存放链路中已执行完成的span（finished span） private String project; //项目名称 private Boolean sampled; //是否上报（由采样率算法生成该值） public SimpleTracer(boolean sampled, String project) { this.project = project; this.sampled = sampled; } public SimpleTracer(String uri, String project) { this.project = project; this.sampled = PushUtils.sampled(uri); //本次追踪是否上报 } @Override public SpanBuilder buildSpan(String operationName) { return new SpanBuilder(operationName); //创建span一般交给Tracer去做，这里由其内部类SpanBuilder触发创建 } //上报span，这个方法一般在一次链路完成时调用，负责将finishedSpans里的数据上报给追踪系统 public synchronized void pushSpans() { if (sampled != null &amp;&amp; sampled) { List finished = this.finishedSpans; if (finished.size() &gt; 0) { finished.stream().filter(SimpleSpan::sampled).forEach(span -&gt; PushHandler.getHandler().pushSpan(span)); //实际负责推送的方法 this.reset(); //每发生一次推送，则清理一次已完成span集合 } } } // Tracer对象内部类SpanBuilder，实现了标准里的Tracer.SpanBuilder接口，用来负责创建span public final class SpanBuilder implements Tracer.SpanBuilder { private final String title; //操作名，也就是span的title private long startMicros; //初始化开始时间 private List references = new ArrayList&lt;&gt;(); //父子关系 private Map&lt;String, Object&gt; initialTags = new HashMap&lt;&gt;(); //tag描述信息初始化 //创建span用的title传入 SpanBuilder(String title) { this.title = title; } @Override public SpanBuilder asChildOf(SpanContext parent) { //传入父子关系 return addReference(References.CHILD_OF, parent); } @Override public SpanBuilder addReference(String referenceType, SpanContext referencedContext) { if (referencedContext != null) { //添加父子关系，其实这里就是初始化了Span里的Reference对象，这个对象会在创建Span对象时作为参数传进去，然后具体关系的确立，是在Span对象内（具体Span类的代码段会展示） this.references.add(new SimpleSpan.Reference((SimpleSpan.SimpleSpanContext) referencedContext, referenceType)); } return this; } @Override public SimpleSpan start() { return startManual(); } @Override public SimpleSpan startManual() { //创建并开始一个span if (this.startMicros == 0) { this.startMicros = SimpleSpan.nowMicros(); //就是在这里初始化startTime的 } //这里触发SimpleSpan的构造方法，之前的references会被传入，此外初始化的tag信息、title、开始时间等也会被传入参与初始化 return new SimpleSpan(SimpleTracer.this, title, startMicros, initialTags, references); } }} 代码块1 上面放了SimpleTracer的代码片段，关键信息已标注，这个类的作用就是帮助创建span，上面还有一个比较重要的方法，也就是sampled方法，该方法用来生成这次链路是否上报（也就是采样率，实际的追踪系统不可能每次的请求都上报，对于一些QPS较高的系统，会带来额外大量的存储数据，因此需要一个上报率），下面来简单看下上报率的实现： 12345678910111213141516171819202122232425262728293031public class PushUtils { public static final Random random = new Random(); private static final Map&lt;String, Long&gt; requestMap = Maps.newConcurrentMap(); public static boolean sampled(String uri) { if (Strings.isNullOrEmpty(uri)) { return false; } Long start = requestMap.get(uri); Long end = System.currentTimeMillis(); if (start == null) { requestMap.put(uri, end); return true; } if ((end - start) &gt;= 60000) { //距离上次上报已经超过1min了 requestMap.put(uri, end); return true; } else { // 没超过1min，则按照1/1000的概率上报 if (random.nextInt(999) == 0) { requestMap.put(uri, end); return true; } } return false; }} 代码块2 这种是比较适中的做法，如果1min内没有上报一次，则必定上报，如果1min内连续上报多次，则按照千分之一的概率上报，这样既保证了低QPS的系统可以有相对较多的链路数据，也可以保证高QPS的系统可以有相对较少的链路数据。 下面来看下SimpleSpan的关键代码段： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788// 链路Span，实现标准里的Span接口public class SimpleSpan implements Span { private final SimpleTracer simpleTracer; //链路追踪对象（一次追踪建议生成一个链路对象，尽量不要用单例，会有同步锁影响并发效率） private final long parentId; // 父span该值为0 private final long startTime; // 计时开始开始时间戳 private final Map&lt;String, Object&gt; tags; //一些扩展信息 private final List references; // 关系，外部传入 private final List errors = new ArrayList&lt;&gt;(); private SimpleSpanContext context; // spanContext,内部包含traceId、span自身id private boolean finished; // 当前span是否结束标识 private long endTime; // 计时结束时间戳 private boolean sampled; // 是否为抽样数据，取决于父节点，依次嫡传下来给其子节点 private String project; // 追踪目标的项目名 private String title; //方法名 SimpleSpan(SimpleTracer tracer, String title, long startTime, Map&lt;String, Object&gt; initialTags, List refs) { this.simpleTracer = tracer; // 这里传入的tracer是针对本次跟踪过程唯一对象，负责收集已完成的span this.title = title; this.startTime = startTime; this.project = tracer.getProject(); this.sampled = tracer.isSampled(); //是否上报，该字段根据具体的采样率方法生成 if (initialTags == null) { this.tags = new HashMap&lt;&gt;(); } else { this.tags = new HashMap&lt;&gt;(initialTags); } if (refs == null) { //span对象由tracer对象创建，创建时会把父子关系传入 this.references = Collections.emptyList(); } else { this.references = new ArrayList&lt;&gt;(refs); } SimpleSpanContext parent = findPreferredParentRef(this.references); //查看是否存在父span if (parent == null) { //通常父span为空的情况，都是链路开始的地方，这里会生成traceId // 当前链路还不存在父span，则本次span就置为父span，下面会生成traceId和当前父span的spanId this.context = new SimpleSpanContext(nextId(), nextId(), new HashMap&lt;&gt;()); this.parentId = 0; //父span的parentId是0 } else { // 当前链路已经存在父span了，那么子span的parentId置为当前父span的id，表示当前span是属于这个父span的子span，同时traceId也延用父span的（表示属于同一链路） this.context = new SimpleSpanContext(parent.traceId, nextId(), mergeBaggages(this.references)); this.parentId = parent.spanId; } } @Nullable private static SimpleSpanContext findPreferredParentRef(List references) { if (references.isEmpty()) { return null; } for (Reference reference : references) { if (References.CHILD_OF.equals(reference.getReferenceType())) { //现有的reference中存在父子关系（简单理解，这个关系就是BuildSpan的时候传入的） return reference.getContext(); //返回父span的context信息（包含traceId和它的spanId） } } return references.get(0).getContext(); } @Override public synchronized void finish(long endTime) { finishedCheck(\"当前span处于完成态\"); this.endTime = endTime; this.simpleTracer.appendFinishedSpan(this); //span完成时放进链路对象的finishedSpans集合里 this.finished = true; } // SimpleSpan的内部类SimpleSpanContext，存放当前Span的id、链路id，实现了标准里的SpanContext接口 public static final class SimpleSpanContext implements SpanContext { private final long traceId; //链路id private final Map&lt;String, String&gt; baggage; private final long spanId; //spanId public SimpleSpanContext(long traceId, long spanId, Map&lt;String, String&gt; baggage) { this.baggage = baggage; this.traceId = traceId; this.spanId = spanId; } } public static final class Reference { //用于建立Span间关系的内部类 private final SimpleSpanContext context; //存放了某一个Span的context（用于跟当前span建立关系时使用） private final String referenceType; //关系类型，目前有两种：child_of和follows_from，第一种代表当前span是上面context里span的子span，第二个则表示同级顺序关系 public Reference(SimpleSpanContext context, String referenceType) { this.context = context; this.referenceType = referenceType; } }} 代码块3 上面就是SimpleSpan的关键实现，关键点已标注，下面来看下数据上报这里的实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class PushHandler { private static final PushHandler handler = new PushHandler(); private BlockingQueue queue; private PushHandler() { this.queue = new LinkedBlockingQueue&lt;&gt;(); //数据管道 new Thread(this::pushTask).start(); } public static PushHandler getHandler() { return handler; } public void pushSpan(SimpleSpan span) { queue.offer(span); } private void pushTask() { if (queue != null) { SimpleSpan span; while (true) { try { span = queue.take(); //为了测试，这里只打印了基本信息，实际环境中这里需要做数据推送（kafka、UnixSocket等） StringBuilder sb = new StringBuilder() .append(\"tracerId=\") .append(span.context().traceId()) .append(\", parentId=\") .append(span.parentId()) .append(\", spanId=\") .append(span.context().spanId()) .append(\", title=\") .append(span.title()) .append(\", 耗时=\") .append((span.endTime() / 1000000) - (span.startTime() / 1000000)) .append(\"ms, tags=\") .append(span.tags().toString()); System.out.println(sb.toString()); } catch (InterruptedException e) { e.printStackTrace(); } } } }} 代码块4 只是做了简单的测试，所以处理逻辑只是简单的做了打印，实际当中这里要上报链路数据（spans）。这里使用了一个阻塞队列做数据接收的缓冲区。 这套实现是非常简单的，只进行简单的计时、推送，并没有涉及active方式的用法，一切创建、建立父子关系均交由开发人员自己把控，清晰度也更高些。 代码完整地址：simple-trace 三、simple-trace的使用看了上面的实现，这里利用simple-trace来进行程序追踪，看一个简单的例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384public class SimpleTest { private SimpleTracer tracer = null; private SimpleSpan parent = null; //假设这里是链路开始的地方 @Test public void test1() { //创建链路 tracer = new SimpleTracer(\"test1\", \"projectName\"); parent = tracer.buildSpan(\"test1\") .withTag(SpanTags.COMPONENT, \"http\") .withTag(SpanTags.SPAN_KIND, \"server\") .start(); //span开始 //-------------------------------------------------- String result1 = getResult1(); //假设getResult1需要链路追踪 System.out.println(\"r1 = \" + result1); String result2 = getResult2(); //假设getResult2需要链路追踪 System.out.println(\"r2 = \" + result2); //-------------------------------------------------- //下面标记着一次链路追踪的结束 parent.finish(); //主span结束 tracer.pushSpans(); //触发span数据推送 } public String getResult1() { //前戏，建立getResult1自己的追踪span SimpleSpan currentSpan = null; if (tracer != null &amp;&amp; parent != null) { //当前链路视为test1方法的子链路，建立父子关系 SimpleSpan.SimpleSpanContext context = new SimpleSpan.SimpleSpanContext(parent.context().traceId(), parent.context().spanId(), new HashMap&lt;&gt;()); //建立父子关系，traceId和父spanId被指定 currentSpan = tracer.buildSpan(\"getResult1\") .addReference(References.CHILD_OF, context) .withTag(SpanTags.COMPONENT, \"redis\") .withTag(SpanTags.SPAN_KIND, \"client\").start(); //启动自己的追踪span } try { Thread.sleep(1000L); return \"result1\"; } catch (InterruptedException e) { e.printStackTrace(); return \"\"; } finally { if (currentSpan != null) { currentSpan.finish(); //最后完成本次链路追踪 } } } public String getResult2() { //前戏，建立getResult2自己的追踪span SimpleSpan currentSpan = null; if (tracer != null &amp;&amp; parent != null) { //当前链路视为test2方法的子链路，建立父子关系 SimpleSpan.SimpleSpanContext context = new SimpleSpan.SimpleSpanContext(parent.context().traceId(), parent.context().spanId(), new HashMap&lt;&gt;()); //建立父子关系，traceId和父spanId被指定 currentSpan = tracer.buildSpan(\"getResult2\") .addReference(References.CHILD_OF, context) .withTag(SpanTags.COMPONENT, \"redis\") .withTag(SpanTags.SPAN_KIND, \"client\").start(); //启动自己的追踪span } try { Thread.sleep(2000L); return \"result2\"; } catch (InterruptedException e) { e.printStackTrace(); return \"\"; } finally { if (currentSpan != null) { currentSpan.finish(); //最后完成本次链路追踪 } } }} 代码块5 运行结果： 12345r1 = result1r2 = result2tracerId=1507767477962777317, parentId=2107142446015091038, spanId=5095502823334701185, title=getResult1, 耗时=1555839336570 - 1555839335569 = 1001ms, tags={span.kind=client, component=redis}tracerId=1507767477962777317, parentId=2107142446015091038, spanId=9071431876337611242, title=getResult2, 耗时=1555839338572 - 1555839336571 = 2001ms, tags={span.kind=client, component=redis}tracerId=1507767477962777317, parentId=0, spanId=2107142446015091038, title=test1, 耗时=1555839338572 - 1555839334687 = 3885ms, tags={span.kind=server, component=http} 通过该实例，关于simple-trace的基本用法已经展示出来了（创建tracer、span、建立关系、tags、finish等），看下打印结果（打印结果就是simple-trace推送数据时直接打印的，耗时是根据startTime和endTime推算出来的），父子关系建立完成，假如说这些数据已经落库完成，那么通过链路系统的API解析和前端渲染，会变成下面这样（绘图和上面测试结果不是同一次，所以图里耗时跟上面打印的耗时不一致😭）： 本篇不讨论图如何生成，可以说下后端可以给前端提供的接口结构以及组装方式：首先可以根据traceId查出来所有相关span，然后根据parentId进行封装层级，比如图4的API结构大致上如下： 123456789101112131415161718192021222324252627282930313233343536373839404142{ \"spanId\": 2107142446015091038, \"traceId\": 1507767477962777317, \"parentId\": 0, \"title\": \"test1\", \"project\": \"projectName\", \"startTime\": 1555839334687, \"endTime\": 1555839338572, \"tags\": { \"span.kind\": \"server\", \"component\": \"http\" }, \"children\": [{ \"spanId\": 5095502823334701185, \"traceId\": 1507767477962777317, \"parentId\": 2107142446015091038, \"title\": \"getResult1\", \"project\": \"projectName\", \"startTime\": 1555839335569, \"endTime\": 1555839336570, \"tags\": { \"span.kind\": \"client\", \"component\": \"redis\" }, \"children\": [] }, { \"spanId\": 9071431876337611242, \"traceId\": 1507767477962777317, \"parentId\": 2107142446015091038, \"title\": \"getResult2\", \"project\": \"projectName\", \"startTime\": 1555839336571, \"endTime\": 1555839338572, \"tags\": { \"span.kind\": \"client\", \"component\": \"redis\" }, \"children\": [] } ]} 包装成上面的结构，前端根据层级关系、startTime、endTime进行调用树和时间轴的渲染即可，在实际生产中，这个层级树可能更加庞大，比如图2。 基本使用很简单，那么基于简单的例子再进行一层抽象，如果在生实际项目中，就不能单单像上面那样使用了，需要封装、解耦，那么实际项目中一般会通过怎样的方式来使用呢？跨系统的时候如何建立层级关系呢？下面针对图2中的例子，进行简单的方案设计（图2过于复杂，这里只说服务A的调用链路，其余按照服务A类推即可），下面将会采用伪代码的方式进行说明问题的解决方案，实际当中需要自己按照实现思路自行封装。 现在引入两个概念，拦截器和Context（上下文），它们属于正常业务中常用的概念，Context是指一次调用产生的上下文信息，上下文信息可以在单次程序调用中的任意位置取到，一般上下文都是利用ThreadLocal（简称TL）实现的，线程本地变量，单纯理解就是只要本次调用的信息都处于同一个线程，那么任意地方都可以通过TL对象拿到上下文对象信息，但是由于系统的复杂度越来越高，一些地方会采用线程池来进行优化业务代码，比如一次调用可能会利用CompletableFuture来进行异步任务调度来优化当前代码执行效率，这个时候单纯使用TL就办不成事儿了，而使用InheritableThreadLocal（简称ITL）又解决不了线程池传递问题，于是就有了阿里推出的TransmittableThreadLocal（简称TTL），这个可以完美解决跨线程传递上下文信息（不管是new Thread还是线程池，都可以准确传递），当然，你也可以仿照TTL的实现，简单代理线程池对象，仍然使用TL实现跨线程传递，也是可以的，TL系列文章传送门：ThreadLocal、InheritableThreadLocal、TransmittableThreadLocal 下面是关于系统上下文的简单定义： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849//自定上下文类public class Context { private SimpleTracer simpleTracer; //当前链路对象 private SimpleSpan parent; //当前链路全局父span //也可以放很多别的上下文内容，这里省略... public SimpleTracer getSimpleTracer() { return simpleTracer; } public void setSimpleTracer(SimpleTracer simpleTracer) { this.simpleTracer = simpleTracer; } public SimpleSpan getParent() { return parent; } public void setParent(SimpleSpan parent) { this.parent = parent; }}public class ContextHolder { //这里仅用TL简单实现，如果项目里使用了线程池，那么这里的实现要变成TTL，并让TTL代理全局的线程池对象，也可以不用TTL，自己代理线程池对象，这里不再详述 private static ThreadLocal contextThreadLocal = new ThreadLocal&lt;&gt;(); private ContextHolder() { } public static void removeContext() { contextThreadLocal.remove(); } public static Context getContext() { return contextThreadLocal.get(); } public static void setContext(Context context) { if (context == null) { removeContext(); } contextThreadLocal.set(context); }} 代码块6 我们把链路对象和链路第一次产生的父span放到上下文，意味着我们可以在这次调用的任意位置通过ContextHolder获取到当前链路对象（伪代码会出现该类），下面来结合图2的A服务链路，结合aop思想，写一次从图2API调用开始到Redis01调用结束的代码。 按照流程，API属于一次Http调用，也是链路入口，那么利用这一点，和Http服务的拦截器功能（大部分系统都会用到一个http调用的拦截器，一般上下文也是这里产生的），伪代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class ApiInterceptor { //开始Http处理请求之前要做的，一般这里产生上下文，并交给TL传递上下文对象，这里也是链路初始化的地方 public void beforeHandle(Request request) { Context context = new Context(); //上下文对象 SimpleTracer tracer = null; SimpleSpan parent = null; //这里是为跨系统调用做的协议头传递，因为我们这个API也可能是公司内别的业务方内部调用，那么这个时候就需要约定协议头，一旦协议头中带有约定好的链路字段，那么就认为我们这个API本次调用相对于别的系统是个子链路 String traceId = request.headers.get(\"x1-trace-id\"); //拿到协议头的父链路id，子链路继承之 String parentId = request.headers.get(\"x1-span-id\"); //拿到协议头的父span信息 String sampled = request.headers.get.get(\"x1-sampled\"); //是否上报 if (traceId != null &amp;&amp; parentId != null &amp;&amp; sampled == true) { tracer = new SimpleTracer(request.getUri, \"所属项目名\"); //这里用url当成是初始化span的title // 符合这种情况的，我们这里的parent其实只是一个相对于别的系统的child SimpleSpan.SimpleSpanContext simpleSpanContext = new SimpleSpan.SimpleSpanContext(traceId, parentId, new HashMap&lt;&gt;()); parent = tracer.buildSpan(request.getUri) .addReference(References.CHILD_OF, simpleSpanContext) //建立父子关系，如果是别的业务方调用我们这个http服务，那么这里这一步，也就建立了跟调用方的父子关系，traceId等是继承的调用方的，意味着本次调用也属于调用方的一环，这也就实现了跨系统的链路追踪 .withTag(SpanTags.COMPONENT, \"http\") .withTag(SpanTags.SPAN_KIND, \"server\").start(); //启动span } else { //执行else，说明该http调用是一次自己完整的调用，不属于任何父链路，那么就无需建立关系，直接初始化tracer即可 tracer = new SimpleTracer(request.getUri, \"所属项目名\"); parent = tracer.buildSpan(request.getUri) .withTag(SpanTags.COMPONENT, \"http\") .withTag(SpanTags.SPAN_KIND, \"server\") .start(); //启动span } //将封装好的tracer和parentSpan设置到上下文对象里去 context.setSimpleTracer(tracer); context.setParent(parent); ContextHolder.setContext(context); //将本次请求生成的上下文对象放进ContextHolder（也就是TL里），方便在任意位置取出使用 } //业务逻辑处理中 public void hadle() { //本次API请求实际走的业务逻辑，也就是A服务调用、B服务调用等这些实际的业务逻辑处理 doing(); } //Http业务处理完成后的触发 public void afterHandler() { //Http调用结束的时候，取出当前链路信息，完成数据的上报 SimpleTracer tracer = ContextHolder.getContext().getTracer(); SimpleSpan parent = ContextHolder.getContext().getParent(); if (tracer != null &amp;&amp; parent != null) { parent.finish(); //结束掉parent Span tracer.pushSpans(); //上报这次产生的链路数据（spans） } }} 代码块7 通过这个外部的API链路包装，可以知道的事情是上下文在这里面充当的角色，API调用是一个系统的入口，这种入口有很多，一次系统调用都会有一个类似的入口，比如RPC调用，跨系统后的rpcServer端也是一个入口，这种入口级的拦截器，before里面做的通常都是建立Tracer，但是代码里不是简单的创建一个Tracer对象就完事儿了，还有协议头的分析，链路系统如何实现跨系统的传输呢？这就牵扯到协议约定，比如Http请求，可以在协议头里约定几个特殊字符串来存放来源系统的tracerId等，结合上面的例子，假如我们这个API是公司内别的系统API01发起的http调用，API01本身也会有链路追踪，API01系统内发起对我们API的http请求，这就属于跨系统调用，我们这次API调用相对于API01是一个子链路，需要建立父子关系，结合上面的例子简单画下这次调用图： 包括API的其他跨系统的调用，比如A服务的调用，也是使用同样的原理进行链路跨系统传输的（很多RPC框架上层协议也是支持扩展协议头（即协议的元数据信息）的，比如grpc的上层协议就是http2，同样有header），那么接下来看下图中（截自图2）标红模块对应的伪代码吧： 这块是指当前系统通过rpc client发起对A服务的调用，从发起调用到A服务响应，这个过程仍然属于API这次调用的子span（没有出系统），但是到了A服务的触发，就牵扯到跨系统，A服务的链路相对于rpc client（图6标红的操作）的span，是一个子span，通过上面对跨系统的处理，这里rpc client里一定会把自身的spanId作为A服务的parentId传过去，包括traceId等，来看下伪代码： 1234567891011121314151617181920212223242526272829public class RpcClient { //等待服务端响应方法 public void requestRpc(RpcRequest request) { //调用前执行 SimpleSpan span = null; SimpleSpan parent = ContextHolder.getContext().getParent(); SimpleTracer tracer = ContextHolder.getContext().getTracer(); if (tracer != null &amp;&amp; parent != null) {//↓这个title就设置成rpc调用的那个方法名即可 span = tracer.buildSpan(request.getRpcMethod).asChildOf(parent) //建立父子关系，因为rpc client调用属于API调用的子链路 .withTag(SpanTags.COMPONENT, \"grpc\") .withTag(SpanTags.PEER_SERVICE, request.getRpcMethod) .withTag(SpanTags.SPAN_KIND, \"client\") .start(); //启动这个span //设置协议头，因为被调用的RPC服务相对于我们来说是个子链路 request.setHeader(\"x1-rpc-span-id\", span.context().spanId()); request.setHeader(\"x1-rpc-trace-id\", span.context().traceId()); request.setHeader(\"x1-rpc-sampled\", span.sampled()); } rpcServerRequest(request); //实际调用rpc服务 //调用后执行 if(span != null){ span.finish(); //完成本次追踪 } }} 代码块8 这样就完成了图6中红线部分的span，然后来看下被调用的服务A内部是怎么处理的（其实很像上面http入口的处理方式）： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class RpcServerInterceptor { //服务的入口，Rpc服务处理请求之前要做的，一般这里产生上下文，并交给TL传递上下文对象，这里也是链路初始化的地方 public void beforeHandle(RpcRequest request) { Context context = new Context(); //上下文对象 SimpleTracer tracer = null; SimpleSpan parent = null; //解析协议头 String traceId = request.headers.get(\"x1-rpc-trace-id\"); //拿到协议头的父链路id，子链路继承之 String parentId = request.headers.get(\"x1-rpc-span-id\"); //拿到协议头的父span信息 String sampled = request.headers.get.get(\"x1-rpc-sampled\"); //是否上报 if (traceId != null &amp;&amp; parentId != null &amp;&amp; sampled == true) { tracer = new SimpleTracer(request.getMethod, \"所属项目名\"); // 符合这种情况的，我们这里的parent其实只是一个相对于别的系统的child SimpleSpan.SimpleSpanContext simpleSpanContext = new SimpleSpan.SimpleSpanContext(traceId, parentId, new HashMap&lt;&gt;()); parent = tracer.buildSpan(request.getMethod) .addReference(References.CHILD_OF, simpleSpanContext) //建立父子关系，如果是别的业务方调用我们这个服务，那么这里这一步，也就建立了跟调用方的父子关系，traceId等是继承的调用方的，意味着本次调用也属于调用方的一环，这也就实现了跨系统的链路追踪 .withTag(SpanTags.COMPONENT, \"rpc\") .withTag(SpanTags.SPAN_KIND, \"server\").start(); //启动span } else { //执行else，说明该rpc调用是一次自己完整的调用，不属于任何父链路，那么就无需建立关系，直接初始化tracer即可 tracer = new SimpleTracer(request.getMethod, \"所属项目名\"); parent = tracer.buildSpan(request.getMethod) .withTag(SpanTags.COMPONENT, \"rpc\") .withTag(SpanTags.SPAN_KIND, \"server\") .start(); //启动span } //将封装好的tracer和parentSpan设置到上下文对象里去 context.setSimpleTracer(tracer); context.setParent(parent); ContextHolder.setContext(context); //将本次请求生成的上下文对象放进ContextHolder（也就是TL里），方便在任意位置取出使用 } //业务逻辑处理中 public void rpcServerHadle() { doing(); } //Rpc业务处理完成后的触发 public void afterHandler() { //Rpc Server调用结束的时候，取出当前链路信息，完成数据的上报 SimpleTracer tracer = ContextHolder.getContext().getTracer(); SimpleSpan parent = ContextHolder.getContext().getParent(); if (tracer != null &amp;&amp; parent != null) { parent.finish(); //结束掉parent Span tracer.pushSpans(); //上报这次产生的链路数据（spans） } }} 代码块9 可以看到，client发起调用时传递的协议字段，在服务端这里被解析了，建立好父子关系后，A服务再去处理自己的逻辑和链路。 没有牵扯到跨系统的链路追踪，如对redis、memcached、mysql等DB的调用，可以简单在调用元方法上搞个aop代理，然后通过通过上下文对象里的Tracer和parent建立父子关系，结束时finish即可，而pushSpans这个动作通常发生在一次系统调用执行完毕的时候发生，比如API的调用结束时、A服务调用结束时，都是pushSpans的触发点。 到这里基本上关于链路追踪的介绍算结束了，因为系统级的实现方式想要完整的展现在一篇文章里不太现实，所以在使用simple-trace sdk的时候使用了伪代码，便于说明问题，文章没有针对整个链路系统作说明，主要是针对数据采集、数据跨系统追踪做了描述，因为数据采集这一环算是比较重要的一环，也是跟业务开发人员息息相关的一环，如果想要完整搞一个链路追踪系统，可以参考之前的架构搭建一套，以完成采集、上报、落库、解析、展示整个流程。","link":"/2019/04/15/%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA%EF%BC%88%E4%BA%8C%EF%BC%89-%E5%88%86%E5%B8%83%E5%BC%8F%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA%E7%B3%BB%E7%BB%9F%E6%95%B0%E6%8D%AE%E9%87%87%E9%9B%86/"},{"title":"链路追踪（一）-分布式链路追踪系统的介绍","text":"一、分布式链路追踪可以做什么？1.1：简单集群架构&amp;微服务架构先来看下最简单的网站集群架构图： 在这个图里，存在从1~n个服务器，通过负载均衡器SLB进行请求分发，在每个服务器里，都做同一件事情。 现在来看下这个系统的具体业务逻辑（就是图1中每台服务器执行的逻辑，这里是假设其中一个业务接口的处理，真实系统中可能存在n多业务接口）： 图2是对系统中某一个接口（API）的逻辑做了描述，假设处理流程就是请求一次Redis服务，然后做下处理，然后再请求下Memecached服务，在做下业务处理，后续可能还有其他各种业务逻辑，我们统称为逻辑块。 现在假设这个接口存在性能问题，那么找对应开发负责人排查是比较容易的，因为服务器只执行一套逻辑，瓶颈点一定发生在当前接口对应代码里的某个点，那么就找接口对应的负责人去排查优化即可。 但是当业务发展到一定的程度，比如上述单系统逻辑越来越复杂（业务接口越来越多，逻辑越来越复杂），就会造成很多我们不愿意看到的问题发生： 每一次微小的改动都需要整体走一次打包、发版的流程，对测试也是种负担，因为n多个人如果同时开发不同的功能，这时候就会对测试和发布流程造成很大的困扰。 如果因为做某次活动，某一个接口可能引入大量请求，需要紧急扩容，那么只能对整体扩容（因为该接口与其他接口都处于同一个系统中）。 系统各模块高度耦合，不适合多人开发和维护。 简单集群带来的问题会随着系统复杂度的提升，维护成本变得越来越大，基于此，便有了微服务架构（微服务是一种架构思想，简单来说就是将复杂庞大耦合度高的系统按照功能特性拆分成一个个独立的系统，通过网络互相通信，这种架构可以借助RPC框架（比如grpc、dubbo）实现拆分。当然，熟悉的HTTP框架也可以做到（比如okhttp），但是受限于HTTP协议，性能可能并没有普通RPC框架高，比如grpc采用HTTP2应用层协议进行数据通信，这个协议相比HTTP1来说，支持数据流的标记，可以在一个长连接上做N多请求和接收的并发处理，属于全双工网络通信，这点放到HTTP1就很难做到，此外，它还采用了轻量级且跨语言的protobuf来编解码信息，在性能上尽可能做到极致）。 结合图2，我们来简单按照业务划分一下服务，可以将A代码块里的逻辑抽象成A服务，将B代码块里的逻辑抽象成B服务，当然还有可能有其他n多细化的服务，网关层API（负责聚合信息以及业务处理的模块，对应上面简单集群里的具体接口），服务注册与发现、SLB等。 下面再来看一下被拆分后的架构图： 这张图是一个很简单的微服务化的架构图，图中虚线部分都是在各服务启动时或者运行期发生的调用，负责注册与发现（如zookeeper、Eurake等都可以作服务注册与发现，这里不再细说，只关注实线部分即可）。 这种架构很好的解决了普通集群架构带来的问题（参考上述1、2、3），微服务架构的好处： 降低了系统（逻辑块）间的耦合度，可以独立开发、独立部署和测试，系统间的边界明确，可以细分相关负责人，开发效率大大提升。 如果因为做某次活动，某一个接口可能引入大量请求，需要紧急扩容，那么只需要将该接口涉及到的服务进行扩容即可，而不是像之前那样整体扩容，降低了维护成本（某种意义上的降低，维护人员要足够多，每个人去负责自己的小模块，如果一个公司只有一个维护人员，微服务反而是在加重维护人员的工作:）。 提高了系统（逻辑块）的复用性，比如上面的服务A做自己的事情，万一以后有个API仍然需要A逻辑块，那么该API只需要再次调用A服务即可（实际应用当中的例子：用户服务）。 服务化以后，每个服务甚至可以用不同的语言来实现（存在支持跨语言的RPC框架，比如grpc），一个公司大了以后，可能存在语言差异，有的组使用JAVA，有的组用Go，通过服务化的方式，来将两个不同语言的系统互联。 上面简单介绍了普通集群架构和微服务架构，同样的，微服务化也意味着系统调用的复杂化，有可能一次API的调用对应大批量的服务调用，服务方自己又有一堆服务调用，那么针对这种问题，我们来模拟一次复杂的API调用（注册与发现服务已隐藏），如图4所示: 这是模拟了一次微服务架构中比较复杂的系统调用。 ⚠️注意：图画的有点歪，微服务架构的设计目标是要高度解耦，每个独立的服务最好都有一份自己独立的资源访问，比如服务A只访问A业务相关的数据库和缓存等资源，图中针对这些资源划分做的很糙 那么现在如果这个较复杂的链路调用上的其中一环发生了性能瓶颈，拖慢了整个API的调用，比如图中的慢标识，现在我们再来模拟一下这个性能问题的排查过程（过程相当鬼畜）： 负责API编写的同学发现API的响应时间总是达不到预期，自己debug发现导致性能问题的原因是服务C，于是找到了服务C的服务负责人，假设就叫他C服务负责人，C服务负责人紧接着排查，发现原来是服务D的调用过慢，于是又跑去找D服务负责人，D服务负责人收到C服务负责人的反馈，然后去查自己的服务，发现自己调用的服务E响应缓慢，于是D服务负责人又跑去找E服务负责人，E服务负责人紧接着排查，发现原来是自己这里调用的Redis_02服务有问题，然后自己排查，如果不是自己调用方式有问题，接下来还可能去联系对应的Redis_02相关维护人员帮助检查瓶颈点。 对比简单集群方式中的单系统性能问题排查，微服务针对此类问题的排查简直是一场噩梦，这其中涉及到的人跟瓶颈节点的深度成正比，因为任何一个环节都有可能存在性能问题，而拖慢整个进度的根源未知，那么有没有一种工具可以完成跨服务跨系统的去跟踪这次的调用链路呢？ 1.2：分布式链路追踪结合上面的问题，分布式链路追踪系统就诞生了，来看下Google的这篇文章：Dapper，大规模分布式系统的跟踪系统，可以对分布式链路追踪系统有个系统的认识。 单纯的理解链路追踪，就是指一次任务的开始到结束，期间调用的所有系统及耗时（时间跨度）都可以完整记录下来，比如上面图4的例子，假设总耗时100ms，存在瓶颈链路C--&gt;D--&gt;E--&gt;Redis02，如果链路追踪系统做好了，链路数据有了，借助前端解析和渲染工具，可以达到下图中的效果： 可以看到从API的调用开始到每个涉及到的系统调用以及系统内部的调用链路和时间跨度被直观的展示出来了，通过上图，可以看到时间跨度最长的就是Redis_02，该服务的调用间接拖慢了E服务、D服务、C服务的响应，最后由C服务直接导致API整体响应缓慢，通过这个图，就可以直接找到对应的责任人去排查对应的问题，而不是像之前那样找一群人。 二、分布式链路追踪系统的组成类似很多监控系统，该系统也分为基础数据采集+数据存储+前端展示几个部分，来看下一个分布式链路系统的基本结构： 上图比较粗略的展示了一个完整的链路追踪系统的结构，本篇文章不会介绍具体的链路追踪系统的实现，可以先简单将该系统理解为接收+存储链路数据的作用，前端也一样，可以先简单理解为请求链路系统API，API内部负责读取db，并将数据封装成前端需要的格式，前端负责绘制图5中的页面即可（只要数据结构约定好，对于专业的前端工程师做出图5的效果是很容易的，当然网上也有现成的前端工具）。 本篇文章主要介绍链路追踪究竟是什么，可以解决什么问题，下一篇将会详细介绍“链路数据采集SDK”，因为这一部分是跟业务组件开发人员直接挂钩的，下一篇会说明链路追踪的数据结构、如何做到链路数据的采集和上报、如何做到跨服务的链路追踪。 开始前可以先了解一个标准：OpenTracing语义标准 这里面讲了两个很重要的概念：Tracer和Span，一个Tracer认为是一次完整的链路，内部包含n多个Span，Span表示具体的一次调用，图5中就是一次完整的调用链路，里面每个耗时条都是一个Span，Tracer和Span存在一对多的关系（看到这里，图6中的链路追踪API的实现可以认为是根据Tracer的id聚合一批存在父子关系的Span封装成定义好的数据结构传给前端进行渲染的），根据图5，可以知道Span与Span之间又存在父子关系。 具体的实现方案和实现方法，下一篇会通过一个针对简单实现了OpenTracing协议的例子来介绍，下一篇会围绕着图5进行展开。","link":"/2019/04/11/%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA%EF%BC%88%E4%B8%80%EF%BC%89-%E5%88%86%E5%B8%83%E5%BC%8F%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%BB%8B%E7%BB%8D/"},{"title":"Java NIO学习与记录（八）： Reactor两种多线程模型的实现","text":"注：本篇文章例子基于上一篇进行：Java NIO学习与记录（七）： Reactor单线程模型的实现 前言：单线程Reactor模型的缺点紧接着上篇Reactor单线程模型的例子来，假设Handler的read那里的处理方式延迟5s，当做是业务性能瓶颈，改变下原来的Handler，让其read方法在处理时延迟5s： 1234567891011121314151617181920212223private void read() throws IOException { if (selectionKey.isValid()) { System.out.println(\"服务端读取数据前\"); readBuffer.clear(); int count = socketChannel.read(readBuffer); if (count &gt; 0) { try { Thread.sleep(5000L); //读取信息后睡眠5s当做业务处理瓶颈 } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(String.format(\"收到来自 %s 的消息: %s\", socketChannel.getRemoteAddress(), new String(readBuffer.array()))); status = SEND; selectionKey.interestOps(SelectionKey.OP_WRITE); } else { selectionKey.cancel(); socketChannel.close(); System.out.println(\"read时-------连接关闭\"); } } } 代码块1 现在同样开启两个客户端同时连接到该服务端，然后请求--&gt;收到响应--&gt;再次请求的流程走10次，会发现，客户端每收到一次响应需要10s，同样的如果开启3个客户端，则需要15s，因为单线程的Reactor模型是串行的，业务处理的瓶颈可以影响到全局的事件分发，这种模型下，如果存在类似例子中的瓶颈点是致命的（例子的5s是夸张处理），因为新进来的连接也会排队，整个select都会被Handler的处理给阻塞掉，举个实际点的例子，redis在使用时大部分时候会避免使用类似keys这种重操作，为什么呢？就是因为redis是单线程，这里说的单线程其实并不是说redis服务端就一个线程，而是说redis采用的NIO Reactor模型就是单线程的Reactor模型，跟上面代码里做的改动一样，5s可以理解成重操作，影响整个模型的正常运作，redis之所以采用单线程模式，是因为redis大部分操作实在是太快了，快到使用这种模式也可以提供近十万/秒的并发能力，单线程模型实现起来简单且可控性强，所以redis很自然的选择了这种模式。回到问题本身，我们自己的业务可能并没有redis那样高的处理能力，搞不好几个网络请求就可以造成性能瓶颈，拖慢甚至拖垮整个处理模型，所以大部分RPC框架和web容器并不会采用单线程的Reactor模型实现，那么有没有什么方法可以优化这种模型呢？比如，把这个瓶颈点利用独立线程异步出去处理，这样可以保证不影响select的执行，也就很好的避免了上面的问题了，下面介绍两种多线程异步的Reactor模型。 一、单Reactor多线程模型模型图： 上图与单线程Reactor模型对比可以看出，读入数据后，对数据的业务处理部分被线程池做了异步处理，也就是说，上述5s的那段瓶颈被放到了子线程去处理，select的执行不会受到任何影响，因此对新的连接处理、多个客户端的响应速度都应该可以得到保障。 现在来改写下前篇文章里的单线程处理模式的Handler，更名为AsyncHandler： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118public class AsyncHandler implements Runnable { private final Selector selector; private final SelectionKey selectionKey; private final SocketChannel socketChannel; private ByteBuffer readBuffer = ByteBuffer.allocate(1024); private ByteBuffer sendBuffer = ByteBuffer.allocate(2048); private final static int READ = 0; //读取就绪 private final static int SEND = 1; //响应就绪 private final static int PROCESSING = 2; //处理中 private int status = READ; //所有连接完成后都是从一个读取动作开始的 //开启线程数为5的异步处理线程池 private static final ExecutorService workers = Executors.newFixedThreadPool(5); AsyncHandler(SocketChannel socketChannel, Selector selector) throws IOException { this.socketChannel = socketChannel; this.socketChannel.configureBlocking(false); selectionKey = socketChannel.register(selector, 0); selectionKey.attach(this); selectionKey.interestOps(SelectionKey.OP_READ); this.selector = selector; this.selector.wakeup(); } @Override public void run() { //如果一个任务正在异步处理，那么这个run是直接不触发任何处理的，read和send只负责简单的数据读取和响应，业务处理完全不阻塞这里的处理 switch (status) { case READ: read(); break; case SEND: send(); break; default: } } private void read() { if (selectionKey.isValid()) { try { readBuffer.clear(); int count = socketChannel.read(readBuffer); if (count &gt; 0) { status = PROCESSING; //置为处理中，处理完成后该状态为响应，表示读入处理完成，接下来可以响应客户端了 workers.execute(this::readWorker); //异步处理 } else { selectionKey.cancel(); socketChannel.close(); System.out.println(\"read时-------连接关闭\"); } } catch (IOException e) { System.err.println(\"处理read业务时发生异常！异常信息：\" + e.getMessage()); selectionKey.cancel(); try { socketChannel.close(); } catch (IOException e1) { System.err.println(\"处理read业务关闭通道时发生异常！异常信息：\" + e.getMessage()); } } } } void send() { if (selectionKey.isValid()) { status = PROCESSING; //置为执行中 workers.execute(this::sendWorker); //异步处理 selectionKey.interestOps(SelectionKey.OP_READ); //重新设置为读 } } //读入信息后的业务处理 private void readWorker() { try { Thread.sleep(5000L); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(String.format(\"收到来自客户端的消息: %s\", new String(readBuffer.array()))); status = SEND; selectionKey.interestOps(SelectionKey.OP_WRITE); //注册写事件 this.selector.wakeup(); //唤醒阻塞在select的线程，因为该interestOps写事件是放到子线程的，select在该channel还是对read事件感兴趣时又被调用，因此如果不主动唤醒，select可能并不会立刻select该读就绪事件（在该例中，可能永远不会被select到） } private void sendWorker() { try { sendBuffer.clear(); sendBuffer.put(String.format(\"我收到来自%s的信息辣：%s, 200ok;\", socketChannel.getRemoteAddress(), new String(readBuffer.array())).getBytes()); sendBuffer.flip(); int count = socketChannel.write(sendBuffer); if (count &lt; 0) { selectionKey.cancel(); socketChannel.close(); System.out.println(\"send时-------连接关闭\"); } else { //再次切换到读 status = READ; } } catch (IOException e) { System.err.println(\"异步处理send业务时发生异常！异常信息：\" + e.getMessage()); selectionKey.cancel(); try { socketChannel.close(); } catch (IOException e1) { System.err.println(\"异步处理send业务关闭通道时发生异常！异常信息：\" + e.getMessage()); } } }} 代码块2 可以看到，read里、send里的逻辑处理被异步出去执行，新增了中间状态“执行中”，主要用来防止事件重复触发，重复执行异步逻辑，当异步逻辑处理完毕才会更改状态值，这时候可以继续处理接下来的事件（读或写）。 把Accptor类里的实现换成AsyncHandler，运行服务端和客户端会发现，两个客户端的响应均为5s，也不会阻塞新增的连接，新增至三个或者更多的客户端基本可以保持客户端响应均为5s（说明：这里5s是夸张比喻，正常瓶颈没这么夸张，若开了n多客户端，每个都阻塞5s，那么线程池也会发生排队，因为子线程个数有限，处理不过来，最后还是阻塞，一定会远超过5s）。 通过多线程Reactor模型，降低了业务代码瓶颈导致影响整个Reactor执行链路的风险，但是即便如此，read、send操作仍然和接收请求（accept）处于同一个线程，这就意味着read、send的处理可能会影响到对客户端连接的接收能力，那么有没有一种办法，可以把读写流程彻底异步出去，负责连接的线程就只负责接收连接？于是多Reactor多线程模型就产生了，这种模型也叫主从Reactor模型，该模型下可以分为一个主Reactor专门处理连接事件，而多个从Reactor负责读写、业务处理等，这样服务端可以接收并处理更多的请求，提升服务端的吞吐能力（该模型或者说所有基于NIO的Reactor模型，都是以提升服务端处理能力为基础的，NIO在某些情况下不一定会比BIO处理速度快，但一定比BIO稳，就像NIO可以利用很少的线程处理大量的客户端请求，而BIO在大量客户端请求过来的情况下，由于各种操作均会阻塞线程，会处理不过来）。 二、主从Reactor模型还是把之前文章的图拿来展示下这种模型的流程，可以与上面图1进行对比，看看发生了哪些变化： 上图就是主从Reactor模型的一个流程，看下与图1的不同之处，多了SubReactor这样一个角色，这个角色就是用来处理读写操作的Reactor，现在仍然基于之前的例子，进行改写，明确需要改写的点： 新增SubReactor Acceptor那里进行初始化一批SubReactor，进行分发处理 为了区分客户端分别是被哪个SubReactor处理的读写操作，还需要改写下AsyncHandler，在里面加上SubReactor的序号，打印信息时进行区分。 ok，总结完改动点，现在基于上面的代码（代码初代目版本：Reactor单线程模型的实现）改写一下这几个类： step1.首先新增SubReactor类1234567891011121314151617181920212223242526272829303132333435363738394041424344public class SubReactor implements Runnable { private final Selector selector; private boolean register = false; //注册开关表示，为什么要加这么个东西，可以参考Acceptor设置这个值那里的描述 private int num; //序号，也就是Acceptor初始化SubReactor时的下标 SubReactor(Selector selector, int num) { this.selector = selector; this.num = num; } @Override public void run() { while (!Thread.interrupted()) { System.out.println(String.format(\"%d号SubReactor等待注册中...\", num)); while (!Thread.interrupted() &amp;&amp; !register) { try { if (selector.select() == 0) { continue; } } catch (IOException e) { e.printStackTrace(); } Set selectedKeys = selector.selectedKeys(); Iterator it = selectedKeys.iterator(); while (it.hasNext()) { dispatch(it.next()); it.remove(); } } } } private void dispatch(SelectionKey key) { Runnable r = (Runnable) (key.attachment()); if (r != null) { r.run(); } } void registering(boolean register) { this.register = register; }} 代码块3 这个类负责Acceptor交给自己的事件select（例子中实际上就是read、send）。 step2.Acceptor类的更改1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class Acceptor implements Runnable { private final ServerSocketChannel serverSocketChannel; private final int coreNum = Runtime.getRuntime().availableProcessors(); // 获取CPU核心数 private final Selector[] selectors = new Selector[coreNum]; // 创建selector给SubReactor使用，个数为CPU核心数（如果不需要那么多可以自定义，毕竟这里会吞掉一个线程） private int next = 0; // 轮询使用subReactor的下标索引 private SubReactor[] reactors = new SubReactor[coreNum]; // subReactor private Thread[] threads = new Thread[coreNum]; // subReactor的处理线程 Acceptor(ServerSocketChannel serverSocketChannel) throws IOException { this.serverSocketChannel = serverSocketChannel; // 初始化 for (int i = 0; i &lt; coreNum; i++) { selectors[i] = Selector.open(); reactors[i] = new SubReactor(selectors[i], i); //初始化sub reactor threads[i] = new Thread(reactors[i]); //初始化运行sub reactor的线程 threads[i].start(); //启动（启动后的执行参考SubReactor里的run方法） } } @Override public void run() { SocketChannel socketChannel; try { socketChannel = serverSocketChannel.accept(); // 连接 if (socketChannel != null) { System.out.println(String.format(\"收到来自 %s 的连接\", socketChannel.getRemoteAddress())); socketChannel.configureBlocking(false); // reactors[next].registering(true); // 注意一个selector在select时是无法注册新事件的，因此这里要先暂停下select方法触发的程序段，下面的weakup和这里的setRestart都是做这个事情的，具体参考SubReactor里的run方法 selectors[next].wakeup(); // 使一個阻塞住的selector操作立即返回 SelectionKey selectionKey = socketChannel.register(selectors[next], SelectionKey.OP_READ); // 当前客户端通道SocketChannel向selector[next]注册一个读事件，返回key selectors[next].wakeup(); // 使一個阻塞住的selector操作立即返回 reactors[next].registering(false); // 本次事件注册完成后，需要再次触发select的执行，因此这里Restart要在设置回false（具体参考SubReactor里的run方法） selectionKey.attach(new AsyncHandler(socketChannel, selectors[next], next)); // 绑定Handler if (++next == selectors.length) { next = 0; //越界后重新分配 } } } catch (IOException e) { e.printStackTrace(); } }} 代码块4 可以跟以前的Acceptor做个对比，做了如下改动： 接受到连接后不再直接触发handler了 初始化一堆SubReactor（从反应堆），每个交给一个线程处理，注册读事件后顺序分配给不同的SubReactor去处理自己的selector监听。 以上，就可以把读写处理+业务处理与接受连接的Reactor彻底分开了，接受连接的事件不再受任何读写、业务相关的影响，只负责接收，目前即便是业务线程池用光线程发生排队，也不会影响到连接的接收，很大程度上降低了服务端的接收能力遭遇瓶颈的风险。 step3.改写AsyncHandler的打印这里就不po代码了，具体就是把SubReactor的序号传给handler，标记触发Handler的Reactor是哪个。 同样的，启动下服务端，再开启两个客户端（跟之前一样，每个客户端发10条消息终止连接），运行结果如下： 服务端： 1234567891011121314151617181920212223242526272829301号SubReactor等待注册中...3号SubReactor等待注册中...0号SubReactor等待注册中...2号SubReactor等待注册中...收到来自 /127.0.0.1:60407 的连接0号SubReactor等待注册中...收到来自 /127.0.0.1:60410 的连接1号SubReactor等待注册中...1号SubReactor触发：收到来自客户端/127.0.0.1:60410的消息: 客户端发送的第1条消息0号SubReactor触发：收到来自客户端/127.0.0.1:60407的消息: 客户端发送的第1条消息0号SubReactor触发：收到来自客户端/127.0.0.1:60407的消息: 客户端发送的第2条消息1号SubReactor触发：收到来自客户端/127.0.0.1:60410的消息: 客户端发送的第2条消息0号SubReactor触发：收到来自客户端/127.0.0.1:60407的消息: 客户端发送的第3条消息1号SubReactor触发：收到来自客户端/127.0.0.1:60410的消息: 客户端发送的第3条消息1号SubReactor触发：收到来自客户端/127.0.0.1:60410的消息: 客户端发送的第4条消息0号SubReactor触发：收到来自客户端/127.0.0.1:60407的消息: 客户端发送的第4条消息1号SubReactor触发：收到来自客户端/127.0.0.1:60410的消息: 客户端发送的第5条消息0号SubReactor触发：收到来自客户端/127.0.0.1:60407的消息: 客户端发送的第5条消息1号SubReactor触发：收到来自客户端/127.0.0.1:60410的消息: 客户端发送的第6条消息0号SubReactor触发：收到来自客户端/127.0.0.1:60407的消息: 客户端发送的第6条消息1号SubReactor触发：收到来自客户端/127.0.0.1:60410的消息: 客户端发送的第7条消息0号SubReactor触发：收到来自客户端/127.0.0.1:60407的消息: 客户端发送的第7条消息0号SubReactor触发：收到来自客户端/127.0.0.1:60407的消息: 客户端发送的第8条消息1号SubReactor触发：收到来自客户端/127.0.0.1:60410的消息: 客户端发送的第8条消息0号SubReactor触发：收到来自客户端/127.0.0.1:60407的消息: 客户端发送的第9条消息1号SubReactor触发：收到来自客户端/127.0.0.1:60410的消息: 客户端发送的第9条消息0号SubReactor触发：收到来自客户端/127.0.0.1:60407的消息: 客户端发送的第10条消息1号SubReactor触发：收到来自客户端/127.0.0.1:60410的消息: 客户端发送的第10条消息0号SubReactor触发：read时-------连接关闭1号SubReactor触发：read时-------连接关闭 客户端： 12345678910111213141516171819202122已完成 /127.0.0.1:2333 的连接已完成 /127.0.0.1:2333 的连接收到来自服务端的消息: 0号SubReactor触发：我收到来自/127.0.0.1:60407的信息辣：客户端发送的第1条消息, 200ok;收到来自服务端的消息: 1号SubReactor触发：我收到来自/127.0.0.1:60410的信息辣：客户端发送的第1条消息, 200ok;收到来自服务端的消息: 0号SubReactor触发：我收到来自/127.0.0.1:60407的信息辣：客户端发送的第2条消息, 200ok;收到来自服务端的消息: 1号SubReactor触发：我收到来自/127.0.0.1:60410的信息辣：客户端发送的第2条消息, 200ok;收到来自服务端的消息: 1号SubReactor触发：我收到来自/127.0.0.1:60410的信息辣：客户端发送的第3条消息, 200ok;收到来自服务端的消息: 0号SubReactor触发：我收到来自/127.0.0.1:60407的信息辣：客户端发送的第3条消息, 200ok;收到来自服务端的消息: 1号SubReactor触发：我收到来自/127.0.0.1:60410的信息辣：客户端发送的第4条消息, 200ok;收到来自服务端的消息: 0号SubReactor触发：我收到来自/127.0.0.1:60407的信息辣：客户端发送的第4条消息, 200ok;收到来自服务端的消息: 1号SubReactor触发：我收到来自/127.0.0.1:60410的信息辣：客户端发送的第5条消息, 200ok;收到来自服务端的消息: 0号SubReactor触发：我收到来自/127.0.0.1:60407的信息辣：客户端发送的第5条消息, 200ok;收到来自服务端的消息: 1号SubReactor触发：我收到来自/127.0.0.1:60410的信息辣：客户端发送的第6条消息, 200ok;收到来自服务端的消息: 0号SubReactor触发：我收到来自/127.0.0.1:60407的信息辣：客户端发送的第6条消息, 200ok;收到来自服务端的消息: 1号SubReactor触发：我收到来自/127.0.0.1:60410的信息辣：客户端发送的第7条消息, 200ok;收到来自服务端的消息: 0号SubReactor触发：我收到来自/127.0.0.1:60407的信息辣：客户端发送的第7条消息, 200ok;收到来自服务端的消息: 1号SubReactor触发：我收到来自/127.0.0.1:60410的信息辣：客户端发送的第8条消息, 200ok;收到来自服务端的消息: 0号SubReactor触发：我收到来自/127.0.0.1:60407的信息辣：客户端发送的第8条消息, 200ok;收到来自服务端的消息: 0号SubReactor触发：我收到来自/127.0.0.1:60407的信息辣：客户端发送的第9条消息, 200ok;收到来自服务端的消息: 1号SubReactor触发：我收到来自/127.0.0.1:60410的信息辣：客户端发送的第9条消息, 200ok;收到来自服务端的消息: 0号SubReactor触发：我收到来自/127.0.0.1:60407的信息辣：客户端发送的第10条消息, 200ok;收到来自服务端的消息: 1号SubReactor触发：我收到来自/127.0.0.1:60410的信息辣：客户端发送的第10条消息, 200ok; 到这里，主从Reactor模型就被改写完成了，上面的例子只是简单演示了下这个模型，所有的例子都是从单线程Reactor模型一点点改写来的，客户端没变过，为的是更好的测试服务端在不同模型下的表现。主从Reactor模型应用的比较多，比如著名NIO框架Netty底层模型也是基于主从Reactor模型来实现的。 到这里java nio的东西已经差不多记录完了，后续会开始netty的学习记录，当然上述例子弱化了buffer的使用，而且例子中不存在粘包拆包的问题（因为都是请求+应答的方式进行），如果把上面的例子改成客户端在未收到响应时就连续发送几条信息，服务端这时再次由写模式切换到读模式，就会从Channel里连续拿到这几条消息，这就导致了粘包问题，那么如何解决类似的问题呢？通常是定义一种协议，来区分消息头和尾，中间的消息体是我们真正需要的数据，这种协议也就是我们常说的应用层协议，比如HTTP、FTP等，这里不做赘述，之后会通过一个例子来完成这部分的补充说明。 代码地址 单线程Reactor模型：https://github.com/exceting/DemoAll/tree/master/jdk/src/main/java/demo/jdk/reactor/simple 多线程Reactor模型：同上，Acceptor里的Handler改成AsyncHandler即可 主从多线程Reactor模型：https://github.com/exceting/DemoAll/tree/master/jdk/src/main/java/demo/jdk/reactor/mainsub","link":"/2019/04/01/Java%20NIO%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%AE%B0%E5%BD%95%EF%BC%88%E5%85%AB%EF%BC%89%EF%BC%9A%20Reactor%E4%B8%A4%E7%A7%8D%E5%A4%9A%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AE%9E%E7%8E%B0/"},{"title":"Java NIO学习与记录（七）： Reactor单线程模型的实现","text":"一、Selector&amp;Channel1.1：各种channel写这个模型需要提前了解Selector以及Channel，之前记录过FileChannel，除此之外还有以下几种Channel： ServerSocketChannel：用于监听新的TCP连接的通道，负责读取&amp;响应，通常用于服务端的实现。 SocketChannel：用于发起TCP连接，读写网络中的数据，通常用于客户端的实现。 DatagramChannel：上述两个通道基于TCP传输协议，而这个通道则基于UDP，用于读写网络中的数据。 FileChannel：从文件读取数据。 本篇重点放在ServerSocketChannel和SocketChannel上，大部分客户端/服务端为了保证数据准确性，都是基于TCP传输协议实现的，由于使用Selector注册必须要求被注册的Channel是非阻塞模式的，因此FileChannel由于没有非阻塞模式（无法设置configureBlocking(false)），没办法和注册到selector。 1.2：selectorSelector是个通道注册器（用法会在程序里标注），是实现Reactor模型的关键，多个通道均可以注册到Selector，Selector负责监听每个Channel的几个事件：连接就绪、写就绪、读就绪，当某个channel注册感兴趣就绪事件到selector时，若发生兴趣事件就绪，则Selector.select()方法不再阻塞，返回兴趣事件集合（可能包含多个channel的），然后按照事件不同进行分发处理。 Selector返回对应的就绪事件，封装为SelectionKey，每个Channel对应一个SelectionKey，这个对象还可以通过attach方法附着处理类（Handler、Acceptor等）。 1.3：一个简单的例子先来看个简单使用Selector做处理的服务端实现，可以简单对Selector和SelectionKey的用法做个了解： 123456789101112131415161718192021222324252627282930313233343536373839public static void main(String[] args) throws IOException { Selector selector = Selector.open(); //打开选择器 ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); //打开通道 serverSocketChannel.configureBlocking(false); //设置通道为非阻塞模式 serverSocketChannel.bind(new InetSocketAddress(2333)); //绑定端口 serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); //注册channel到选择器，指定监听该Channel的哪些事件，初始化都是对连接事件监听（因为是入口） while (selector.select() &gt; 0) { // 若收到就绪事件select返回“感兴趣”事件集合，否则阻塞当前线程 Set keys = selector.selectedKeys(); //获取本次拿到的事件集合 Iterator iterator = keys.iterator(); while (iterator.hasNext()) { SelectionKey key = iterator.next(); iterator.remove(); if (key.isAcceptable()) { //当前就绪事件为连接事件 ServerSocketChannel skc = (ServerSocketChannel) key.channel(); //连接就绪触发，说明已经有客户端通道连了过来，这里需要拿服务端通道去获取客户端通道 SocketChannel socketChannel = skc.accept(); //获取客户端通道（连接就绪，说明客户端接下来可能还有别的动作，比如读和写） socketChannel.configureBlocking(false); //同样的需要设置非阻塞模式 System.out.println(String.format(\"收到来自 %s 的连接\", socketChannel.getRemoteAddress())); socketChannel.register(selector, SelectionKey.OP_READ); //将该客户端注册到选择器，感兴趣事件设置为读（客户端连接完毕，很肯能会往服务端写数据，因此这里要注册读事件用以接收这些数据） } else if (key.isReadable()) { //当前事件为读就绪 SocketChannel socketChannel = (SocketChannel) key.channel(); //能触发读就绪，说明客户端已经开始往服务端写数据，通过SelectionKey拿到当前客户端通道 ByteBuffer buffer = ByteBuffer.allocate(1024); int count = socketChannel.read(buffer); //从通道读入数据 if (count &lt; 0) { //若本次读就绪拿到-1，则认为客户端主动断开了连接 socketChannel.close(); //服务端关闭客户端通道 key.cancel(); //断连后就将该事件从选择器的SelectionKey集合中移除（这里说一下，这里不是真正意义上的移除，这里是取消，会将该key放入取消队列里，在下次select函数调用时才负责清空） System.out.println(\"连接关闭\"); continue; } System.out.println(String.format(\"收到来自 %s 的消息: %s\", socketChannel.getRemoteAddress(), new String(buffer.array()))); } keys.remove(key); } } } 代码块1 上面是一个简单的例子，接下来，就利用选择器、通道来实现Reactor单线程模型。 二、单Reactor单线程模型的服务端实现实现服务端，服务端负责接收客户端的连接，接收客户端的请求数据以及响应客户端。 把上一篇的结构图再拿过来展示下，看看需要做的有哪些模块： 通过上图，我们需要实现的模块有Reactor、Acceptor、Handler，下面来逐个编写： 2.1：Reactor核心模块该模块内部包含两个核心方法，select和dispatch，该模块负责监听就绪事件和对事件的分发处理： 123456789101112131415161718192021222324252627282930313233343536373839404142public class Reactor implements Runnable { private final Selector selector; private final ServerSocketChannel serverSocketChannel; public Reactor(int port) throws IOException { //Reactor初始化 selector = Selector.open(); //打开一个Selector serverSocketChannel = ServerSocketChannel.open(); //建立一个Server端通道 serverSocketChannel.socket().bind(new InetSocketAddress(port)); //绑定服务端口 serverSocketChannel.configureBlocking(false); //selector模式下，所有通道必须是非阻塞的 //Reactor是入口，最初给一个channel注册上去的事件都是accept SelectionKey sk = serverSocketChannel.register(selector, SelectionKey.OP_ACCEPT); //attach callback object, Acceptor sk.attach(new Acceptor(serverSocketChannel, selector)); } @Override public void run() { try { while (!Thread.interrupted()) { selector.select(); //就绪事件到达之前，阻塞 Set selected = selector.selectedKeys(); //拿到本次select获取的就绪事件 Iterator it = selected.iterator(); while (it.hasNext()) { //这里进行任务分发 dispatch((SelectionKey) (it.next())); } selected.clear(); } } catch (IOException e) { e.printStackTrace(); } } void dispatch(SelectionKey k) { Runnable r = (Runnable) (k.attachment()); //这里很关键，拿到每次selectKey里面附带的处理对象，然后调用其run，这个对象在具体的Handler里会进行创建，初始化的附带对象为Acceptor（看上面构造器） //调用之前注册的callback对象 if (r != null) { r.run(); } }} 代码块2 细节已标注。 2.2：实现Acceptor模块这个模块只负责处理连接就绪事件，有了这个事件就可以拿到客户单的SocketChannel，就可以继续完成接下来的读写任务了： 1234567891011121314151617181920212223242526public class Acceptor implements Runnable { private final Selector selector; private final ServerSocketChannel serverSocketChannel; Acceptor(ServerSocketChannel serverSocketChannel, Selector selector) { this.serverSocketChannel = serverSocketChannel; this.selector = selector; } @Override public void run() { SocketChannel socketChannel; try { socketChannel = serverSocketChannel.accept(); if (socketChannel != null) { System.out.println(String.format(\"收到来自 %s 的连接\", socketChannel.getRemoteAddress())); new Handler(socketChannel, selector); //这里把客户端通道传给Handler，Handler负责接下来的事件处理（除了连接事件以外的事件均可） } } catch (IOException e) { e.printStackTrace(); } }} 代码块3 细节已标注。 2.3：Handler模块的实现这个模块负责接下来的读写操作： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788public class Handler implements Runnable { private final SelectionKey selectionKey; private final SocketChannel socketChannel; private ByteBuffer readBuffer = ByteBuffer.allocate(1024); private ByteBuffer sendBuffer = ByteBuffer.allocate(2048); private final static int READ = 0; private final static int SEND = 1; private int status = READ; Handler(SocketChannel socketChannel, Selector selector) throws IOException { this.socketChannel = socketChannel; //接收客户端连接 this.socketChannel.configureBlocking(false); //置为非阻塞模式（selector仅允非阻塞模式） selectionKey = socketChannel.register(selector, 0); //将该客户端注册到selector，得到一个SelectionKey，以后的select到的就绪动作全都是由该对象进行封装 selectionKey.attach(this); //附加处理对象，当前是Handler对象，run是对象处理业务的方法 selectionKey.interestOps(SelectionKey.OP_READ); //走到这里，说明之前Acceptor里的建连已完成，那么接下来就是读取动作，因此这里首先将读事件标记为“感兴趣”事件 selector.wakeup(); //唤起select阻塞 } @Override public void run() { try { switch (status) { case READ: read(); break; case SEND: send(); break; default: } } catch (IOException e) { //这里的异常处理是做了汇总，常出的异常就是server端还有未读/写完的客户端消息，客户端就主动断开连接，这种情况下是不会触发返回-1的，这样下面read和write方法里的cancel和close就都无法触发，这样会导致死循环异常（read/write处理失败，事件又未被cancel，因此会不断的被select到，不断的报异常） System.err.println(\"read或send时发生异常！异常信息：\" + e.getMessage()); selectionKey.cancel(); try { socketChannel.close(); } catch (IOException e2) { System.err.println(\"关闭通道时发生异常！异常信息：\" + e2.getMessage()); e2.printStackTrace(); } } } private void read() throws IOException { if (selectionKey.isValid()) { readBuffer.clear(); int count = socketChannel.read(readBuffer); //read方法结束，意味着本次\"读就绪\"变为\"读完毕\"，标记着一次就绪事件的结束 if (count &gt; 0) { System.out.println(String.format(\"收到来自 %s 的消息: %s\", socketChannel.getRemoteAddress(), new String(readBuffer.array()))); status = SEND; selectionKey.interestOps(SelectionKey.OP_WRITE); //注册写方法 } else { //读模式下拿到的值是-1，说明客户端已经断开连接，那么将对应的selectKey从selector里清除，否则下次还会select到，因为断开连接意味着读就绪不会变成读完毕，也不cancel，下次select会不停收到该事件 //所以在这种场景下，（服务器程序）你需要关闭socketChannel并且取消key，最好是退出当前函数。注意，这个时候服务端要是继续使用该socketChannel进行读操作的话，就会抛出“远程主机强迫关闭一个现有的连接”的IO异常。 selectionKey.cancel(); socketChannel.close(); System.out.println(\"read时-------连接关闭\"); } } } void send() throws IOException { if (selectionKey.isValid()) { sendBuffer.clear(); sendBuffer.put(String.format(\"我收到来自%s的信息辣：%s, 200ok;\", socketChannel.getRemoteAddress(), new String(readBuffer.array())).getBytes()); sendBuffer.flip(); int count = socketChannel.write(sendBuffer); //write方法结束，意味着本次写就绪变为写完毕，标记着一次事件的结束 if (count &lt; 0) { //同上，write场景下，取到-1，也意味着客户端断开连接 selectionKey.cancel(); socketChannel.close(); System.out.println(\"send时-------连接关闭\"); } //没断开连接，则再次切换到读 status = READ; selectionKey.interestOps(SelectionKey.OP_READ); } }} 代码块4 细节已标注。 关键模块已实现，下面来启动服务端： 1new Thread(new Reactor(2333)).start(); 代码块5 三、客户端的编写接下来同样利用selector编写客户端，客户端需要做的事情就是发送消息到服务端，等待服务端响应，然后再次发送消息，发够10条消息断开连接： 3.1：Client入口模块1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class NIOClient implements Runnable { private Selector selector; private SocketChannel socketChannel; NIOClient(String ip, int port) { try { selector = Selector.open(); //打开一个Selector socketChannel = SocketChannel.open(); socketChannel.configureBlocking(false); //设置为非阻塞模式 socketChannel.connect(new InetSocketAddress(ip, port)); //连接服务 //入口，最初给一个客户端channel注册上去的事件都是连接事件 SelectionKey sk = socketChannel.register(selector, SelectionKey.OP_CONNECT); //附加处理类，第一次初始化放的是连接就绪处理类 sk.attach(new Connector(socketChannel, selector)); } catch (IOException e) { e.printStackTrace(); } } @Override public void run() { try { while (!Thread.interrupted()) { selector.select(); //就绪事件到达之前，阻塞 Set selected = selector.selectedKeys(); //拿到本次select获取的就绪事件 Iterator it = selected.iterator(); while (it.hasNext()) { //这里进行任务分发 dispatch((SelectionKey) (it.next())); } selected.clear(); } } catch (IOException e) { e.printStackTrace(); } } void dispatch(SelectionKey k) { Runnable r = (Runnable) (k.attachment()); //这里很关键，拿到每次selectKey里面附带的处理对象，然后调用其run，这个对象在具体的Handler里会进行创建，初始化的附带对象为Connector（看上面构造器） //调用之前注册的callback对象 if (r != null) { r.run(); } }} 代码块6 细节已标注。 3.2：Connector模块（建连）123456789101112131415161718192021222324public class Connector implements Runnable { private final Selector selector; private final SocketChannel socketChannel; Connector(SocketChannel socketChannel, Selector selector) { this.socketChannel = socketChannel; this.selector = selector; } @Override public void run() { try { if (socketChannel.finishConnect()) { //这里连接完成（与服务端的三次握手完成） System.out.println(String.format(\"已完成 %s 的连接\", socketChannel.getRemoteAddress())); new Handler(socketChannel, selector); //连接建立完成后，接下来的动作交给Handler去处理（读写等） } } catch (IOException e) { e.printStackTrace(); } }} 代码块7 细节已标注。 3.3：客户端Handler模块实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879public class Handler implements Runnable { private final SelectionKey selectionKey; private final SocketChannel socketChannel; private ByteBuffer readBuffer = ByteBuffer.allocate(2048); private ByteBuffer sendBuffer = ByteBuffer.allocate(1024); private final static int READ = 0; private final static int SEND = 1; private int status = SEND; //与服务端不同，默认最开始是发送数据 private AtomicInteger counter = new AtomicInteger(); Handler(SocketChannel socketChannel, Selector selector) throws IOException { this.socketChannel = socketChannel; //接收客户端连接 this.socketChannel.configureBlocking(false); //置为非阻塞模式（selector仅允非阻塞模式） selectionKey = socketChannel.register(selector, 0); //将该客户端注册到selector，得到一个SelectionKey，以后的select到的就绪动作全都是由该对象进行封装 selectionKey.attach(this); //附加处理对象，当前是Handler对象，run是对象处理业务的方法 selectionKey.interestOps(SelectionKey.OP_WRITE); //走到这里，说明之前Connect已完成，那么接下来就是发送数据，因此这里首先将写事件标记为“感兴趣”事件 selector.wakeup(); //唤起select阻塞 } @Override public void run() { try { switch (status) { case SEND: send(); break; case READ: read(); break; default: } } catch (IOException e) { //这里的异常处理是做了汇总，同样的，客户端也面临着正在与服务端进行写/读数据时，突然因为网络等原因，服务端直接断掉连接，这个时候客户端需要关闭自己并退出程序 System.err.println(\"send或read时发生异常！异常信息：\" + e.getMessage()); selectionKey.cancel(); try { socketChannel.close(); } catch (IOException e2) { System.err.println(\"关闭通道时发生异常！异常信息：\" + e2.getMessage()); e2.printStackTrace(); } } } void send() throws IOException { if (selectionKey.isValid()) { sendBuffer.clear(); int count = counter.incrementAndGet(); if (count &lt;= 10) { sendBuffer.put(String.format(\"客户端发送的第%s条消息\", count).getBytes()); sendBuffer.flip(); //切换到读模式，用于让通道读到buffer里的数据 socketChannel.write(sendBuffer); //则再次切换到读，用以接收服务端的响应 status = READ; selectionKey.interestOps(SelectionKey.OP_READ); } else { selectionKey.cancel(); socketChannel.close(); } } } private void read() throws IOException { if (selectionKey.isValid()) { readBuffer.clear(); //切换成buffer的写模式，用于让通道将自己的内容写入到buffer里 socketChannel.read(readBuffer); System.out.println(String.format(\"收到来自服务端的消息: %s\", new String(readBuffer.array()))); //收到服务端的响应后，再继续往服务端发送数据 status = SEND; selectionKey.interestOps(SelectionKey.OP_WRITE); //注册写事件 } }} 代码块8 细节已标注。 下面启动客户端去连接之前的服务端： 12new Thread(new NIOClient(\"127.0.0.1\", 2333)).start();new Thread(new NIOClient(\"127.0.0.1\", 2333)).start(); 代码块9 上面模拟了两个客户端同时连到服务端，运行结果如下： 单线程Reactor模型有个致命的缺点，通过上述例子可以看出，整个执行流程都是线性的，客户端请求→服务端读取→服务端响应→客户端收到响应→客户端再次发送请求，那么在这个链路中，如果handler中某个位置存在性能瓶颈，比如我们可以改造下服务端的send方法： 1234567try { Thread.sleep(2000L); //响应2s} catch (InterruptedException e) { e.printStackTrace();}int count = socketChannel.write(sendBuffer); 代码块10 在响应客户端之前睡眠2s，当做是性能瓶颈点，同样的再次开两个客户端同时访问服务端，每个客户端发送10条消息，会发现，程序直接运行了40s，这是大多数情况下不愿意看到的，因此，就有了多线程Reactor模式，跟BIO为了提高性能将读操作放到一个独立线程处理一样，Reactor这样做，也是为了解决上面提到的性能问题，只不过NIO比BIO做异步有个最大的优势就是NIO不会阻塞一个线程，类似read这种操作状态都是由selector负责监听的，不像BIO里都是阻塞的，只要被异步出去，那么一定是非阻塞的业务代码（除非是人为将代码搞成阻塞），而BIO由于read本身阻塞，因此会阻塞掉整个业务线程，这也是同样是异步为什么NIO可以更加高效的原因之一。 那么单线程Reactor适用于什么情况呢？适用于那种程序复杂度很低的系统，例如redis，其大部分操作都是非常高效的，很多命令的时间复杂度直接为O(1)，这种情况下适合这种简单的Reactor模型实现服务端。","link":"/2019/03/27/Java%20NIO%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%AE%B0%E5%BD%95%EF%BC%88%E4%B8%83%EF%BC%89%EF%BC%9A%20Reactor%E5%8D%95%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%AE%9E%E7%8E%B0/"},{"title":"java性能火焰图的生成","text":"一、前言开始之前，你需要准备的环境： Linux系统机器或者虚拟机一台，里面需要安装的软件：git、jdk、perl。 二、简单介绍java性能分析火焰图的所做的事情就是能够分析出java程序运行期间存在的性能问题，因为某段代码拖慢整个程序执行是不允许的，因此靠火焰图的绘制和分析就可以找出类似的“问题代码段”。 那么这个图是怎么来的呢？首先跟大多数监控系统一样，数据采集+前端绘图，这个图也是根据某些数据绘制而成的，绘图工具本篇文章采用FlameGraph，而负责收集这些数据的工具，本篇采用async-profiler，这个工具会在程序运行期间向jvm发送信号采集其运行期数据（简单来说就是通过该工具可以找出程序中占用CPU资源时间最长的代码块，这里async-profiler的实现使用了jvmti，戳这里简单了解一下），然后生成相应的数据格式文件，而FlameGraph则负责读取和解析数据文件生成对应的火焰图（svg文件）。 三、使用&amp;安装🔥3.1：环境搭建确认你的机器已经安装了git、jdk、perl、c++编译器，部分可参考：安装杂记 🔥3.2：clone相关项目下载下来所需要的两个项目（这里建议放到data目录下）： 12git clone https://github.com/jvm-profiling-tools/async-profilergit clone https://github.com/brendangregg/FlameGraph 🔥3.3：编译下载好以后，需要打开async-profiler文件，输入make指令进行编译： 12cd async-profilermake 🔥3.4：编写测试程序编译完成后，我们来写一个简单的java程序： 123456789101112131415161718192021222324public class Test { public static void main(String[] args) throws Exception { Test test = new Test(); while (true) { test.func1(); test.func2(); test.func3(); } } public void func1() throws Exception { //调用第一个方法，需要100ms Thread.sleep(100L); } public void func2() throws Exception { //调用第二个方法，需要500ms Thread.sleep(500L); } public void func3() throws Exception { //调用第三个方法，需要1500ms Thread.sleep(1500L); }} 代码块1 非常简单的一个java类，main方法里所做的事情也很简单，现在把这个文件搞到data目录下，javac命令编译，java命令启动。 然后找到这个java程序的进程id： 123ps -ef | grep javaroot 30937 17605 0 19:12 pts/0 00:00:00 java Testroot 30961 23135 0 19:12 pts/1 00:00:00 /bin/grep --color=auto java 可以确认此时Test类运行时的java进程pid = 30937，当然也可以使用jps命令直接查看java进程，效果是一样的。 🔥3.5：生成火焰图数据ok，上述步骤完成后，现在进入async-profiler那个项目的目录下，然后输入如下指令： 1./profiler.sh -d 60 -o collapsed -f /tmp/test_01.txt ${pid} 上面的-d表示的是持续时长，后面60代表持续采集时间60s，-o表示的是采集规范，这里用的是collapsed，-f后面的路径，表示的是数据采集后生成的数据存放的文件路径（这里放在了/tmp/test_01.txt），${pid}表示的是采集目标进程的pid，也就是上面提到的30937 回车运行，运行期间阻塞，知道约定时间完成。运行完成后，现在去tmp下看看有没有对应文件： 🔥3.6：生成svg文件上一步产生的文件里的内容，肉眼是很难看懂的，所以现在FlameGraph的作用就体现出来了，它可以读取该文件并生成直观的火焰图，现在进入该项目目录下面，执行如下语句： 1perl flamegraph.pl --colors=java /tmp/test_01.txt &gt; test_01.svg 因为是perl文件，这里使用perl指令运行该文件，后面--colors表示着色风格，这里是java，后面的是数据文件的路径，这里是刚刚上面生成的那个文件/tmp/test_01.txt，最后的test_01.svg就是最终生成的火焰图文件存放的路径和文件命名，这里是命名为test_01.svg并保存在当前路径，运行后看到该文件已经存在于当前目录下： 🔥3.7：展示现在下载下来该文件，使用浏览器打开，效果如下： 果然还是看不懂啊-_-|| 后续会更新这东西怎么看和分析，或者说我这篇文章里的java例子可能并不能很好的体现出什么。 续更续更，公司内部火焰图已经上线，通过更为复杂的业务场景生成的图反而看起来更容易理解一些，因为业务代码的调用也会打印出来，下面贴一下内部某业务系统火焰图： 这张图是在某个业务系统运行时，采样60s生成的火焰图，通过这样一张图可以看出，x轴为调用顺序，y轴为栈深，线条颜色无实际意义（并不是越红性能越差之类的），线条长度代表CPU执行该方法时所花费的时间占比，一般来说需要关注的就是栈顶，且宽度比较大的那个。因为一般处于栈顶的，而且宽度比较大的调用栈，说明其存在性能问题，这样分析的原因如下： 栈深度与y轴高度成正比，一般造成性能问题的都在调用栈的栈顶位置，因为栈顶位置的性能问题会间接拖慢整个调用栈，比如上图中每个栈底的线条都很长，这是因为越往上栈越深，对下层的影响就越大，可以简单抽象成方法调用：A调用B，B调用C，C慢会间接导致B慢，从而导致A慢，当然符合这种情况就适合之前说的看栈顶分析瓶颈的方法，如果A本身就慢呢？通过火焰图也是可以看出来的，比如栈底的线条宽度很宽，但是建立在该栈底的调用链上，线条都很窄，火焰图呈现┻型，那么就可以认定，栈底方法存在性能问题，一般情况下都是从栈顶看起，视情况而定~","link":"/2019/03/22/java%E6%80%A7%E8%83%BD%E7%81%AB%E7%84%B0%E5%9B%BE%E7%9A%84%E7%94%9F%E6%88%90/"},{"title":"【杂记】linux下各种软件安装方法（持续记录）","text":"1.安装jdk网上一堆说先从windows下压缩包，然后通过共享文件夹copy到linux系统里，然后解压安装，emmmmm 首先进入usr文件夹，新建java文件夹： 1mkdir java 直接通过wget命令下载压缩包（如果找不到wget工具，可以通过apt-get install wget安装此工具）： 1wget --no-cookies --no-check-certificate --header \"Cookie: gpw_e24=http%3A%2F%2Fwww.oracle.com%2F; oraclelicense=accept-securebackup-cookie\" \"http://download.oracle.com/otn-pub/java/jdk/8u141-b15/336fa29ff2bb4ef291e347e091f7f4a7/jdk-8u141-linux-x64.tar.gz\" 后面url需要按照自己需要调整。 进入所在文件夹（这里指java文件夹）解压： 1tar -zxvf jdk-8u141-linux-x64.tar.gz 解压好了如下： 12root@xxx-xxx-xxx-01:/usr/java # ls -a. .. jdk1.8.0_141 jdk-8u141-linux-x64.tar.gz 接着配置环境变量，输入指令： 1vim /etc/profile 然后编辑： 1234export JAVA_HOME=/usr/java/jdk1.8.0_141 export PATH=$JAVA_HOME/bin:$PATHexport CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jarexport JRE_HOME=$JAVA_HOME/jre 然后让其生效： 1source /etc/profile 最后进行测试看看是否生效了： 1234root@xxx-xxx1-xxx-01:~# java -versionjava version \"1.8.0_141\"Java(TM) SE Runtime Environment (build 1.8.0_141-b15)Java HotSpot(TM) 64-Bit Server VM (build 25.141-b15, mixed mode) 出现版本号，视为安装配置成功。 2.安装Perl1234567wget http://www.cpan.org/src/5.0/perl-5.26.1.tar.gztar zxvf perl-5.26.1.tar.gzcd perl-5.26.1./Configure -demakemake testmake install wget后面的路径可以按需更改。安装过程比较耗时间，安装完成后可通过perl -version查看是否安装成功。 3.tcpdump抓包工具12apt-get updateapt-get install tcpdump 抓包工具tcpdump可以抓到容器内的网络请求，具体用法如下： 1tcpdump -i any -A -n port 80 | grep -C 50 'path' 上面是抓取端口为80的网络交互，且过滤出包含path关键词的交互，展示50行。 比如你想抓取http请求，知道http请求端口是80，还知道http请求具体的path，那么就可以抓取一个接口的请求信息（包含请求报文、响应报文），redis等同理，知道端口，知道关键词，就可以抓到交互。 4.C++编译器1apt-get install g++ 一般用于编译c++程序，缺少这个编译器进行make编译c++代码时，会报g++: not found的错误。 5.zookeeper的安装&amp;启动先下载zookeeper的安装包，然后解压： 1tar -zxvf zookeeper-3.4.6.tar.gz 解压后进入该包路径，然后进入conf目录修改zoo_sample.cfg的名字为zoo.cfg： 1mv zoo_sample.cfg zoo.cfg 然后打开该文件： 12345678910111213141516171819202122232425262728# The number of milliseconds of each ticktickTime=2000# The number of ticks that the initial # synchronization phase can takeinitLimit=10# The number of ticks that can pass between # sending a request and getting an acknowledgementsyncLimit=5# the directory where the snapshot is stored.# do not use /tmp for storage, /tmp here is just # example sakes.dataDir=/tmp/zookeeper# the port at which the clients will connectclientPort=2181# the maximum number of client connections.# increase this if you need to handle more clients#maxClientCnxns=60## Be sure to read the maintenance section of the # administrator guide before turning on autopurge.## http://zookeeper.apache.org/doc/current/zookeeperAdmin.html#sc_maintenance## The number of snapshots to retain in dataDir#autopurge.snapRetainCount=3# Purge task interval in hours# Set to \"0\" to disable auto purge feature#autopurge.purgeInterval=1 重要解释： tickTime：这个时间是作为 Zookeeper服务器之间或客户端与服务器之间维持心跳的时间间隔，也就是每个 tickTime时间就会发送一个心跳。dataDir：顾名思义就是Zookeeper保存数据的目录，默认情况下，Zookeeper的日志文件是在bin目录下，有一个zookeeper.out文件。clientPort：这个端口就是客户端连接 Zookeeper服务器的端口，Zookeeper会监听这个端口，接受客户端的访问请求。伪集群模式下，这个端口需要配置成不同的。如果是多台虚拟机或者服务器下，则无需更改。 接下来，我们来标记下该zk节点的id（节点号），在dataDir显示的路径下新建myid文件，写上一个数字（1~255间），这里写的是1： 123vim myid1:wq 然后继续回到conf目录下，编辑zoo.cfg，在下面添加如下配置： 1server.1=xx.xx.xxx.xx:8881:7771 前面的server.1里的1就是之前在myid里写的id号，zk节点唯一标识，后面的xx.xx.xxx.xx标识本机ip； 再往后的8881表示的是这个服务器与集群中的 Leader 服务器交换信息的端口（自定义）； 再后面的7771表示的是万一集群中的Leader服务器挂了，需要一个端口来重新进行选举，选出一个新的 Leader，而这个端口就是用来执行选举时服务器相互通信的端口。 接下来返回到zk的bin目录，进行启动这个zk服务： 1./zkServer.sh start 看到下面的打印说明启动成功： 123JMX enabled by defaultUsing config: /usr/zk/zookeeper-3.4.6/bin/../conf/zoo.cfgStarting zookeeper ... STARTED 集群搭建比较简单，直接改下配置，把zoo.cfg下面的ip+port往下面加节点就行了，例子： 123server.x=yyy.yy.yyy.yy:8881:7771server.x=yyy.yy.yyy.yy:8881:7771server.x=yyy.yy.yyy.yy:8881:7771 注意，集群里的每一个节点都要加上上面的配置，上面配置里的x就是指之前单机的myid文件放的id号，需要注意的是集群模式下，这些id是不允许有重复的，后面的yy.yy指的是节点ip地址，再往后的8881和7771之前有解释过，上翻查看。 这样配置后，将所有节点重启一遍即可，期间会进行Leader的选举，完成后可以运行bin目录下的zkServer.sh status查看其身份。","link":"/2019/03/22/%E3%80%90%E6%9D%82%E8%AE%B0%E3%80%91linux%E4%B8%8B%E5%90%84%E7%A7%8D%E8%BD%AF%E4%BB%B6%E5%AE%89%E8%A3%85%E6%96%B9%E6%B3%95%EF%BC%88%E6%8C%81%E7%BB%AD%E8%AE%B0%E5%BD%95%EF%BC%89/"},{"title":"Java NIO学习与记录（六）： NIO线程模型","text":"上一篇说的是基于操作系统的IO处理模型，那么这一篇来介绍下服务器端基于IO模型和自身线程的处理方式。 一、基于BIO下的线程处理模式这种处理模型是基于阻塞IO进行的，上一篇讲过，阻塞IO会阻塞每一个IO操作，直到事件就绪，下面来看下阻塞IO下的服务端线程模型： 如上图所示，该线程模型基于阻塞IO模型实现，针对每个请求都需要抽出来一个线程进行处理读入数据、业务处理数据、返回响应结果给客户端，这个过程中读、写操作均会阻塞，且跟业务处理串行执行，该模式下，并发量过大时会大量创建线程，发生的大量上下文切换，从而导致CPU资源占用过大，当连接建立后，若当前线程暂无可读数据，则线程会一直阻塞在读操作上，造成线程资源浪费，即便使用线程池进行优化，虽然避免了大量创建线程，但也会出现线程资源浪费的问题，高并发下可能会造成排队、响应不及时的问题。 具体BIO服务器的实现参考：SocketChannel与BIO服务器 二、基于NIO下的Reactor线程模型利用操作系统NIO的API实现，Java对其API的调用进行了封装（select等），这里先不探讨怎么利用java的api去调用，先来看看它的基本流程是怎样的，Reactor模式下的线程模型又会根据线程数量、线程池数量的不同，细分了三种线程模型。 2.1：单Reactor单线程模型这是最简单的Reactor模型，整个过程中的事件处理全部发生在一个线程里： 上图示意就是个简单的NIO单Reactor单线程处理模型，流程如下： Reactor对象通过select监听客户端的请求事件，收到事件消息后通过dispatch进行任务分发。 如果是建连请求，则交由Acceptor对象处理连接请求，然后创建一个Handler对象继续完成后续处理 若不是建连请求，则dispatch会调用对应连接的Handler进行处理，Handle负责完成连接成功后的后续处理（读操作、写操作、业务处理等） 此模型很简单，易于理解，但是存在一定的问题，比如单线处理程模型下，无法发挥多核CPU的性能，如果Handler上的业务处理很慢，则意味着整个程序无法处理其他连接事件，造成性能问题。适用于业务处理快速、客户端连接较少的情况。 2.2：单Reactor多线程模型相较于上面的模型，对业务处理模块进行了异步处理，流程图如下： 上图示意属于单Reactor多线程处理模型，流程如下： Reactor对象通过select监听客户端的请求事件，收到事件消息后通过dispatch进行任务分发。 如果是建连请求，则交由Acceptor对象处理连接请求，然后创建一个Handler对象继续完成后续处理 若不是建连请求，则dispatch会调用对应连接的Handler进行处理，Handle负责完成连接成功后的读操作，读出来数据后的业务处理部分交由线程池异步处理，业务处理完成后发送给Handler处理完成的消息，然后再由Handler发送处理响应信息给对应的Client。 本模型充分利用了多核CPU的处理能力，降低了由业务处理引起的性能问题，Reactor线程仅负责接收连接、读写操作。但是Reactor除了负责连接处理外仍然负责读写操作，大量的请求下仍然可能仍然存在性能问题。 2.3：主从Reactor多线程模型这个模型中将会独立出另一个Reactor对象来处理非连接处理的其他处理，命名为从Reactor（SubReactor），流程图如下： 上图示意属于主从Reactor多线程处理模型，流程如下： 主Reactor对象（MainReactor）通过select监听客户端的连接事件，收到连接事件后交由Acceptor处理。 Acceptor处理完成后，MainReactor将此连接分配给SubReactor处理，SubReactor将此连接加入连接队列进行事件监听并建立Handler进行后续的各种操作，同上面的模型一致，SubReactor会监听新的事件，如果有新的事件发生，则调用Handler进行相应的处理。 Handler读出来数据后的业务处理部分交由线程池异步处理，业务处理完成后发送给Handler处理完成的消息，然后再由Handler发送处理响应信息给对应的Client。 该模型存在两个线程分别处理Reactor事件，主线程只负责处理连接事件，子线程只负责处理读写事件，这样主线程可以处理更多的连接，而不用关心子线程里的读写处理是否会影响到自己。目前这种模型被广泛使用在各种项目中（如Netty、Memcached等）。 以上的线程模型都是基于同步IO，异步IO这里不作说明，目前大部分项目都采用NIO的API进行实现（该模式下又分成了上述3种线程处理模型）。 下一篇将会针对NIO下的三种线程处理模型，介绍下Selector，以及利用Selector来写一下具体的实现代码。","link":"/2019/03/20/Java%20NIO%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%AE%B0%E5%BD%95%EF%BC%88%E5%85%AD%EF%BC%89%EF%BC%9A%20NIO%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B/"},{"title":"Java NIO学习与记录（五）： 操作系统的I/O模型","text":"在开始介绍NIO Reactor模式之前，先来介绍下操作系统的五种I/O模型，了解了这些模型，对理解java nio会有不小的帮助。 前言：一次网络请求的流程先来看下一个服务端处理一次网络请求的流程图： 一、图1解析1.1：内核空间&amp;用户空间内核空间：指操作系统运行时用于程序调度、虚拟内存的使用或者连接硬件资源的程序逻辑。 用户空间：应用程序能够申请使用的空间。 操作系统采用虚拟存储器，操作系统核心是内核（Kernel），独立于普通应用程序，它既可以访问受保护的内存空间，又有访问底层硬件设备的所有权限，为了保证内核安全，使得用户进程不直接操作内核，因此操作系统将虚拟存储器分为两个部分：内核空间&amp;用户空间 1.2：网络请求流程根据图1，客户端发起一个请求到服务端，请求首先到达的是服务端网卡（步骤1），然后将请求数据copy到内核空间的内核缓冲区内（步骤2），到这一步，我们说一个数据报已经准备好了。 用户空间里的web服务进程（我们真正的业务程序）发起读取内核缓冲区里的数据，若内核缓冲区准备好了数据报，则会将数据报由内核缓冲区copy到用户空间的web服务进程内（步骤3），然后拿着这些数据进行逻辑处理（步骤4），然后将处理结果copy到内和缓冲区（步骤5），然后内核缓冲区将该数据copy到网卡（步骤6），然后远程传输给客户端（步骤7），这就完成了一次网络请求-响应处理。 这里需要指出步骤3下面这一步，这一步没有计入步骤，但这一步恰好是理解I/O是否发生阻塞的关键，下面介绍阻塞/非阻塞IO时会详细讲。 1.3：套接字(socket)&amp;文件描述符(fd)TCP用主机的IP地址加上主机上的端口号作为TCP连接的端点，这种端点就叫做套接字（socket），套接字提供了很多供应用程序使用的API，比如accept、read、write等。 文件描述符(fd)，Unix/Linux系统下，其作为一个socket的句柄，可以看做是一个文件，在socket上收发数据，相当于对一个文件进行读写，所以一个socket句柄，通常也用表示文件句柄的fd来表示。 二、I/O模型2.1：阻塞&amp;非阻塞调用阻塞与非阻塞的概念是针对调用方（一般指我们的业务程序，如图1中的web服务器进程）来说的。 阻塞调用：图1中步骤1、2执行期间，没有数据到达内核缓冲区，这个时候web服务器进程发起的获取数据的请求会被直接阻塞，当前相关线程会被挂起，直到步骤1、2完成，有数据写入内核缓冲区，这个时候才会唤醒线程执行步骤3和4. 非阻塞调用：与阻塞调用相反，当没有数据到达内核缓冲区时，web服务发起的获取数据的请求不会发生阻塞，相关线程可以选择做其他事情，然后轮询着查询请求结果即可，当某次轮询出结果，则进行步骤3和4的操作。 2.2：同步&amp;异步处理同步与异步的概念是针对被调用方（一般是指内核空间里的IO处理，如图1中的步骤1和2）来说的（一定要区分和理解阻塞/非阻塞、同步/异步这两个概念）。 同步处理：被调用方得到最终处理结果才返回给调用方。 异步处理：被调用方不用得到结果，只需返回一个状态给调用方，然后开始IO处理，处理完了就主动返回通知调用方。 2.3：数据输入的两个阶段一个网络输入流程包含下面两个阶段： 数据准备（步骤1、2）。 将准备好的数据从内核空间复制到用户空间（步骤3）。 2.4：阻塞IO模型我们从图1的步骤3下面的那次请求开始画图，阻塞式I/O模型处理流程如下： 从上图可以看出，阻塞IO模型是指从应用程序发起从socket获取数据（recvfrom）那一刻起，如果内核里没有准备好的数据报，则直接阻塞应用程序，导致应用程序无法去做别的任何事情，直到数据报准备好，被阻塞的程序才会被唤醒，继续处理下面拿到的数据报。 阻塞IO模型只允许一个线程处理一个连接请求，因此当并发量大的时候，会创建大量线程，线程切换开销很大，导致程序处理性能低下。具体参考BIO模式的服务端实现：SocketChannel与BIO服务器 2.5：非阻塞IO模型同样从应用程序发起获取数据的地方开始画图，非阻塞式I/O模型处理流程如下： 从上图可以看出，非阻塞模式也是相对于调用者的，调用者在发送获取数据的请求时会将对应套接口设置为非阻塞，这样在数据报还未准备好的时候，应用程序就不会被阻塞了，然后应用程序再通过轮询的方式进行询问数据报是否已经准备好，当准备好后停止轮询，接下来的逻辑跟阻塞IO一致。对比可以发现，阻塞与非阻塞都是以调用方的角度看的，而且阻塞与否全在第一个阶段，第二个阶段都是一致的。非阻塞IO虽然不会阻塞应用程序，但是因为需要长时间的轮询，对于CPU来说，将会进行大量无意义的切换，资源利用率较低。 2.6：非阻塞IO-多路复用模型2.6.1：模型介绍IO多路复用模型处理流程如下： 从上图可以看出，IO多路复用，其实是找了个代理select，帮助监听多个IO通道的状态，某个通道有新状态产生，才触发recvfrom操作，没有新的状态产生，则select会阻塞。注意这里的阻塞，与阻塞IO模型里的不同，阻塞IO模型是指一个IO操作发生的阻塞行为，而这里select可以同时阻塞多个IO通道，也就是说select可能会监听到一个以上的IO通道的状态，直到有数据可读、可写时，才真正触发IO操作的函数。 🌿 多路复用 图4里的多路复用是说利用某个IO函数（这里是指select）同时监听多个IO通道的状态变更，这样应用程序就可以通过一个函数同时监听多个通道的就绪状态（如连接就绪、读就绪），多路复用跟后面要讲的NIO不是同一个概念，它只是一种处理模型，而NIO是一组API，它提供的select函数恰好可以实现这种数据处理模型。 另外一种多路复用是指基于传输层协议（如TCP）的特性来实现的数据流传输方式，根据TCP特性，同一个TCP连接可以同时传输多条数据和接收多条数据，而实现这种多路复用的方式取决于应用层协议（全双工通信的应用层协议，比如HTTP2）。多路是指多个数据流，复用是指复用同一个资源（这个资源放到图4就是指select函数，放到通信方式里就是指TCP连接），可以参考其原始概念：多路复用-百度百科。以及这篇知乎上的回答：IO 多路复用是什么意思？ 2.6.2：select、poll、epoll函数上述三个函数均提供IO多路复用的解决方案，但是它们之间存在差异性，下面会介绍具体的区别： select： select 函数监视的fd分为writefds、readfds、exceptfds三类，调用后select函数会阻塞，直到有fd就绪（可读、可写、或except），或超时（指定等待时间，如果立即返回设为null即可），函数返回。当select函数返回后，可以通过遍历fdset，来找到已经就绪的fd。 优点：跨平台支持，目前几乎所有的平台都支持。 缺点：单个进程内其监视的fd存在最大限制，一般为1024个（linux32）或者2048个（linux64）。另外一个缺点就是其会不断的轮询fdset，不管存不存在活跃的socket，它都会全部遍历一遍fdset来查找就绪的fd，导致浪费许多CPU的时间去做这件事。最后一个缺点是其可能会维护一个存放大量fd的数据结构，这样会使用户空间和内核空间在传递该结构时复制开销过大。 poll： 本质上和select没有区别，但是它解决了select监视fd个数的限制。 优点：对于监视的fd，采用链表结构存储，无个数限制。 缺点：基本上select有的缺点它都有，其次它还有个特点：水平触发，也就是说poll到的fd没有被处理掉，下次依旧能被poll到。 select和poll一样，在大量客户端连接进来时，它们的效率会随着客户端数量而线性下降。 epoll： Linux2.6开始支持的一个函数，是对select和poll的增强版本。 优点：没有监视fd个数的限制，主动通知（回调）机制，只关注活跃的fd，不用像select和poll那样全量遍历去找就绪的fd，因而也不存在随着客户端数量的增多而性能下降的问题。最后是内存拷贝，利用mmap()文件映射内存加速与内核空间的消息传递，即epoll使用mmap减少复制开销。 缺点：在大量客户端连接，并且大量活跃的fd时，其性能可能还不如select/poll。 2.7：信号驱动式IO模型信号驱动IO模型处理流程如下： 通过上图，在信号驱动 IO 模型中，应用程序使用套接口进行信号驱动 IO，并安装一个信号处理函数，进程继续运行并不会发生阻塞； 当数据准备好时，进程会收到一个 SIGIO 信号，可以在信号处理函数中调用 IO 操作函数处理数据。 这种模式下在大量IO操作时可能会发生信号队列溢出而导致无法通知。在TCP下，该模式几乎没用，TCP下可通知的条件过多，每一个都进行判断会消耗掉大量的资源。 2.8：异步IO模型（AIO）上面介绍的几种IO模型，对于IO处理本身而言，都是同步的，只有这个模型，针对IO处理本身来讲，是异步的。 下面来看看流程图： 由上图看出，此模型下首先由应用程序告知内核启动某个操作，并让内核在整个操作包括将数据从内核拷贝到应用程序的缓冲区的过程中，完成后通知应用程序。 这跟上面的信号驱动IO模型有所不同，这个模型通知给应用程序时，IO操作已经全部完成，应用程序直接拿数据就好，无需再做任何IO操作（这就是此模型叫异步IO处理的原因），而信号驱动通常是返回给应用程序一个数据报准备状态，真正的IO操作仍需要应用程序进行。 目前AIO并不完善，最常用的高性能IO模型仍然是IO多路复用模型。 三、总结这几种模型除了AIO属于异步IO以外，其余的几种全都是同步IO（即需要应用程序主动进行IO操作），而是否阻塞应用程序取决于第一个阶段的处理方式，前几种IO模型的区别全在于第一阶段的处理。 本文参考：https://zhuanlan.zhihu.com/p/43933717","link":"/2019/03/19/Java%20NIO%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%AE%B0%E5%BD%95%EF%BC%88%E4%BA%94%EF%BC%89%EF%BC%9A%20%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%9A%84IO%E6%A8%A1%E5%9E%8B/"},{"title":"利用CompletableFuture优化程序的执行效率","text":"一、线程池的Future模式在了解java8的CompletableFuture之前，先通过Future来解决一个问题，看个例子： 假设现在有一个网站，首页有顶部Banner位、左边栏、右边栏、用户信息几大模块需要加载，现在出一个接口，要求包装并吐出这几大模块的内容 先来抽象一个首页接口对象： 1234567891011121314151617public class WebModule { private String top; //顶部Banner位 private String left; //左边栏 private String right; //右边栏 private String user; //用户信息 //...get...set... @Override public String toString() { return String.format(\"top: %s; left: %s; right: %s; user: %s\", top, left, right, user); }} 代码块1 现在提供下面几个业务方法来获取这些信息： 1234567891011121314151617181920212223242526272829303132333435private String getTop() { // 这里假设getTop需要执行200ms try { Thread.sleep(200L); } catch (InterruptedException e) { e.printStackTrace(); } return \"顶部banner位\"; } private String getLeft() { // 这里假设getLeft需要执行50ms try { Thread.sleep(50L); } catch (InterruptedException e) { e.printStackTrace(); } return \"左边栏\"; } private String getRight() { // 这里假设getRight需要执行80ms try { Thread.sleep(80L); } catch (InterruptedException e) { e.printStackTrace(); } return \"右边栏\"; } private String getUser() { // 这里假设getUser需要执行100ms try { Thread.sleep(100L); } catch (InterruptedException e) { e.printStackTrace(); } return \"用户信息\"; } 代码块2 ok，现在来实现下这个接口： 123456789// 同步获取public WebModule getWebModuleMsgSync() { WebModule webModule = new WebModule(); webModule.setTop(getTop()); webModule.setLeft(getLeft()); webModule.setRight(getRight()); webModule.setUser(getUser()); return webModule;} 代码块3 上面的代码会一次调用一个方法来赋值，最终返回接口对象，这个方法的最终耗时为几个业务方法耗时的总和： 12通过同步方法获取首页全部信息消耗时间：435ms结果为：top: 顶部banner位; left: 左边栏; right: 右边栏; user: 用户信息 430ms左右的执行时间，其实这几个模块是相互独立没有影响的，因此可以使用线程池的Future模式来进行多线程处理优化： 12345678910111213// 异步获取public WebModule getWebModuleMsgAsync() throws ExecutionException, InterruptedException { Future top = executorService.submit(this::getTop); Future left = executorService.submit(this::getLeft); Future right = executorService.submit(this::getRight); Future user = executorService.submit(this::getUser); WebModule webModule = new WebModule(); webModule.setTop(top.get()); webModule.setLeft(left.get()); webModule.setRight(right.get()); webModule.setUser(user.get()); return webModule;} 代码块4 这几个方法会被异步执行，get方法会被阻塞，直到执行结束，运行结果如下： 12通过异步方法获取首页全部信息消耗时间：276ms结果为：top: 顶部banner位; left: 左边栏; right: 右边栏; user: 用户信息 可以看到，执行速度几乎降了近200ms，这取决于最慢的那个任务的耗时。 通过上述的例子可以发现，很多程序都是可以通过异步充分利用CPU资源的方式来进行优化处理的，单看上面的程序没什么问题，但是仔细想想会发现太过局限，因为几个模块相互独立，但在实际开发中，我们可能存在B方法需要拿到A方法的结果才可以往下进行的问题，所以上面的程序就不太适用了，java8出现了今天要说的一个内容：CompletableFuture，该类可以帮助你实现上面所说的任务顺序调度，不相干的程序依然在异步，相干的存在先后顺序的将会通过一定的设置来满足自己的顺序期望。 二、CompletableFuture现在再来假设一个例子，现在存在以下几个方法的调用： zero方法、a方法、b方法、ab方法、c方法、d方法、e方法 定义如下： 123456789101112131415161718192021222324252627282930313233343536//各个方法，sleep当成是执行时间private void zero() { sleep(100L); System.out.println(\"zero方法触发！\\n-----------------------------\");}private String a() { sleep(500L); return \"a\";}private String b(String a) { sleep(1000L); return a + \"b\";}private String c() { sleep(500L); return \"c\";}private String ab(String a, String b) { sleep(100L); return a + \"|\" + b;}private void d(String a) { sleep(1000L); System.out.println(\"d方法触发，拿到的a = \" + a);}private String e(String a) { sleep(100L); return a + \"e\";} 代码块5 根据上面的方法定义，可以整理出来其执行关系： zero、a、c都是独立调用的方法，而b、d、e方法都需要拿到a的执行结果值才能触发，ab方法则要求更加苛刻，需要同时拿到a和b的执行结果才可以触发，现在假设需要把所有的方法都触发一遍，我们又期望通过异步的方式来尽可能的优化代码，这个时候如果还用上面例子里的方式，恐怕就很难进行下去了，因为很多方法存在相互依赖的现象，不过现在有了CompletableFuture，这个问题就可以解决了，来看下代码（方法及作用都写在注释上了，下面的文章就不多做说明了）： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667public static void main(String[] args) throws ExecutionException, InterruptedException { long s = System.currentTimeMillis(); Test t = new Test(); //runAsync用于执行没有返回值的异步任务 CompletableFuture future0 = CompletableFuture.runAsync(t::zero) .exceptionally(e -&gt; { System.out.println(\"Zero出错！\"); return null; }); //这里是异常处理，指的是该异步任务执行中出错，应该做的处理 //supplyAsync方法用于执行带有返回值的异步任务 CompletableFuture futureA = CompletableFuture.supplyAsync(t::a) .exceptionally(e -&gt; { System.out.println(\"方法A出错！\"); return null; }); //thenCompose方法用于连接两个CompletableFuture任务，如下代表futureA结束后将执行结果交由另外一个CompletableFuture处理，然后将执行链路最终赋值给futureB CompletableFuture futureB = futureA.thenCompose(a -&gt; CompletableFuture.supplyAsync(() -&gt; t.b(a))) .exceptionally(e -&gt; { System.out.println(\"方法B出错！\"); return null; }); //thenAccept方法用于将一个任务的结果，传给需要该结果的任务，如下表示futureD的执行需要futureA的结果，与thenApply不同的是，这个方法没有有返回值 CompletableFuture futureD = futureA.thenAccept(t::d); //thenApply方法用于将一个任务的结果，传给需要该结果的任务，如下表示futureE的执行需要futureA的结果，与thenAccept不同的是，这个方法有返回值 CompletableFuture futureE = futureA.thenApply(t::e) .exceptionally(e -&gt; { System.out.println(\"方法E出错！\"); return null; }); /** * thenApply方法概念容易与thenCompose混淆，毕竟最终目的很相似 */ //thenCombine方法用于连接多个异步任务的结果，如下ab方法需要futureA和futureB的执行结果，那么就可以使用thenCombine进行连接 //注意，执行到ab这里，说明futureA和futureB一定已经执行完了 CompletableFuture futureAB = futureA.thenCombine(futureB, t::ab) .exceptionally(e -&gt; { System.out.println(\"方法AB出错！\"); return null; }); //单纯的一个异步任务，不依赖任何其他任务 CompletableFuture futureC = CompletableFuture.supplyAsync(t::c) .exceptionally(e -&gt; { System.out.println(\"方法C出错！\"); return null; }); //allOf如果阻塞结束则表示所有任务都执行结束了 CompletableFuture.allOf(future0, futureA, futureB, futureAB, futureC, futureD, futureE).get(); System.out.println(\"方法Zero输出：\" + future0.get()); System.out.println(\"方法A输出：\" + futureA.get()); System.out.println(\"方法B输出：\" + futureB.get()); System.out.println(\"方法AB输出：\" + futureAB.get()); System.out.println(\"方法C输出：\" + futureC.get()); System.out.println(\"方法D输出：\" + futureD.get()); System.out.println(\"方法E输出：\" + futureE.get()); System.out.println(\"耗时：\" + (System.currentTimeMillis() - s) + \"ms\"); } 代码块6 输出结果如下： 1234567891011zero方法触发！-----------------------------d方法触发，拿到的a = a方法Zero输出：null方法A输出：a方法B输出：ab方法AB输出：a|ab方法C输出：c方法D输出：null方法E输出：ae耗时：1668ms 可以看到，逻辑方面是没有任何问题的，也按照预期的顺序和方式进行了，注意看这里的运行时间，约等于1600ms，与第一个例子时长取决于执行时间最长的那个方法不同，上面的例子时长取决于有序的执行链的耗时最长的执行时间，分析下上面的程序，顺序链最长的，就是ab这条，ab需要a和b全部执行完，而b又依赖a的结果，因此ab执行完的时间就是500+1000的时间（a需要500ms，b又需要等待a，500ms后b触发，b自身又需要1000ms，等都结束了，再触发ab方法，而ab方法又需要100ms的执行时间，因此ab是最长的耗时方法，ab耗时=500+1000+100） 需要说明的是上述例子里用到的方法，几乎每个都有个重载方法，用来传递一个线程池对象，例子里用的都是不传的，用的是其内部的ForkJoinPool.commonPool()。 CompletableFuture的用法还有很多很多，较常用的应该就是例子里的几种，更多的用法以后会继续记录到这里。","link":"/2019/03/14/%E5%88%A9%E7%94%A8CompletableFuture%E4%BC%98%E5%8C%96%E7%A8%8B%E5%BA%8F%E7%9A%84%E6%89%A7%E8%A1%8C%E6%95%88%E7%8E%87/"},{"title":"图解java多线程设计模式（二）","text":"一、join &amp; interrupt这俩方法属于线程对象里的方法，属于线程本身的操作。 1.1：join方法用于等待一个线程的终止，等待期间将会阻塞，直到被等待的线程终止结束。 所以join可以用来做多任务异步处理，比如还是拿利用CompletableFuture优化程序的执行效率这篇里的第一个例子做优化，这篇文章里使用线程池的future模式进行多任务异步处理，现在使用join改写下： 再来简单贴下这几个方法： 1234567891011121314151617181920212223242526272829303132333435private String getTop() { // 这里假设getTop需要执行200ms try { Thread.sleep(200L); } catch (InterruptedException e) { e.printStackTrace(); } return \"顶部banner位\"; } private String getLeft() { // 这里假设getLeft需要执行50ms try { Thread.sleep(50L); } catch (InterruptedException e) { e.printStackTrace(); } return \"左边栏\"; } private String getRight() { // 这里假设getRight需要执行80ms try { Thread.sleep(80L); } catch (InterruptedException e) { e.printStackTrace(); } return \"右边栏\"; } private String getUser() { // 这里假设getUser需要执行100ms try { Thread.sleep(100L); } catch (InterruptedException e) { e.printStackTrace(); } return \"用户信息\"; } 代码块1 然后现在使用简单的线程做异步处理： 123456789101112131415161718192021222324// 简单异步获取 public WebModule getWebModuleMsgSimpleAsync() throws ExecutionException, InterruptedException { WebModule webModule = new WebModule(); Thread topTask = new Thread(() -&gt; webModule.setTop(this.getTop())); Thread leftTask = new Thread(() -&gt; webModule.setLeft(this.getLeft())); Thread rightTask = new Thread(() -&gt; webModule.setRight(this.getRight())); Thread userTask = new Thread(() -&gt; webModule.setUser(this.getUser())); //触发各个异步任务 topTask.start(); leftTask.start(); rightTask.start(); userTask.start(); //等待所有的任务均执行完毕 topTask.join(); leftTask.join(); rightTask.join(); userTask.join(); return webModule; } 代码块2 测试代码： 12345678@Test public void testSimpleASync() throws Exception { // 同步方法测试，预估耗时200ms long start = System.currentTimeMillis(); WebModule module = webHome.getWebModuleMsgSimpleAsync(); System.out.println(\"通过异步方法获取首页全部信息消耗时间：\" + (System.currentTimeMillis() - start) + \"ms\"); System.out.println(\"结果为：\" + module.toString()); } 代码块3 测试结果： 12通过异步方法获取首页全部信息消耗时间：272ms结果为：top: 顶部banner位; left: 左边栏; right: 右边栏; user: 用户信息 比预估的要多72ms，经过后来的测试，发现这72ms耗时发生在线程创建的时候，以及后续线程状态转换带来的消耗，下面等待异步结束的时间约等于200ms，符合预期。 1.2：interrupt方法用于主动终止一个线程，线程本身调用该方法后，视为已终止状态，join解除阻塞，下面来用interrupt和join来做个实验： 123456789101112131415161718192021222324252627282930313233343536373839404142434445public class JoinTest { private boolean isStop = false; public static void main(String[] args) throws Exception { JoinTest test = new JoinTest(); Thread loopT = new Thread(test::loopTask); loopT.start(); sleep(2000L); //2s后终止线程 test.setStop(true); long s = System.currentTimeMillis(); loopT.join(); System.out.println(\"线程终止后，join阻塞时间为：\" + (System.currentTimeMillis() - s)); System.out.println(\"end~\"); } public void setStop(boolean stop) { isStop = stop; } public void loopTask() { while (!isStop) { //若状态为false，则继续执行下面的逻辑，每隔1s打印一次 sleep(1000L); System.out.println(\"loop trigger ~\"); } Thread.currentThread().interrupt(); //在这里终止掉当前线程 //事实上，在终止掉线程后，还有接下来的逻辑要执行 long s = System.currentTimeMillis(); for (int i = 0; i &lt; 1000000; i++) { int[] a = new int[100]; //模拟耗时操作，这里不能用sleep了，因为当前线程已经被终止了 } System.out.println(\"线程终止后，逻辑块运行时间：\" + (System.currentTimeMillis() - s)); } public static void sleep(long time) { try { Thread.sleep(time); } catch (InterruptedException e) { e.printStackTrace(); } }} 代码块4 执行结果： 12345loop trigger ~loop trigger ~线程终止后，逻辑块运行时间：129线程终止后，join阻塞时间为：129end~ 即便线程被终止了，后面的逻辑也会触发，join依旧会选择阻塞，直到后续逻辑执行完毕，事实上，大部分任务都可以及时的终止，比如第一个例子，异步出去的任务，最终都会执行完成，线程变为终止状态，join都可以顺利结束，但是反观上例，如果没人及时的设置isStop的值，程序会一直执行下去，没有终止态，join会无止境的终止下去，这里提一下stop，线程的stop方法已被官方标记为不建议使用的方法，如果把上例的interrupt的调用换成stop，来看看其运行结果： 1234loop trigger ~loop trigger ~线程终止后，join阻塞时间为：0end~ 可以看到，线程终止后的后续逻辑均没有触发，等于说stop是一种很粗暴的终止线程的方式，一旦被stop，那么里面的业务逻辑将直接断掉，因此官方并不推荐使用该方法来终止线程。 而interrupt，仅仅是对目标线程发送了了一个中断信号（改变了线程的中断状态而已），当目标线程再次通过obj.wait、thread.sleep、thread.join方法进入阻塞状态时，接收到该信号，就会抛出InterruptedException异常，这时候需要业务方自行处理或者直接抛出，以结束线程阻塞状态（这里需要注意的是被obj.wait方法阻塞时，抛出该异常需要目标线程再次获得实例对象obj的锁才行）。 上述三个需要花费时间的方法均抛出了InterruptedException异常，针对这些特性，想要完成以下操作就非常方便了： 取消wait方法等待notify/notifyAll的处理 取消在sleep方法指定时间内停止的处理 取消join方法等待其他线程终止的处理 取消之后所做的处理，取决于需求，可能会终止线程，或者通知用户已取消，或者终止当前处理进入下一个处理阶段。 二、线程状态迁移图 上面的图展示出来的状态太多太杂，原因是它反应的是jvm里线程的状态迁移，如果换算成实际对操作系统层面的线程的影响，状态则少得多，至少无论是TIMED_WAITING还是BLOCKED，对应到操作系统的线程身上，那就是阻塞（或称等待，WAIT），现在就操作系统层面线程状态变化来简化一下上面的状态迁移图： 这里需要说明的是，JVM里的线程本质上就是操作系统里的线程，图1是告诉你JVM里线程的状态有多少种，其实就是按照各种影响线程的方法（join、wait、notify等）进行了细分，而图2则展示出了操作系统层面上的线程状态，上面也说过，其实无论是由join导致的线程阻塞，还是wait导致的线程阻塞，对于实际的线程影响性质都是一样的，那就是陷入等待。","link":"/2019/03/13/%E5%9B%BE%E8%A7%A3java%E5%A4%9A%E7%BA%BF%E7%A8%8B%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%88%E4%BA%8C%EF%BC%89/"},{"title":"Java NIO学习与记录（四）： SocketChannel与BIO服务器","text":"SocketChannel可以创建连接TCP服务的客户端，用于为服务发送数据，SocketChannel的写操作和连接操作在非阻塞模式下不会发生阻塞，这篇文章里的客户端采用SocketChannel实现，利用线程池模拟多个客户端并发访问服务端的情景。服务端仍然采用ServerSocket来实现，主要用来看下阻塞模式下的服务端在并发访问时所做出的的处理。 一、使用SocketChannel实现一个客户端1234567891011121314151617181920212223242526272829303132333435363738394041424344private static ExecutorService ctp = Executors.newCachedThreadPool(); public static void main(String[] args) { for (int i = 0; i &lt; 10; i++) { ctp.submit(IOTest::client); //并发十个客户端连接过去 } } public static void client() { ByteBuffer buffer = ByteBuffer.allocate(1024); //定义缓冲区 SocketChannel socketChannel = null; try { socketChannel = SocketChannel.open(); //打开SocketChannel socketChannel.configureBlocking(false); //设置为非阻塞模式 socketChannel.connect(new InetSocketAddress(\"127.0.0.1\", 2333)); //连接服务 while (true) { if(socketChannel.finishConnect()){ //这里的finishConnect是尝试连接，有可能返回false，因此使用死循环进行连接检查，确保连接已经正常建立。 System.out.println(\"客户端已连接到服务器\"); int i = 0; while (i &lt; 5) { TimeUnit.SECONDS.sleep(1); //隔一秒钟写一条 String info = \"来自客户端的第\" + (i++) + \"条消息\"; buffer.clear(); buffer.put(info.getBytes()); buffer.flip(); while (buffer.hasRemaining()) { socketChannel.write(buffer); //给服务写消息 } } break; } } } catch (IOException | InterruptedException e) { e.printStackTrace(); } finally { try { if (socketChannel != null) { System.out.println(\"客户端Channel关闭\"); socketChannel.close(); } } catch (IOException e) { e.printStackTrace(); } } 代码块1 上面会同时产生10个客户端去连接服务端 二、使用ServerSocket实现一个BIO的TCP服务12345678910111213141516171819202122232425262728293031323334ServerSocket serverSocket = null; int recvMsgSize = 0; InputStream in = null; try { serverSocket = new ServerSocket(2333); //开一个监听2333端口的TCP服务 byte[] recvBuf = new byte[1024]; while (true) { Socket clntSocket = serverSocket.accept(); //探听有没有新的客户端连接进来，没有就阻塞 SocketAddress clientAddress = clntSocket.getRemoteSocketAddress(); //通过跟服务连接上的客户端socket，拿到客户端地址 System.out.println(\"连接成功，处理客户端：\" + clientAddress); in = clntSocket.getInputStream(); //数据流 while ((recvMsgSize = in.read(recvBuf)) != -1) { //读取发送的数据，当客户端未断开连接，且不往服务端发数据的时候，说明一直处于准备读的状态，会一直阻塞下去，直到有数据写入（读就绪） byte[] temp = new byte[recvMsgSize]; System.arraycopy(recvBuf, 0, temp, 0, recvMsgSize); System.out.println(\"收到客户端\" + clientAddress + \"的消息内容：\" + new String(temp)); //打印消息 } System.out.println(\"-----------------------------------\"); } } catch (IOException e) { e.printStackTrace(); } finally { try { if (serverSocket != null) { System.out.println(\"socket关闭！\"); serverSocket.close(); } if (in != null) { System.out.println(\"stream连接关闭！\"); in.close(); } } catch (IOException e) { e.printStackTrace(); } } 代码块2 运行上面的代码，服务端打印如下： 12345678910111213141516171819202122232425262728293031323334连接成功，处理客户端：/127.0.0.1:54688收到客户端/127.0.0.1:54688的消息内容：来自客户端的第0条消息收到客户端/127.0.0.1:54688的消息内容：来自客户端的第1条消息收到客户端/127.0.0.1:54688的消息内容：来自客户端的第2条消息收到客户端/127.0.0.1:54688的消息内容：来自客户端的第3条消息收到客户端/127.0.0.1:54688的消息内容：来自客户端的第4条消息-----------------------------------连接成功，处理客户端：/127.0.0.1:54680收到客户端/127.0.0.1:54680的消息内容：来自客户端的第0条消息来自客户端的第1条消息来自客户端的第2条消息来自客户端的第3条消息来自客户端的第4条消息-----------------------------------连接成功，处理客户端：/127.0.0.1:54689收到客户端/127.0.0.1:54689的消息内容：来自客户端的第0条消息来自客户端的第1条消息来自客户端的第2条消息来自客户端的第3条消息来自客户端的第4条消息-----------------------------------连接成功，处理客户端：/127.0.0.1:54682收到客户端/127.0.0.1:54682的消息内容：来自客户端的第0条消息来自客户端的第1条消息来自客户端的第2条消息来自客户端的第3条消息来自客户端的第4条消息-----------------------------------连接成功，处理客户端：/127.0.0.1:54683收到客户端/127.0.0.1:54683的消息内容：来自客户端的第0条消息来自客户端的第1条消息来自客户端的第2条消息来自客户端的第3条消息来自客户端的第4条消息-----------------------------------连接成功，处理客户端：/127.0.0.1:54684收到客户端/127.0.0.1:54684的消息内容：来自客户端的第0条消息来自客户端的第1条消息来自客户端的第2条消息来自客户端的第3条消息来自客户端的第4条消息-----------------------------------连接成功，处理客户端：/127.0.0.1:54685收到客户端/127.0.0.1:54685的消息内容：来自客户端的第0条消息来自客户端的第1条消息来自客户端的第2条消息来自客户端的第3条消息来自客户端的第4条消息-----------------------------------连接成功，处理客户端：/127.0.0.1:54681收到客户端/127.0.0.1:54681的消息内容：来自客户端的第0条消息来自客户端的第1条消息来自客户端的第2条消息来自客户端的第3条消息来自客户端的第4条消息-----------------------------------连接成功，处理客户端：/127.0.0.1:54686收到客户端/127.0.0.1:54686的消息内容：来自客户端的第0条消息来自客户端的第1条消息来自客户端的第2条消息来自客户端的第3条消息来自客户端的第4条消息-----------------------------------连接成功，处理客户端：/127.0.0.1:54687收到客户端/127.0.0.1:54687的消息内容：来自客户端的第0条消息来自客户端的第1条消息来自客户端的第2条消息来自客户端的第3条消息来自客户端的第4条消息----------------------------------- 可以看到，消息是按照顺序，一个一个连接进来，然后完成处理的，至于后面的消息为什么会被合并成一个，也是这个原因，因为阻塞，所以等第一个连接逐条输出完成后，第二个连接进来，这时很可能客户端的SocketChannel已经将十条消息全部写入channel，等第一个连接处理完成后，接到第二条消息时就已经是全部的消息了，因此一次性输出，后面的合并也是这个原因（主要客户端使用NIO实现，因此写和连接服务不会发生阻塞，因此在第次个请求服务端还在处理时，其余的客户端数据也在执行并写入通道，最终服务端处理完第一个连接，然后继续接收第二个连接时，数据便是完整的5条数据了）。 上面的服务端是一个典型的阻塞IO的服务，accept在没有连接进来时会发生阻塞，read在客户端连接没关闭，且不再写消息时，服务端的read将一直处于读等待状态并阻塞，直到收到新的消息转为读就绪才会继续往下执行（这就是上面例子里第一个进来的连接可以逐条输出的原因），完全串行化，过程如下图： 下面，来改造下服务端，让其处理能力更好一些，除了accept，下面的处理逻辑全部交给线程池处理： 1234567891011121314151617181920212223while (true) { Socket clntSocket = serverSocket.accept(); //探听有没有新的客户端连接进来，没有就阻塞 SocketAddress clientAddress = clntSocket.getRemoteSocketAddress(); //通过跟服务连接上的客户端socket，拿到客户端地址 System.out.println(\"连接成功，处理客户端：\" + clientAddress); ctp.execute(() -&gt; { int recvMsgSize = 0; InputStream in = null; //数据流 try { in = clntSocket.getInputStream(); while ((recvMsgSize = in.read(recvBuf)) != -1) { //读取发送的数据，当客户端未断开连接，且不往服务端发数据的时候，说明一直处于准备读的状态，会一直阻塞下去，直到有数据写入（读就绪） byte[] temp = new byte[recvMsgSize]; System.arraycopy(recvBuf, 0, temp, 0, recvMsgSize); System.out.println(\"收到客户端\" + clientAddress + \"的消息内容：\" + new String(temp)); //打印消息 } System.out.println(\"-----------------------------------\"); } catch (IOException e) { e.printStackTrace(); } }); } 代码块3 运行结果： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970连接成功，处理客户端：/127.0.0.1:55259连接成功，处理客户端：/127.0.0.1:55265连接成功，处理客户端：/127.0.0.1:55266连接成功，处理客户端：/127.0.0.1:55257连接成功，处理客户端：/127.0.0.1:55260连接成功，处理客户端：/127.0.0.1:55258连接成功，处理客户端：/127.0.0.1:55261连接成功，处理客户端：/127.0.0.1:55262连接成功，处理客户端：/127.0.0.1:55263连接成功，处理客户端：/127.0.0.1:55264收到客户端/127.0.0.1:55265的消息内容：来自客户端的第0条消息收到客户端/127.0.0.1:55266的消息内容：来自客户端的第0条消息收到客户端/127.0.0.1:55258的消息内容：来自客户端的第0条消息收到客户端/127.0.0.1:55257的消息内容：来自客户端的第0条消息收到客户端/127.0.0.1:55261的消息内容：来自客户端的第0条消息收到客户端/127.0.0.1:55262的消息内容：来自客户端的第0条消息收到客户端/127.0.0.1:55263的消息内容：来自客户端的第0条消息收到客户端/127.0.0.1:55260的消息内容：来自客户端的第0条消息收到客户端/127.0.0.1:55259的消息内容：来自客户端的第0条消息收到客户端/127.0.0.1:55264的消息内容：来自客户端的第0条消息收到客户端/127.0.0.1:55265的消息内容：来自客户端的第1条消息收到客户端/127.0.0.1:55266的消息内容：来自客户端的第1条消息收到客户端/127.0.0.1:55258的消息内容：来自客户端的第1条消息收到客户端/127.0.0.1:55257的消息内容：来自客户端的第1条消息收到客户端/127.0.0.1:55261的消息内容：来自客户端的第1条消息收到客户端/127.0.0.1:55260的消息内容：来自客户端的第1条消息收到客户端/127.0.0.1:55262的消息内容：来自客户端的第1条消息收到客户端/127.0.0.1:55263的消息内容：来自客户端的第1条消息收到客户端/127.0.0.1:55259的消息内容：来自客户端的第1条消息收到客户端/127.0.0.1:55264的消息内容：来自客户端的第1条消息收到客户端/127.0.0.1:55266的消息内容：来自客户端的第2条消息收到客户端/127.0.0.1:55262的消息内容：来自客户端的第2条消息收到客户端/127.0.0.1:55261的消息内容：来自客户端的第2条消息收到客户端/127.0.0.1:55260的消息内容：来自客户端的第2条消息收到客户端/127.0.0.1:55263的消息内容：来自客户端的第2条消息收到客户端/127.0.0.1:55257的消息内容：来自客户端的第2条消息收到客户端/127.0.0.1:55265的消息内容：来自客户端的第2条消息收到客户端/127.0.0.1:55258的消息内容：来自客户端的第2条消息收到客户端/127.0.0.1:55259的消息内容：来自客户端的第2条消息收到客户端/127.0.0.1:55264的消息内容：来自客户端的第2条消息收到客户端/127.0.0.1:55258的消息内容：来自客户端的第3条消息收到客户端/127.0.0.1:55266的消息内容：来自客户端的第3条消息收到客户端/127.0.0.1:55257的消息内容：来自客户端的第3条消息收到客户端/127.0.0.1:55262的消息内容：来自客户端的第3条消息收到客户端/127.0.0.1:55261的消息内容：来自客户端的第3条消息收到客户端/127.0.0.1:55263的消息内容：来自客户端的第3条消息收到客户端/127.0.0.1:55265的消息内容：来自客户端的第3条消息收到客户端/127.0.0.1:55260的消息内容：来自客户端的第3条消息收到客户端/127.0.0.1:55264的消息内容：来自客户端的第3条消息收到客户端/127.0.0.1:55259的消息内容：来自客户端的第3条消息收到客户端/127.0.0.1:55266的消息内容：来自客户端的第4条消息收到客户端/127.0.0.1:55265的消息内容：来自客户端的第4条消息-----------------------------------收到客户端/127.0.0.1:55263的消息内容：来自客户端的第4条消息收到客户端/127.0.0.1:55261的消息内容：来自客户端的第4条消息-----------------------------------收到客户端/127.0.0.1:55260的消息内容：来自客户端的第4条消息-----------------------------------收到客户端/127.0.0.1:55262的消息内容：来自客户端的第4条消息-----------------------------------收到客户端/127.0.0.1:55257的消息内容：来自客户端的第4条消息----------------------------------------------------------------------收到客户端/127.0.0.1:55258的消息内容：来自客户端的第4条消息----------------------------------------------------------------------收到客户端/127.0.0.1:55264的消息内容：来自客户端的第4条消息-----------------------------------收到客户端/127.0.0.1:55259的消息内容：来自客户端的第4条消息----------------------------------- 消息被分开了，接收连接虽然仍然是串行，但实际的处理速度在多线程的帮助下已经比之前快很多了，流程如下图： 三、BIO总结综合看下来，传统的阻塞IO，按照图2的方式进行，虽然利用多线程避免了read等操作的阻塞对accept的影响，提高了处理效率，但想象下，如果现在存在高并发的情况，图2的模型如果不使用线程池，就会创建大量线程，会发生大量的线程上下文切换，影响整体效率，并且会影响新的线程，如果使用线程池，虽然某种程度上避免了线程的创建和上下文切换的量级，但是在大量并发的场景下，会发生排队，一旦发生排队，紧接着就会影响到accept。","link":"/2019/03/08/Java%20NIO%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%AE%B0%E5%BD%95%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%20SocketChannel%E4%B8%8EBIO%E6%9C%8D%E5%8A%A1%E5%99%A8/"},{"title":"Java NIO学习与记录（三）： Scatter&Gather介绍及使用","text":"上一篇知道了Buffer的工作机制，以及FileChannel的简单用法，这一篇介绍下Scatter&amp;Gather 1.Scatter（分散）用于描述在Channel中读取的数据分散在不同的Buffer里。 接着上一篇的例子（rua文件内容为123456789），改造下代码： 1234567891011121314151617181920212223readFile = new RandomAccessFile(\"D:\\\\rua.txt\", \"r\");FileChannel readChannel = readFile.getChannel();ByteBuffer first = ByteBuffer.allocate(2); //第一块bufferByteBuffer second = ByteBuffer.allocate(2); //第二块bufferByteBuffer[] byteBuffers = {first, second};long bytesRead = readChannel.read(byteBuffers); //从通道里读取数据到Buffer内（最大不超过Buffer容积）while (bytesRead != -1) { //当读不到任何东西时返回-1 System.out.println(\"\\nheader里的数据------此时byteRead=\" + bytesRead); first.flip(); //切换到Buffer读模式，读模式下可以读取到之前写入Buffer的数据 while (first.hasRemaining()) { System.out.print(\"-\" + (char) first.get()); //第一块Buffer读出的数据用减号分割，用于跟第二块区分 } first.clear(); System.out.println(\"\\nbody里的数据------此时byteRead=\" + bytesRead); second.flip(); while (second.hasRemaining()) { System.out.print(\"+\" + (char) second.get()); //第二块Buffer读出的数据用加号分割，用于跟第一块区分 } second.clear(); // 切换回Buffer的写模式 System.out.println(\"\\n----------------------------------------------\"); bytesRead = readChannel.read(byteBuffers); //跟上面一样，再次从通道读取数据到Buffer中}System.out.print(\"\\n-----------程序结束\"); 代码块1 上面的代码开了两个Buffer，然后传给了Channel.read一个Buffer数组，运行结果如下： 12345678910111213141516171819header里的数据------此时byteRead=4-1-2body里的数据------此时byteRead=4+3+4----------------------------------------------header里的数据------此时byteRead=4-5-6body里的数据------此时byteRead=4+7+8----------------------------------------------header里的数据------此时byteRead=1-9body里的数据------此时byteRead=1---------------------------------------------------------程序结束 可以看到，文件里的内容被分段加载出来了，first buffer里首选读取一段，然后接着second buffer再接着读取接下来的一段。上面例子符合Scatter的描述。 看过网上一些文章，说的最多的例子就是协议头数据体分开处理的例子： 假设通过Channel获取到的数据存在固定长度的协议头，以及已知最大长度限制的数据体，就可以通过两个Buffer来接收，一个是header buffer，一个是body buffer， 但这个对数据要求很严苛，结合上面的例子，不难发现，想要做到准确无误的处理这个例子，就得要求事先必须知道header的长度，以及数据体的最大长度上限，为什么要这样呢？因为如果不知道header的长度，那么header buffer就可能会读到body buffer里的东西或者body buffer里读到header buffer里的东西，如果不知道body的上限长度，那么如果body数据长度超过了body buffer的长度，body里的数据就会再次读到header buffer中去（这个可以结合上面的例子理解）。 2.Gather（聚集）用于描述在将不同Buffer里的数据写到同一个Channel中去。 来看个例子： 1234567891011readFile = new RandomAccessFile(\"D:\\\\haha.txt\", \"rw\");FileChannel channel = readFile.getChannel();ByteBuffer first = ByteBuffer.allocate(5); //第一块bufferByteBuffer second = ByteBuffer.allocate(5); //第二块bufferfirst.put(\"aa\".getBytes());second.put(\"bb\".getBytes());first.flip();second.flip();ByteBuffer[] byteBuffers = {first, second};channel.write(byteBuffers);System.out.print(\"\\n-----------程序结束\"); 代码块2 运行结束后，haha.txt里的内容为： 1aabb 可以看到，最终写入的数据就是按照顺序把两个buffer里的内容传输进去了。 同样的，还是以网上的协议头数据体的例子说事儿，这个跟Scatter下的传输方式比较起来就不会那么严格了，看到上面，初始容积为5个字节，但实际写到文件里的每个buffer仍然是两个字节，因为Gather模式下，Channel读取Buffer数据的时候，只会读取position到limit间的数据（可读区域），因此这里不用像多Buffer读一样要求那么严格，我们可以随意定义header buffer的长度，只要大于协议头本身长度即可，body buffer的要求其实是同上，也是大于数据体的长度上限即可。 这就是Scatter和Gather的全部内容了~其实简单理解，就是多Buffer操作，以及对网上那个例子，进行了更详细一点的说明。","link":"/2019/03/07/Java%20NIO%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%AE%B0%E5%BD%95%EF%BC%88%E4%B8%89%EF%BC%89%EF%BC%9A%20Scatter&Gather%E4%BB%8B%E7%BB%8D%E5%8F%8A%E4%BD%BF%E7%94%A8/"},{"title":"Java NIO学习与记录（二）：FileChannel与Buffer用法与说明","text":"上一篇简单介绍了NIO，这一篇将介绍FileChannel结合Buffer的用法，主要介绍Buffer 一、FileChannel例子上一篇说到，这个Channel属于文件通道，专门读取文件信息，NIO读取文件内容的简单的例子： 123456789101112131415161718192021222324252627public static void readFile() { RandomAccessFile file = null; try { file = new RandomAccessFile(\"D:\\\\rua.txt\", \"rw\"); FileChannel fileChannel = file.getChannel(); //获取文件通道 ByteBuffer buf = ByteBuffer.allocate(2); //分配容积为2字节的一块Buffer，用来读取数据 int bytesRead = fileChannel.read(buf); //从通道里读取数据到Buffer内（最大不超过Buffer容积） while (bytesRead != -1) { //当读不到任何东西时返回-1 buf.flip(); //切换到Buffer读模式，读模式下可以读取到之前写入Buffer的数据 while (buf.hasRemaining()) { //循环输出Buffer中的数据 System.out.print((char) buf.get()); } buf.compact(); //或者调用clear，切换回Buffer的写模式 bytesRead = fileChannel.read(buf); //跟上面一样，再次从通道读取数据到Buffer中 } } catch (IOException e) { e.printStackTrace(); } finally { try { if (file != null) { file.close(); } } catch (IOException e) { e.printStackTrace(); } } } 代码块1 rua.txt文件内容为：123456789 上述代码运行后输出如下： 1123456789 文件正常读取，可以结合上面的注释，来分析下过程，接下来要利用上面的例子介绍Buffer的一些概念。 二、Buffer的概念2.1：Buffer操作的步骤第一篇说过，Buffer是一个缓冲区，是一个容器，负责从通道读取数据或者写数据给通道，通过上面的例子，我们可以看到Buffer在读取通道数据时的几个步骤： step1：分配空间 1ByteBuffer.allocate(1024); //除此之外，还可以通过allocateDirector分配空间（具体不了解，先放一边，回头补） 代码块2 step2：从通道读取数据，写入Buffer 1int bytesRead = fileChannel.read(buf); 代码块3 step3：读取Buffer内的内容，切换回Buffer读模式 1buf.flip(); //切换到Buffer读模式，读模式下可以读取到之前写入Buffer的数据 代码块4 step4：循环执行读写操作，每取完一次Buffer中的值，都切换回写模式，再次从通道读取数据到Buffer 12345678while (bytesRead != -1) { //当读不到任何东西时返回-1 buf.flip(); //切换到Buffer读模式，读模式下可以读取到之前写入Buffer的数据 while (buf.hasRemaining()) { //循环输出Buffer中的数据 System.out.print((char) buf.get()); } buf.compact(); //或者调用clear，切换回Buffer的写模式 bytesRead = fileChannel.read(buf); //跟上面一样，再次从通道读取数据到Buffer中} 代码块5 2.2：Buffer读写流程详解2.2.1：重要属性的介绍Buffer具备的几个重要概念： capacity：缓冲区数组的总长度（容积） position：下一个需要操作的数据元素的位置 limit：缓冲区不可操作的下一个元素的位置，limit &lt;= capacity mark：用于记录position的前一个位置或默认是-1 2.2.2：Buffer内部的操作流程结合上面的概念，除了mark（再往下介绍），其余几个指标的操作变化如下图： 初始化一个容积为10的Buffer，初识位置limit = capacity，position位于第一个位置 当容器内写入了5个数据元素之后，position的位置变到了第6个位置，标记当前写入到哪里了 这时候不再写入数据了，开始切换回Buffer的读模式（flip），发生的变化如下： 发现，原先读模式下的position的位置被limit替换掉了，而position被重置为了第一个位置，这是因为现在读模式下想要读取之前写入的内容，为了保证读取的数据都是可读的（之前写入的），就需要有一个标记，来记录之前写模式下，操作到哪里了，position被重置为第一个位置，也很容易理解，因为切换了读模式，从头开始读取已写入的数据。 通过上面的描述，我们清楚了buffer是如何利用position、limit、capacity来完成读写操作的，下面我们来介绍下具体读写操作时Bufer发生的操作： Buffer切换读模式的方法有：flip Buffer切换写模式的方法有：clear、compact Buffer读数据：get 使用clear切换回写模式的时候，position会被置为0（也就是最初的位置），limit置为capacity（也就是最后的位置），意味着切换写模式之前未读的数据，将会被新一轮的写入覆盖，就再也找不回来了，所以除clear这个操作，Buffer还提供了compact方法来切换读模式，这个方法会把所有未读的数据拷贝到Buffer的起始位置，然后position指向最后一个未读数据的后一位，这样，下次开启读模式的时候，position操作同上，置为0，因此之前未读完被落下的数据也就在这时候被读到了。 下面我们来还原下这个转换过程： 走到上面的步骤后，我们切换成写模式，下面这个图分别表示了clear和compact两个方法下的两种操作： 根据图4和图5，结合上面的话，更容易理解clear和compact两种方式切换写模式所做的内部操作，以及为什么clear会丢数据，而compact不会。 下面通过一开始的例子，把中间读取数据的地方稍微做下修改： 12345678910ByteBuffer buf = ByteBuffer.allocate(2);int bytesRead = readChannel.read(buf);while (bytesRead != -1) { buf.flip(); char result = (char) buf.get(); //虽然读进来了2个字节，但这里只取一个 System.out.print(result); buf.clear(); // 使用clear切换回Buffer的写模式 bytesRead = readChannel.read(buf);}System.out.println(\"-----------程序结束\"); 代码块6 输出结果： 113579-----------程序结束 发现丢了一些数据，现在把clear改成compact，运行结果为： 112345678-----------程序结束 发现，除了9，都输出来了（至于9为啥没输出，因为每次只取了一个字节的数据呀~） 那么，如果切换到读模式，但是不读，然后切换回写模式继续写，会发生什么？ 改造上述代码如下： 12345678ByteBuffer buf = ByteBuffer.allocate(2);int bytesRead = readChannel.read(buf);while (bytesRead != -1) { buf.flip(); buf.clear(); // 不读，立刻切回写模式 bytesRead = readChannel.read(buf);}System.out.println(\"-----------程序结束\"); 代码块7 调用clear的情况下输出： 1-----------程序结束 而调用compact方法，却发生阻塞(死循环)了，结合之前的图，我们可以知道，如果不读，意味着在compact下会把未读的数据copy到Buffer里，如果一点都不读，那么意味着被copy的这批数据会占满整个Buffer，以至于position没有下一个位置可用，就会发生文件里的数据没办法被安排进缓冲区（意味着文件读不完），bytesRead一直不等于-1，发生死循环。 通过上面的图和例子，基本上可以理清楚读模式、写模式（包含不同的切换方式）下的Buffer内部处理方式。 2.3：Buffer的其他操作除了上面几种常规用法，Buffer还提供了其他的几个操作方法 2.3.1：rewind这个方法可以在读模式下，重置position的位置，也就是说在get执行后。position发生了位移，这个方法可以重置position的位置为初始位置，看例子： 12345678910ByteBuffer buf = ByteBuffer.allocate(2); //每次可读入两个字节int bytesRead = readChannel.read(buf);while (bytesRead != -1) { buf.flip(); System.out.print((char) buf.get()); buf.rewind(); //重置position System.out.print((char) buf.get());//这时候读到的数据跟上面是同一个 buf.clear(); bytesRead = readChannel.read(buf); } 代码块8 运行结果： 11133557799 可以看到，每次循环拿到两个字节，但两次获取的数据都是同一个，因为rewind把读取游标重置成初始位置了（也即是位置0） ⭐️ 这里说下，如果把上面的第二个get方法去掉，然后把clear模式改成compact模式同样也会发生死循环，因为rewind重置了游标，重置后又没有get方法再次读取，导致把本次的两个字节又复制进了Buffer，跟之前说的不读一样，会导致Buffer没有多余的空间放文件里的数据，导致一直读不完，发生死循环。 2.3.2：mark &amp; reset这两个方法放到一起说，因为mark跟reset不放在一起使用，没有任何意义。 mark：用于标记当前position的位置 reset：用于恢复被mark标记的position的位置 例子： 1234567891011ByteBuffer buf = ByteBuffer.allocate(2);int bytesRead = readChannel.read(buf);while (bytesRead != -1) { buf.flip(); buf.mark(); //标记当前位置，这里也就是初始位置 System.out.print((char) buf.get()); //读取到了初识位置数据 buf.reset(); //重置position到mark标记时那个值 System.out.print((char) buf.get()); //这里由于被reset了，因此输出的还是初识位置的数据 buf.clear(); bytesRead = readChannel.read(buf);} 代码块9 输出结果： 11133557799 会发现，上下两个打印都是是一样的数据。嘛，还是跟上面一样，再回顾一下，这里如果用compact会怎么操作？如果使用compact会打印如下语句： 11122334455667788 这个现在也很好理解了，因为重置了位置所以每个数据被打印了两次，由于mark的原因，每次实际上相当于只读了一个数据，所以剩下的一个数据被顺延到下次循环里打印，以此类推。 2.3.3：equals &amp; compareTo equals：用来比较两个Buffer是否相等，判等条件为 类型相同（byte、char等） 剩余元素个数相等 剩余元素相同 compareTo：比较两个Buffer中剩余元素的大小，如果满足如下条件，则认为buffer1小于buffer2： buffer1中第一个与buffer2不相等的元素小于buffer2的那个元素 所有元素相同，但是buffer1先比buffer2耗尽 三、实例：边读边写例子：将rua.txt里的内容在读的同时写入文件haha.txt里 12345678910111213readFile = new RandomAccessFile(\"D:\\\\rua.txt\", \"r\");writeFile = new RandomAccessFile(\"D:\\\\haha.txt\", \"rw\");FileChannel readChannel = readFile.getChannel(); //获取只读文件通道FileChannel writeChannel = writeFile.getChannel(); //获取写文件通道ByteBuffer readBuf = ByteBuffer.allocate(2); //分配容积为2字节的一块Buffer，用来读取数据int bytesRead = readChannel.read(readBuf); //从通道里读取数据到Buffer内（最大不超过Buffer容积）while (bytesRead != -1) { //当读不到任何东西时返回-1 readBuf.flip(); //切换到Buffer读模式，读模式下可以读取到之前写入Buffer的数据 writeChannel.write(readBuf); //将现在的buffer里的数据写到文件haha.txt里 readBuf.compact(); // 切换回Buffer的写模式 bytesRead = readChannel.read(readBuf); //跟上面一样，再次从通道读取数据到Buffer中} 代码块10 结果haha.txt里的内容为：123456789 四、Buffer的分类 类别 解释 ByteBuffer 支持存放字节类型数据，抽象类，有DirectByteBuffer、HeapByteBuffer、MappedByteBuffer两个子类，下面进行说明 CharBuffer 支持存放char类型数据 DoubleBuffer 支持存放double类型数据 FloatBuffer 支持存放float类型数据 IntBuffer 支持存放int类型数据 LongBuffer 支持存放long类型数据 ShortBuffer 支持存放short类型数据 表1 通过上表可以看到，Buffer有很多实现，其中大部分都对应一种基本类型，ByteBuffer比较特殊，下面介绍下它的两个子类。 ByteBuffer—-&gt;HeapByteBuffer 直接通过byte数组实现的在java堆上的缓冲区。 ByteBuffer—-&gt;DirectByteBuffer 直接在java堆外申请的一块内存，将文件映射到该内存空间，在大文件读写方面的效率非常高。 ByteBuffer—–&gt;MappedByteBuffer 同样写效率非常高。 关于这几个Buffer后续会专门整理一篇文章来写。","link":"/2019/03/05/Java%20NIO%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%AE%B0%E5%BD%95%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9AFileChannel%E4%B8%8EBuffer%E7%94%A8%E6%B3%95%E4%B8%8E%E8%AF%B4%E6%98%8E/"},{"title":"Java NIO学习与记录（一）：初识NIO","text":"工作中有些地方间接用到了netty，netty是一个NIO框架，对于NIO却不是那么熟悉，这个系列的文章是我在学习NIO时的一个记录，也期待自己可以更好的掌握NIO。 一、NIO是什么？一组由操作系统提供的底层API，java支持对它们的封装，我们需要做的是利用java来使用它们。非阻塞式IO，与传统的BIO（阻塞式IO）不同，NIO可以通过通道（Channels）来监听各通道的动作，一个线程就可以完成对多个通道的动作监听，这些动作包括连接就绪、写就绪、读就绪等，举个例子，建立连接这个动作在BIO中会发生阻塞，直到连接建立完成，而在NIO中，建连只是单线程里Selector监听的一个动作，也就是说在建立连接之前（即建连就绪之前），这个线程是可以继续处理其他动作的（比如读、写等等）。没有传统IO在IO交互时的阻塞现象，仅通过一个线程来处理n多IO通道（Channels），大大提升了程序的处理能力。 二、NIO核心组成部分2.1：Channels &amp; Buffer这两个放在一起讲，Channel是指各种IO通道（包括File、Socket等），也可以理解为双向IO流，即：Channel既可以将自己的内容读到Buffer中，也可以从Buffer中读取数据写入到自己里面。其次，Channel并不一定是双向的，一个Channel如果实现定义read( )方法的 ReadableByteChannel 接口,而另一个 Channel 类也许实现 WritableByteChannel 接口以提供write( )方法。实现这两种接口其中之一的类都是单向的，只能在单个方向上传输数据。如果一个类同时实现这两个接口,那么它是双向的,可以双向传输数据（读&amp;写）。 结合上面描述，Buffer是一个容器，负责存储从Channel里读到的数据或者存储准备写入Channel的数据，属于一个缓冲区。 常见的Channel： FileChannel：文件通道 DatagramChannel：一个能收发UDP包的通道。因为UDP是无连接的网络协议，所以不能像其它通道那样读取和写入。它发送和接收的是数据包 SocketChannel：一个连接到TCP网络套接字的通道 ServerSocketChannel：一个可以监听新进来的TCP连接的通道 常见的Buffer： ByteBuffer、CharBuffer、DoubleBuffer、FloatBuffer、IntBuffer、LongBuffer、ShortBuffer 可以发现，涵盖了基本传输的数据类型。 2.2：Selector可以理解为“Channel事件选择处理器”，第一部分有提到说NIO属于非阻塞式IO，关键点在于Selector可以运行于一个单线程里，然后通过一个死循环（阻塞方法）来监听注册到选择器内的Channel的事件（注意这里，一个Channel的事件想要被选择器监听到，则要求该Channel必须注册到选择器里），这些事件上面也提到过包含：连接就绪、写就绪、读就绪等。这些动作全部由Channel自己完成，什么时候完成了通知选择器去进行相应的逻辑处理即可，而不必阻塞在IO交互上（也是与传统BIO不同的点），选择器工作如下图： 本篇简单介绍了下NIO，接下来将会介绍具体如何去使用","link":"/2019/03/05/Java%20NIO%E5%AD%A6%E4%B9%A0%E4%B8%8E%E8%AE%B0%E5%BD%95%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9A%E5%88%9D%E8%AF%86NIO/"},{"title":"简单工厂模式&策略模式-简介与区别","text":"前言：两种模式的相似点与不同点不得不说，这两种模式真的很像。 相似点：都用到了面向对象的继承、多态、抽象，都拥有相似的结构。 不同点：工厂模式仅提供具体的实例对象，怎么使用这个对象是client的自由，策略模式client可以通过策略类来决定使用哪个实例的哪个方法。 一、两种模式的公共相同部分下面，我们假设有一台红白机，里面有一些游戏，每个游戏拥有play（玩）和uninstall（卸载）两个方法。 按照工厂和策略模式，我们抽象出来一个Game接口： 1234567public interface Game { void play(); void uninstall();} 代码块1 然后，我们假设游戏机里有魂斗罗、马戏团、默认的俄罗斯方块三款游戏，每个游戏有不同的玩法和卸载算法： 123456789101112131415161718192021222324252627282930313233343536373839404142// 魂斗罗，实现Gamepublic class Hundouluo implements Game { @Override public void play() { System.out.println(\"游戏：魂斗罗...playing\"); } @Override public void uninstall() { System.out.println(\"游戏：魂斗罗...卸载\"); }}// 马戏团，实现Gamepublic class Maxituan implements Game { @Override public void play() { System.out.println(\"游戏：马戏团...playing\"); } @Override public void uninstall() { System.out.println(\"游戏：马戏团...卸载\"); }}// 默认的俄罗斯方块，实现Gamepublic class Default implements Game { @Override public void play() { System.out.println(\"游戏：俄罗斯方块...playing\"); } @Override public void uninstall() { System.out.println(\"游戏：俄罗斯方块...卸载\"); }} 代码块2 ok，工厂模式和策略模式的相同部分就已经写好了，通过上面的代码，我们可以发现这两种模式都是需要把相同的部分抽象出来，通过多态来实例化不同的对象，调用其对应的实现。 二、两种模式的不同部分的实现2.1：工厂模式工厂需要一个工厂类，用来返回具体的实例对象用，代码如下： 1234567891011121314public class GameFactory { public static Game getGame(String name) { switch (name) { //根据传来的游戏名（这里偷懒用了首字母），来实例化具体的对象 case \"hdl\": return new Hundouluo(); case \"mxt\": return new Maxituan(); default: return new Default(); } }} 代码块3 2.2：策略模式策略模式需要策略类来封装具体的行为（方法），并且还可以指定使用哪个实例的哪个行为，代码如下： 123456789101112131415161718192021222324252627282930313233343536373839// 为了和工厂做充分的区分，这里定义了两个类型的context，分别维护一个行为算法（也就是方法函数，其次建立两个context是为了说明问题，实际使用时可能不需要这么多）// 用来维护play这个算法的实现public class PlayContext { private Game game; public PlayContext() { this.game = new Default(); } public PlayContext(Game game) { this.game = game; // 这里根据传入的具体实例赋值 } public void trigger() { this.game.play(); // 这里是对行为的封装，只提供play方法的触发 }}// 用来维护uninstall这个算法的实现public class UninstallContext { private Game game; public UninstallContext() { this.game = new Default(); } public UninstallContext(Game game) { this.game = game; // 这里根据传入的具体实例赋值 } public void trigger() { this.game.uninstall(); // 这里是对行为的封装，只提供uninstall方法的触发 }} 代码块4 测试代码： 1234new PlayContext(new Hundouluo()).trigger();new UninstallContext(new Hundouluo()).trigger();new PlayContext(new Maxituan()).trigger();new UninstallContext(new Maxituan()).trigger(); 代码块5 运行结果： 1234游戏：魂斗罗...playing游戏：魂斗罗...卸载游戏：马戏团...playing游戏：马戏团...卸载 通过上面的实验，和对比，会发现，工厂模式是简单的对实例的封装，而策略模式更在意的是对具体实例的具体行为（方法）的封装。 还有一种情况就是利用工厂模式的思想，实现的策略模式，我们现在来改造下上面的PlayContext源码： 1234567891011121314151617181920212223242526public class PlayContext { private Game game; public PlayContext() { this.game = new Default(); } public PlayContext(String name) { switch (name) { //根据传来的游戏名（这里偷懒用了首字母），来实例化具体的对象 case \"hdl\": this.game = new Hundouluo(); break; case \"mxt\": this.game = new Maxituan(); break; default: this.game = new Default(); } } public void trigger() { this.game.play(); // 这里是对行为的封装，只提供play方法的触发 }} 代码块6 测试类： 1234new PlayContext(\"hdl\").trigger();new UninstallContext(new Hundouluo()).trigger();new PlayContext(\"mxt\").trigger();new UninstallContext(new Maxituan()).trigger(); 代码块7 测试结果： 1234游戏：魂斗罗...playing游戏：魂斗罗...卸载游戏：马戏团...playing游戏：马戏团...卸载 三、总结策略模式是一种定义一系列算法的方法，所有这些算法完成的都是相同的工作，只是实现不同，它可以以相同的方式调用所有算法，减少了各种算法类与使用算法类之间的耦合。 工厂模式仅提供对应的实例，不对其方法做封装，减少了具体实现的实例与使用实例的业务方的耦合。 （↑描述待改进）","link":"/2019/02/27/%E7%AE%80%E5%8D%95%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F&%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F-%E7%AE%80%E4%BB%8B%E4%B8%8E%E5%8C%BA%E5%88%AB/"},{"title":"图解java多线程设计模式（一）","text":"去年看完的《图解java多线程设计模式》，可惜当时没做笔记，导致后来忘了许多东西，打算再温习下这本书，顺便在这里记录一下~ 一、顺序执行、并行、并发 顺序执行：多个操作按照顺序依次执行。 并行：多个任务同时进行，同一时间内可以执行多个任务，这种方式，叫做并行执行，比如多核处理器，多个核可以同时处理多个任务。 并发：多个任务通过切分时间段，来达到“同时进行”的效果，比如单核处理器，在“同时”处理多个任务时，就会不停的切换来执行不同的任务，不可能有同一时间执行不同任务的情况。 下面引用别人的一句话来说明下并行和并发： 并发是两个任务可以在重叠的时间段内启动，运行和完成。并行是任务在同一时间运行，例如，在多核处理器上，并发是独立执行过程的组合，而并行是同时执行（可能相关的）计算。 并发是一次处理很多事情，并行是同时做很多事情。 应用程序可以是并发的，但不是并行的，这意味着它可以同时处理多个任务，但是没有两个任务在同一时刻执行。应用程序可以是并行的，但不是并发的，这意味着它同时处理多核CPU中的任务的多个子任务。一个应用程序可以即不是并行的，也不是并发的，这意味着它一次一个地处理所有任务。应用程序可以即是并行的也是并发的，这意味着它同时在多核CPU中同时处理多个任务。 二、synchronized修饰符当我们说一个线程获得锁以后，则意味着这个线程可以执行当前对象（或类）里的synchronized方法，而且他线程则需要排队等待该线程释放锁以后才可能获得锁，进而执行锁里面的程序。 synchronized修饰后，存在对象锁和类锁两种类型。 2.1：对象锁123synchronized (this){ ...略} 代码块1 2.2：类锁123synchronized (XXX.class){ ...略} 代码块2 2.3：区别和作用域对象锁指的是当前线程获得了某个实例的锁，假如有个Word类，有A、B两个同步方法，C属于普通方法，如图所示： 可以发现，对象锁的作用域只针对当前对象生效，就像w1和w2里的A方法可以被不同的线程同时执行，但是同一个对象内的同步块，却只允许持有当前对象锁的线程执行，如t2、t3均被挡在了外面，当t1释放锁以后，t2、t3才会重新竞争锁，竞争到锁以后就会执行自己想要执行的同步逻辑。 类锁指的是当前线程获得了某个类的锁，还是Word类，有A、B两个static方法（静态方法属于类方法，加synchronized修饰符后等效于上面提到的synchronized(Word.class))，C属于普通static方法，如图所示： 跟上面相比较，这里的t5受到了t1的影响，因为t1获得了Word类的锁，w1和w2共属一个类，因此t1获得类锁以后，其他线程想要访问这个类里的同步块，就得等到t1释放锁以后才可以继续竞争锁然后执行自己想要执行的同步逻辑。 三、线程间的通信3.1：Wait这几个方法是属于每个实例对象的，所有实例都拥有一个“等待队列”（虚拟概念，实例里并不存在该字段），它是在实例的wait方法调用后存放停止操作线程的队列。执行wait方法后，线程进入当前实例的“等待队列”，以下几种情况可以让线程退出“等待队列”： 其他线程调用notify、notifyAll方法来将其唤醒 其他线程调用interrupt来将其唤醒 wait方法本身超时 当执行了下面的代码： 1obj.wait(); 代码块3 我们可以说当前线程在obj上发生了等待，当前线程进入了obj的“等待队列”，此时当前线程会让出锁，让其他线程继续竞争获得该实例的锁（因此这里有个规则，调用wait的线程必须持有当前实例对象的锁） 过程如下图： 3.2：notify现在先来介绍下notify，该方法会将等待队列里的线程取出，让其退出等待并参与锁竞争然后继续执行上次wait后没有执行完的语句。整体过程如下图所示： 可以看到，t1在被挂起后，会因为t2调用了同实例的notify方法，而让t1被从等待队列里释放，重新加入到所得竞争力，t2执行完毕后释放锁，锁又再次被t1竞争到，t1将继续执行上次被挂起时后面未执行完的语句。 需要指出的是，如果等待队列里的线程是多个，那么被唤醒的那一个，将会是等待队列里所有线程随机的一个，不会特定哪一个线程会被唤起。 3.3：notifyAll接下来介绍notifyAll方法，顾名思义，就是将等待队列里的线程全部唤起，然后这些线程将全部加入到锁竞争，竞争到，继续完成上次被挂起时未执行完毕的操作，流程图如下： 说明，当线程调用实例的wait、notify、notifyAll方法有个大前提，就是必须要求该线程拥有该实例的锁，否则会抛IllegalMonitorStateException异常。 在编写程序时，是该选择notify还是选择notifyAll？这个可以指出的是，notifyAll往往更加健壮，而notify由于唤起的线程少，因此效率会更高，但是存在程序停止的风险。 附上使用wait、notify进行线程通信的例子： 利用ReentrantLock简单实现一个阻塞队列 java设计模式：简单实现生产者和消费者模式","link":"/2019/02/26/%E5%9B%BE%E8%A7%A3java%E5%A4%9A%E7%BA%BF%E7%A8%8B%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%EF%BC%88%E4%B8%80%EF%BC%89/"},{"title":"ThreadLocal系列（三）-TransmittableThreadLocal的使用及原理解析","text":"一、基本使用首先，TTL是用来解决ITL解决不了的问题而诞生的，所以TTL一定是支持父线程的本地变量传递给子线程这种基本操作的，ITL也可以做到，但是前面有讲过，ITL在线程池的模式下，就没办法再正确传递了，所以TTL做出的改进就是即便是在线程池模式下，也可以很好的将父线程本地变量传递下去，先来看个例子： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103// 需要注意的是，使用TTL的时候，要想传递的值不出问题，线程池必须得用TTL加一层代理（下面会讲这样做的目的） private static ExecutorService executorService = TtlExecutors.getTtlExecutorService(Executors.newFixedThreadPool(2)); private static ThreadLocal tl = new TransmittableThreadLocal&lt;&gt;(); //这里采用TTL的实现 public static void main(String[] args) { new Thread(() -&gt; { String mainThreadName = \"main_01\"; tl.set(1); executorService.execute(() -&gt; { sleep(1L); System.out.println(String.format(\"本地变量改变之前(1), 父线程名称-%s, 子线程名称-%s, 变量值=%s\", mainThreadName, Thread.currentThread().getName(), tl.get())); }); executorService.execute(() -&gt; { sleep(1L); System.out.println(String.format(\"本地变量改变之前(1), 父线程名称-%s, 子线程名称-%s, 变量值=%s\", mainThreadName, Thread.currentThread().getName(), tl.get())); }); executorService.execute(() -&gt; { sleep(1L); System.out.println(String.format(\"本地变量改变之前(1), 父线程名称-%s, 子线程名称-%s, 变量值=%s\", mainThreadName, Thread.currentThread().getName(), tl.get())); }); sleep(1L); //确保上面的会在tl.set执行之前执行 tl.set(2); // 等上面的线程池第一次启用完了，父线程再给自己赋值 executorService.execute(() -&gt; { sleep(1L); System.out.println(String.format(\"本地变量改变之后(2), 父线程名称-%s, 子线程名称-%s, 变量值=%s\", mainThreadName, Thread.currentThread().getName(), tl.get())); }); executorService.execute(() -&gt; { sleep(1L); System.out.println(String.format(\"本地变量改变之后(2), 父线程名称-%s, 子线程名称-%s, 变量值=%s\", mainThreadName, Thread.currentThread().getName(), tl.get())); }); executorService.execute(() -&gt; { sleep(1L); System.out.println(String.format(\"本地变量改变之后(2), 父线程名称-%s, 子线程名称-%s, 变量值=%s\", mainThreadName, Thread.currentThread().getName(), tl.get())); }); System.out.println(String.format(\"线程名称-%s, 变量值=%s\", Thread.currentThread().getName(), tl.get())); }).start(); new Thread(() -&gt; { String mainThreadName = \"main_02\"; tl.set(3); executorService.execute(() -&gt; { sleep(1L); System.out.println(String.format(\"本地变量改变之前(3), 父线程名称-%s, 子线程名称-%s, 变量值=%s\", mainThreadName, Thread.currentThread().getName(), tl.get())); }); executorService.execute(() -&gt; { sleep(1L); System.out.println(String.format(\"本地变量改变之前(3), 父线程名称-%s, 子线程名称-%s, 变量值=%s\", mainThreadName, Thread.currentThread().getName(), tl.get())); }); executorService.execute(() -&gt; { sleep(1L); System.out.println(String.format(\"本地变量改变之前(3), 父线程名称-%s, 子线程名称-%s, 变量值=%s\", mainThreadName, Thread.currentThread().getName(), tl.get())); }); sleep(1L); //确保上面的会在tl.set执行之前执行 tl.set(4); // 等上面的线程池第一次启用完了，父线程再给自己赋值 executorService.execute(() -&gt; { sleep(1L); System.out.println(String.format(\"本地变量改变之后(4), 父线程名称-%s, 子线程名称-%s, 变量值=%s\", mainThreadName, Thread.currentThread().getName(), tl.get())); }); executorService.execute(() -&gt; { sleep(1L); System.out.println(String.format(\"本地变量改变之后(4), 父线程名称-%s, 子线程名称-%s, 变量值=%s\", mainThreadName, Thread.currentThread().getName(), tl.get())); }); executorService.execute(() -&gt; { sleep(1L); System.out.println(String.format(\"本地变量改变之后(4), 父线程名称-%s, 子线程名称-%s, 变量值=%s\", mainThreadName, Thread.currentThread().getName(), tl.get())); }); System.out.println(String.format(\"线程名称-%s, 变量值=%s\", Thread.currentThread().getName(), tl.get())); }).start(); } private static void sleep(long time) { try { Thread.sleep(time); } catch (InterruptedException e) { e.printStackTrace(); } } 代码块1 运行结果： 1234567891011121314线程名称-Thread-2, 变量值=4本地变量改变之前(3), 父线程名称-main_02, 子线程名称-pool-1-thread-1, 变量值=3线程名称-Thread-1, 变量值=2本地变量改变之前(1), 父线程名称-main_01, 子线程名称-pool-1-thread-2, 变量值=1本地变量改变之前(1), 父线程名称-main_01, 子线程名称-pool-1-thread-1, 变量值=1本地变量改变之前(3), 父线程名称-main_02, 子线程名称-pool-1-thread-2, 变量值=3本地变量改变之前(3), 父线程名称-main_02, 子线程名称-pool-1-thread-2, 变量值=3本地变量改变之前(1), 父线程名称-main_01, 子线程名称-pool-1-thread-1, 变量值=1本地变量改变之后(2), 父线程名称-main_01, 子线程名称-pool-1-thread-2, 变量值=2本地变量改变之后(4), 父线程名称-main_02, 子线程名称-pool-1-thread-1, 变量值=4本地变量改变之后(4), 父线程名称-main_02, 子线程名称-pool-1-thread-1, 变量值=4本地变量改变之后(4), 父线程名称-main_02, 子线程名称-pool-1-thread-2, 变量值=4本地变量改变之后(2), 父线程名称-main_01, 子线程名称-pool-1-thread-1, 变量值=2本地变量改变之后(2), 父线程名称-main_01, 子线程名称-pool-1-thread-2, 变量值=2 程序有些啰嗦，为了说明问题，加了很多说明，但至少通过上面的例子，不难发现，两个主线程里都使用线程池异步，而且值在主线程里还发生过改变，测试结果展示一切正常，由此可以知道TTL在使用线程池的情况下，也可以很好的完成传递，而且不会发生错乱。 那么是不是对普通线程异步也有这么好的支撑呢？ 改造下上面的测试代码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192private static ThreadLocal tl = new TransmittableThreadLocal&lt;&gt;(); public static void main(String[] args) { new Thread(() -&gt; { String mainThreadName = \"main_01\"; tl.set(1); new Thread(() -&gt; { sleep(1L); System.out.println(String.format(\"本地变量改变之前(1), 父线程名称-%s, 子线程名称-%s, 变量值=%s\", mainThreadName, Thread.currentThread().getName(), tl.get())); }).start(); new Thread(() -&gt; { sleep(1L); System.out.println(String.format(\"本地变量改变之前(1), 父线程名称-%s, 子线程名称-%s, 变量值=%s\", mainThreadName, Thread.currentThread().getName(), tl.get())); }).start(); new Thread(() -&gt; { sleep(1L); System.out.println(String.format(\"本地变量改变之前(1), 父线程名称-%s, 子线程名称-%s, 变量值=%s\", mainThreadName, Thread.currentThread().getName(), tl.get())); }).start(); sleep(1L); //确保上面的会在tl.set执行之前执行 tl.set(2); // 等上面的线程池第一次启用完了，父线程再给自己赋值 new Thread(() -&gt; { sleep(1L); System.out.println(String.format(\"本地变量改变之后(2), 父线程名称-%s, 子线程名称-%s, 变量值=%s\", mainThreadName, Thread.currentThread().getName(), tl.get())); }).start(); new Thread(() -&gt; { sleep(1L); System.out.println(String.format(\"本地变量改变之后(2), 父线程名称-%s, 子线程名称-%s, 变量值=%s\", mainThreadName, Thread.currentThread().getName(), tl.get())); }).start(); new Thread(() -&gt; { sleep(1L); System.out.println(String.format(\"本地变量改变之后(2), 父线程名称-%s, 子线程名称-%s, 变量值=%s\", mainThreadName, Thread.currentThread().getName(), tl.get())); }).start(); System.out.println(String.format(\"线程名称-%s, 变量值=%s\", Thread.currentThread().getName(), tl.get())); }).start(); new Thread(() -&gt; { String mainThreadName = \"main_02\"; tl.set(3); new Thread(() -&gt; { sleep(1L); System.out.println(String.format(\"本地变量改变之前(3), 父线程名称-%s, 子线程名称-%s, 变量值=%s\", mainThreadName, Thread.currentThread().getName(), tl.get())); }).start(); new Thread(() -&gt; { sleep(1L); System.out.println(String.format(\"本地变量改变之前(3), 父线程名称-%s, 子线程名称-%s, 变量值=%s\", mainThreadName, Thread.currentThread().getName(), tl.get())); }).start(); new Thread(() -&gt; { sleep(1L); System.out.println(String.format(\"本地变量改变之前(3), 父线程名称-%s, 子线程名称-%s, 变量值=%s\", mainThreadName, Thread.currentThread().getName(), tl.get())); }).start(); sleep(1L); //确保上面的会在tl.set执行之前执行 tl.set(4); // 等上面的线程池第一次启用完了，父线程再给自己赋值 new Thread(() -&gt; { sleep(1L); System.out.println(String.format(\"本地变量改变之后(4), 父线程名称-%s, 子线程名称-%s, 变量值=%s\", mainThreadName, Thread.currentThread().getName(), tl.get())); }).start(); new Thread(() -&gt; { sleep(1L); System.out.println(String.format(\"本地变量改变之后(4), 父线程名称-%s, 子线程名称-%s, 变量值=%s\", mainThreadName, Thread.currentThread().getName(), tl.get())); }).start(); new Thread(() -&gt; { sleep(1L); System.out.println(String.format(\"本地变量改变之后(4), 父线程名称-%s, 子线程名称-%s, 变量值=%s\", mainThreadName, Thread.currentThread().getName(), tl.get())); }).start(); System.out.println(String.format(\"线程名称-%s, 变量值=%s\", Thread.currentThread().getName(), tl.get())); }).start(); } 代码块2 相比代码块1，这一段的异步全都是普通异步，未采用线程池的方式进行异步，看下运行结果： 1234567891011121314本地变量改变之后(4), 父线程名称-main_02, 子线程名称-Thread-14, 变量值=4本地变量改变之前(1), 父线程名称-main_01, 子线程名称-Thread-5, 变量值=1线程名称-Thread-1, 变量值=2本地变量改变之前(1), 父线程名称-main_01, 子线程名称-Thread-3, 变量值=1本地变量改变之后(2), 父线程名称-main_01, 子线程名称-Thread-11, 变量值=2本地变量改变之前(3), 父线程名称-main_02, 子线程名称-Thread-6, 变量值=3本地变量改变之后(4), 父线程名称-main_02, 子线程名称-Thread-12, 变量值=4本地变量改变之后(4), 父线程名称-main_02, 子线程名称-Thread-10, 变量值=4本地变量改变之前(3), 父线程名称-main_02, 子线程名称-Thread-8, 变量值=3本地变量改变之前(3), 父线程名称-main_02, 子线程名称-Thread-4, 变量值=3本地变量改变之前(1), 父线程名称-main_01, 子线程名称-Thread-7, 变量值=1线程名称-Thread-2, 变量值=4本地变量改变之后(2), 父线程名称-main_01, 子线程名称-Thread-9, 变量值=2本地变量改变之后(2), 父线程名称-main_01, 子线程名称-Thread-13, 变量值=2 ok，可以看到，达到了跟第一个测试一致的结果。 到这里，通过上述两个例子，TTL的基本使用，以及其解决的问题，我们已经有了初步的了解，下面我们来解析一下其内部原理，看看TTL是怎么完成对ITL的优化的。 二、原理分析先来看TTL里面的几个重要属性及方法 TTL定义： 1public class TransmittableThreadLocal extends InheritableThreadLocal 代码块3 可以看到，TTL继承了ITL，意味着TTL首先具备ITL的功能。 再来看看一个重要属性holder： 12345678910111213141516/** * 这是一个ITL类型的对象，持有一个全局的WeakMap（weakMap的key是弱引用，同TL一样，也是为了解决内存泄漏的问题），里面存放了TTL对象 * 并且重写了initialValue和childValue方法，尤其是childValue，可以看到在即将异步时父线程的属性是直接作为初始化值赋值给子线程的本地变量对象（TTL）的 */ private static InheritableThreadLocal&lt;Map&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt;&gt; holder = new InheritableThreadLocal&lt;Map&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt;&gt;() { @Override protected Map&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt; initialValue() { return new WeakHashMap&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt;(); } @Override protected Map&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt; childValue(Map&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt; parentValue) { return new WeakHashMap&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt;(parentValue); } }; 代码块4 再来看下set和get： 123456789101112131415161718192021222324//下面的方法均属于TTL类@Override public final void set(T value) { super.set(value); if (null == value) removeValue(); else addValue(); } @Override public final T get() { T value = super.get(); if (null != value) addValue(); return value; } private void removeValue() { holder.get().remove(this); //从holder持有的map对象中移除 } private void addValue() { if (!holder.get().containsKey(this)) { holder.get().put(this, null); //从holder持有的map对象中添加 } } 代码块5 TTL里先了解上述的几个方法及对象，可以看出，单纯的使用TTL是达不到支持线程池本地变量的传递的，通过第一部分的例子，可以发现，除了要启用TTL，还需要通过TtlExecutors.getTtlExecutorService包装一下线程池才可以，那么，下面就来看看在程序即将通过线程池异步的时候，TTL帮我们做了哪些操作（这一部分是TTL支持线程池传递的核心部分）： 首先打开包装类，看下execute方法在执行时做了些什么。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849// 此方法属于线程池包装类ExecutorTtlWrapper@Override public void execute(@Nonnull Runnable command) { executor.execute(TtlRunnable.get(command)); //这里会把Rannable包装一层，这是关键，有些逻辑处理，需要在run之前执行 } // 对应上面的get方法，返回一个TtlRunnable对象，属于TtLRannable包装类 @Nullable public static TtlRunnable get(@Nullable Runnable runnable) { return get(runnable, false, false); } // 对应上面的get方法 @Nullable public static TtlRunnable get(@Nullable Runnable runnable, boolean releaseTtlValueReferenceAfterRun, boolean idempotent) { if (null == runnable) return null; if (runnable instanceof TtlEnhanced) { // 若发现已经是目标类型了（说明已经被包装过了）直接返回 // avoid redundant decoration, and ensure idempotency if (idempotent) return (TtlRunnable) runnable; else throw new IllegalStateException(\"Already TtlRunnable!\"); } return new TtlRunnable(runnable, releaseTtlValueReferenceAfterRun); //最终初始化 } // 对应上面的TtlRunnable方法 private TtlRunnable(@Nonnull Runnable runnable, boolean releaseTtlValueReferenceAfterRun) { this.capturedRef = new AtomicReference&lt;Object&gt;(capture()); //这里将捕获后的父线程本地变量存储在当前对象的capturedRef里 this.runnable = runnable; this.releaseTtlValueReferenceAfterRun = releaseTtlValueReferenceAfterRun; } // 对应上面的capture方法，用于捕获当前线程（父线程）里的本地变量，此方法属于TTL的静态内部类Transmitter @Nonnull public static Object capture() { Map&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt; captured = new HashMap&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt;(); for (TransmittableThreadLocal&lt;?&gt; threadLocal : holder.get().keySet()) { // holder里目前存放的k-v里的key，就是需要传给子线程的TTL对象 captured.put(threadLocal, threadLocal.copyValue()); } return captured; // 这里返回的这个对象，就是当前将要使用线程池异步出来的子线程，所继承的本地变量合集 } // 对应上面的copyValue，简单的将TTL对象里的值返回（结合之前的源码可以知道get方法其实就是获取当前线程（父线程）里的值，调用super.get方法） private T copyValue() { return copy(get()); } protected T copy(T parentValue) { return parentValue; } 代码块6 结合上述代码，大致知道了在线程池异步之前需要做的事情，其实就是把当前父线程里的本地变量取出来，然后赋值给Rannable包装类里的capturedRef属性，到此为止，下面会发生什么，我们大致上可以猜出来了，接下来大概率会在run方法里，将这些捕获到的值赋给子线程的holder赋对应的TTL值，那么我们继续往下看Rannable包装类里的run方法是怎么实现的： 1234567891011121314151617181920//run方法属于Rannable的包装类TtlRunnable@Override public void run() { Object captured = capturedRef.get(); // 获取由之前捕获到的父线程变量集 if (captured == null || releaseTtlValueReferenceAfterRun &amp;&amp; !capturedRef.compareAndSet(captured, null)) { throw new IllegalStateException(\"TTL value reference is released after run!\"); } /** * 重点方法replay，此方法用来给当前子线程赋本地变量，返回的backup是此子线程原来就有的本地变量值（原生本地变量，下面会详细讲）， * backup用于恢复数据（如果任务执行完毕，意味着该子线程会归还线程池，那么需要将其原生本地变量属性恢复） */ Object backup = replay(captured); try { runnable.run(); // 执行异步逻辑 } finally { restore(backup); // 结合上面对于replay的解释，不难理解，这个方法就是用来恢复原有值的 } } 代码块7 根据上述代码，我们看到了TTL在异步任务执行前，会先进行赋值操作（就是拿着异步发生时捕获到的父线程的本地变量，赋给自己），当任务执行完，就恢复原生的自己本身的线程变量值。 下面来具体看这俩方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677//下面的方法均属于TTL的静态内部类Transmittable@Nonnull public static Object replay(@Nonnull Object captured) { @SuppressWarnings(\"unchecked\") Map&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt; capturedMap = (Map&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt;) captured; //使用此线程异步时捕获到的父线程里的本地变量值 Map&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt; backup = new HashMap&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt;(); //当前线程原生的本地变量，用于使用完线程后恢复用 //注意：这里循环的是当前子线程原生的本地变量集合，与本方法相反，restore方法里循环这个holder是指：该线程运行期间产生的变量+父线程继承来的变量 for (Iterator&lt;? extends Map.Entry&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt;&gt; iterator = holder.get().entrySet().iterator(); iterator.hasNext(); ) { Map.Entry&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt; next = iterator.next(); TransmittableThreadLocal&lt;?&gt; threadLocal = next.getKey(); backup.put(threadLocal, threadLocal.get()); // 所有原生的本地变量都暂时存储在backup里，用于之后恢复用 /** * 检查，如果捕获到的线程变量里，不包含当前原生变量值，则从当前原生变量里清除掉，对应的线程本地变量也清掉 * 这就是为什么会将原生变量保存在backup里的原因，为了恢复原生值使用 * 那么，为什么这里要清除掉呢？因为从使用这个子线程做异步那里，捕获到的本地变量并不包含原生的变量，当前线程 * 在做任务时的首要目标，是将父线程里的变量完全传递给任务，如果不清除这个子线程原生的本地变量， * 意味着很可能会影响到任务里取值的准确性。 * * 打个比方，有ttl对象tl，这个tl在线程池的某个子线程里存在对应的值2，当某个主线程使用该子线程做异步任务时 * tl这个对象在当前主线程里没有值，那么如果不进行下面这一步的操作，那么在使用该子线程做的任务里就可以通过 * 该tl对象取到值2，不符合预期 */ if (!capturedMap.containsKey(threadLocal)) { iterator.remove(); threadLocal.superRemove(); } } // 这一步就是直接把父线程本地变量赋值给当前线程了（这一步起就刷新了holder里的值了，具体往下看该方法，在异步线程运行期间，还可能产生别的本地变量，比如在真正的run方法内的业务代码，再用一个tl对象设置一个值） setTtlValuesTo(capturedMap); // 这个方法属于扩展方法，ttl本身支持重写异步任务执行前后的操作，这里不再具体赘述 doExecuteCallback(true); return backup; } // 结合之前Rannable包装类的run方法来看，这个方法就是使用上面replay记录下的原生线程变量做恢复用的 public static void restore(@Nonnull Object backup) { @SuppressWarnings(\"unchecked\") Map&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt; backupMap = (Map&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt;) backup; // call afterExecute callback doExecuteCallback(false); // 注意，这里的holder取出来的，实际上是replay方法设置进去的关于父线程里的所有变量（结合上面来看，就是：该线程运行期间产生的变量+父线程继承来的变量） for (Iterator&lt;? extends Map.Entry&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt;&gt; iterator = holder.get().entrySet().iterator(); iterator.hasNext(); ) { Map.Entry&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt; next = iterator.next(); TransmittableThreadLocal&lt;?&gt; threadLocal = next.getKey(); /** * 同样的，如果子线程原生变量不包含某个父线程传来的对象，那么就删除，可以思考下，这里的清除跟上面replay里的有什么不同？ * 这里会把不属于原生变量的对象给删除掉（这里被删除掉的可能是父线程继承下来的，也可能是异步任务在执行时产生的新值） */ if (!backupMap.containsKey(threadLocal)) { iterator.remove(); threadLocal.superRemove(); } } // 同样调用这个方法，进行值的恢复 setTtlValuesTo(backupMap); } // 真正给当前子线程赋值的方法，对应上面的setTtlValuesTo方法 private static void setTtlValuesTo(@Nonnull Map&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt; ttlValues) { for (Map.Entry&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt; entry : ttlValues.entrySet()) { @SuppressWarnings(\"unchecked\") TransmittableThreadLocal&lt;Object&gt; threadLocal = (TransmittableThreadLocal&lt;Object&gt;) entry.getKey(); threadLocal.set(entry.getValue()); //赋值，注意，从这里开始，子线程的holder里的值会被重新赋值刷新，可以参照上面ttl的set方法的实现 } } 代码块8 ok，到这里基本上把TTL比较核心的代码看完了，下面整理下整个流程，这是官方给出的时序图： 上图第一行指的是类名称，下面的流程指的是类所做的事情，根据上面罗列出来的源码，结合这个时序图，可以比较直观一些的理解整个流程。 三、TTL中线程池子线程原生变量的产生这一节是为了验证上面replay和restore，现在通过一个例子来验证下，先把源码down下来，在源码的restore和replay上分别加上输出语句，遍历holder： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657//replay前后打印holder里面的值public static Object replay(@Nonnull Object captured) { @SuppressWarnings(\"unchecked\") Map&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt; capturedMap = (Map&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt;) captured; Map&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt; backup = new HashMap&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt;(); System.out.println(\"--------------------replay前置，当前拿到的holder里的TTL列表\"); for (Iterator&lt;? extends Map.Entry&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt;&gt; iterator = holder.get().entrySet().iterator(); iterator.hasNext(); ) { Map.Entry&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt; next = iterator.next(); TransmittableThreadLocal&lt;?&gt; threadLocal = next.getKey(); System.out.println(String.format(\"replay前置里拿到原生的ttl_k=%s, ttl_value=%s\", threadLocal.hashCode(), threadLocal.get())); } for...//代码省略，具体看上面 setTtlValuesTo(capturedMap); doExecuteCallback(true); System.out.println(\"--------------------reply后置，当前拿到的holder里的TTL列表\"); for (Iterator&lt;? extends Map.Entry&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt;&gt; iterator = holder.get().entrySet().iterator(); iterator.hasNext(); ) { Map.Entry&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt; next = iterator.next(); TransmittableThreadLocal&lt;?&gt; threadLocal = next.getKey(); System.out.println(String.format(\"replay后置里拿到原生的ttl_k=%s, ttl_value=%s\", threadLocal.hashCode(), threadLocal.get())); } return backup; }//restore前后打印holder里面的值public static void restore(@Nonnull Object backup) { @SuppressWarnings(\"unchecked\") Map&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt; backupMap = (Map&lt;TransmittableThreadLocal&lt;?&gt;, Object&gt;) backup; // call afterExecute callback doExecuteCallback(false); System.out.println(\"--------------------restore前置，当前拿到的holder里的TTL列表\"); for (Iterator&lt;? extends Map.Entry&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt;&gt; iterator = holder.get().entrySet().iterator(); iterator.hasNext(); ) { Map.Entry&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt; next = iterator.next(); TransmittableThreadLocal&lt;?&gt; threadLocal = next.getKey(); System.out.println(String.format(\"restore前置里拿到当前线程内变量，ttl_k=%s, ttl_value=%s\", threadLocal.hashCode(), threadLocal.get())); } for...//省略代码，具体具体看上面 setTtlValuesTo(backupMap); System.out.println(\"--------------------restore后置，当前拿到的holder里的TTL列表\"); for (Iterator&lt;? extends Map.Entry&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt;&gt; iterator = holder.get().entrySet().iterator(); iterator.hasNext(); ) { Map.Entry&lt;TransmittableThreadLocal&lt;?&gt;, ?&gt; next = iterator.next(); TransmittableThreadLocal&lt;?&gt; threadLocal = next.getKey(); System.out.println(String.format(\"restore后置里拿到当前线程内变量，ttl_k=%s, ttl_value=%s\", threadLocal.hashCode(), threadLocal.get())); } } 代码块9 代码这样做的目的，就是要说明线程池所谓的原生本地变量是怎么产生的，以及replay和restore是怎么设置和恢复的，下面来看个简单的例子： 123456789101112131415161718192021private static ExecutorService executorService = TtlExecutors.getTtlExecutorService(Executors.newFixedThreadPool(1)); private static ThreadLocal tl = new TransmittableThreadLocal(); private static ThreadLocal tl2 = new TransmittableThreadLocal(); public static void main(String[] args) throws InterruptedException { tl.set(1); tl2.set(2); executorService.execute(new Runnable() { @Override public void run() { try { Thread.sleep(1000L); } catch (InterruptedException e) { e.printStackTrace(); } } }); } 代码块10 运行结果如下： 123456789101112--------------------replay前置，当前拿到的holder里的TTL列表replay前置里拿到原生的ttl_k=1259475182, ttl_value=2replay前置里拿到原生的ttl_k=929338653, ttl_value=1--------------------reply后置，当前拿到的holder里的TTL列表replay后置里拿到原生的ttl_k=1259475182, ttl_value=2replay后置里拿到原生的ttl_k=929338653, ttl_value=1--------------------restore前置，当前拿到的holder里的TTL列表restore前置里拿到当前线程内变量，ttl_k=1259475182, ttl_value=2restore前置里拿到当前线程内变量，ttl_k=929338653, ttl_value=1--------------------restore后置，当前拿到的holder里的TTL列表restore后置里拿到当前线程内变量，ttl_k=1259475182, ttl_value=2restore后置里拿到当前线程内变量，ttl_k=929338653, ttl_value=1 我们会发现，原生值产生了，从异步开始，就确定了线程池里的线程具备了1和2的值，那么，再来改动下上面的测试代码： 12345678910111213141516171819202122232425262728293031public static void main(String[] args) throws InterruptedException { tl.set(1); executorService.execute(new Runnable() { @Override public void run() { try { Thread.sleep(100L); } catch (InterruptedException e) { e.printStackTrace(); } } }); Thread.sleep(1000L); tl2.set(2);//较第一次换下位置，换到第一次使用线程池后执行（这意味着下面这次异步不会再触发Thread的init方法了） System.out.println(\"---------------------------------------------------------------------------------\"); executorService.execute(new Runnable() { @Override public void run() { try { Thread.sleep(1000L); } catch (InterruptedException e) { e.printStackTrace(); } } }); } 代码块11 运行结果为： 12345678910111213141516171819--------------------replay前置，当前拿到的holder里的TTL列表replay前置里拿到原生的ttl_k=929338653, ttl_value=1--------------------reply后置，当前拿到的holder里的TTL列表replay后置里拿到原生的ttl_k=929338653, ttl_value=1--------------------restore前置，当前拿到的holder里的TTL列表restore前置里拿到当前线程内变量，ttl_k=929338653, ttl_value=1--------------------restore后置，当前拿到的holder里的TTL列表restore后置里拿到当前线程内变量，ttl_k=929338653, ttl_value=1-----------------------------------------------------------------------------------------------------replay前置，当前拿到的holder里的TTL列表replay前置里拿到原生的ttl_k=929338653, ttl_value=1--------------------reply后置，当前拿到的holder里的TTL列表replay后置里拿到原生的ttl_k=1020371697, ttl_value=2replay后置里拿到原生的ttl_k=929338653, ttl_value=1--------------------restore前置，当前拿到的holder里的TTL列表restore前置里拿到当前线程内变量，ttl_k=1020371697, ttl_value=2restore前置里拿到当前线程内变量，ttl_k=929338653, ttl_value=1--------------------restore后置，当前拿到的holder里的TTL列表restore后置里拿到当前线程内变量，ttl_k=929338653, ttl_value=1 可以发现，第一次异步时，只有一个值被传递了下去，然后第二次异步，新加了一个tl2的值，但是看第二次异步的打印，会发现，restore恢复后，仍然是第一次异步发生时放进去的那个tl的值。 通过上面的例子，基本可以确认，所谓线程池内线程的本地原生变量，其实是第一次使用线程时被传递进去的值，我们之前有说过TTL是继承至ITL的，之前的文章也说过，线程池第一次启用时是会触发Thread的init方法的，也就是说，在第一次异步时拿到的主线程的变量会被传递给子线程，作为子线程的原生本地变量保存起来，后续是replay操作和restore操作也是围绕着这个原生变量（即原生holder里的值）来进行设置、恢复的，设置的是当前父线程捕获到的本地变量，恢复的是子线程原生本地变量。 holder里持有的可以理解就是当前线程内的所有本地变量，当子线程将异步任务执行完毕后，会执行restore进行恢复原生本地变量，具体参照上面的代码和测试代码。 四、总结到这里基本上确认了TTL是如何进行线程池传值的，以及被包装的run方法执行异步任务之前，会使用replay进行设置父线程里的本地变量给当前子线程，任务执行完毕，会调用restore恢复该子线程原生的本地变量（目前原生本地变量的产生，就只碰到上述测试代码中的这一种情况，即线程第一次使用时通过ITL属性以及Thread的init方法传给子线程，还不太清楚有没有其他方式设置）。 其实，正常程序里想要完成线程池上下文传递，使用TL就足够了，我们可以效仿TTL包装线程池对象的原理，进行值传递，异步任务结束后，再remove，以此类推来完成线程池值传递，不过这种方式过于单纯，且要求上下文为只读对象，否则子线程存在写操作，就会发生上下文污染。 TTL项目地址（可以详细了解下它的其他特性和用法）：https://github.com/alibaba/transmittable-thread-local","link":"/2019/02/20/ThreadLocal%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89-TransmittableThreadLocal%E7%9A%84%E4%BD%BF%E7%94%A8%E5%8F%8A%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/"},{"title":"ThreadLocal系列（二）-InheritableThreadLocal的使用及原理解析","text":"一、基本使用我们继续来看之前写的例子： 123456789101112131415161718private static ThreadLocal tl = new ThreadLocal&lt;&gt;();public static void main(String[] args) throws Exception { tl.set(1); System.out.println(String.format(\"当前线程名称: %s, main方法内获取线程内数据为: %s\", Thread.currentThread().getName(), tl.get())); fc(); new Thread(() -&gt; { fc(); }).start(); Thread.sleep(1000L); //保证下面fc执行一定在上面异步代码之后执行 fc(); //继续在主线程内执行，验证上面那一步是否对主线程上下文内容造成影响 } private static void fc() { System.out.println(String.format(\"当前线程名称: %s, fc方法内获取线程内数据为: %s\", Thread.currentThread().getName(), tl.get())); } 代码块1 输出为： 1234当前线程名称: main, main方法内获取线程内数据为: 1当前线程名称: main, fc方法内获取线程内数据为: 1当前线程名称: Thread-0, fc方法内获取线程内数据为: null当前线程名称: main, fc方法内获取线程内数据为: 1 我们会发现，父线程的本地变量是无法传递给子线程的，这当然是正常的，因为线程本地变量来就不应该相互有交集，但是有些时候，我们的确是需要子线程里仍然可以获取到父线程里的本地变量，现在就需要借助TL的一个子类：InheritableThreadLocal（下面简称ITL），来完成上述要求 现在我们将例子里的 1private static ThreadLocal tl = new ThreadLocal&lt;&gt;(); 代码块2 改为： 1private static ThreadLocal tl = new InheritableThreadLocal&lt;&gt;(); 代码块3 然后我们再来运行下结果： 1234当前线程名称: main, main方法内获取线程内数据为: 1当前线程名称: main, fc方法内获取线程内数据为: 1当前线程名称: Thread-0, fc方法内获取线程内数据为: 1当前线程名称: main, fc方法内获取线程内数据为: 1 可以发现，子线程里已经可以获得父线程里的本地变量了。 结合之前讲的TL的实现，简单理解起来并不难，基本可以认定，是在创建子线程的时候，父线程的ThreadLocalMap（下面简称TLMap）里的值递给了子线程，子线程针对上述tl对象持有的k-v进行了copy，其实这里不是真正意义上对象copy，只是给v的值多了一条子线程TLMap的引用而已，v的值在父子线程里指向的均是同一个对象，因此任意线程改了这个值，对其他线程是可见的，为了验证这一点，我们可以改造以上测试代码： 12345678910111213141516171819202122232425private static ThreadLocal tl = new InheritableThreadLocal&lt;&gt;(); private static ThreadLocal tl2 = new InheritableThreadLocal&lt;&gt;(); public static void main(String[] args) throws Exception { tl.set(1); Hello hello = new Hello(); hello.setName(\"init\"); tl2.set(hello); System.out.println(String.format(\"当前线程名称: %s, main方法内获取线程内数据为: tl = %s，tl2.name = %s\", Thread.currentThread().getName(), tl.get(), tl2.get().getName())); fc(); new Thread(() -&gt; { Hello hello1 = tl2.get(); hello1.setName(\"init2\"); fc(); }).start(); Thread.sleep(1000L); //保证下面fc执行一定在上面异步代码之后执行 fc(); //继续在主线程内执行，验证上面那一步是否对主线程上下文内容造成影响 } private static void fc() { System.out.println(String.format(\"当前线程名称: %s, fc方法内获取线程内数据为: tl = %s，tl2.name = %s\", Thread.currentThread().getName(), tl.get(), tl2.get().getName())); } 代码块4 输出结果为： 1234当前线程名称: main, main方法内获取线程内数据为: tl = 1，tl2.name = init当前线程名称: main, fc方法内获取线程内数据为: tl = 1，tl2.name = init当前线程名称: Thread-0, fc方法内获取线程内数据为: tl = 1，tl2.name = init2当前线程名称: main, fc方法内获取线程内数据为: tl = 1，tl2.name = init2 可以确认，子线程里持有的本地变量跟父线程里那个是同一个对象。 二、原理分析通过上述的测试代码，基本可以确定父线程的TLMap被传递到了下一级，那么我们基本可以确认ITL是TL派生出来专门解决线程本地变量父传子问题的，那么下面通过源码来分析一下ITL到底是怎么完成这个操作的。 先来了解下Thread类，上节说到，其实最终线程本地变量是通过TLMap存储在Thread对象内的，那么来看下Thread对象内关于TLMap的两个属性： 12ThreadLocal.ThreadLocalMap threadLocals = null;ThreadLocal.ThreadLocalMap inheritableThreadLocals = null; 代码块5 Thread类里其实有两个TLMap属性，第一个就是普通TL对象为其赋值，第二个则由ITL对象为其赋值，来看下TL的set方法的实现，这次针对该方法介绍下TL子类的相关方法实现： 123456789101112131415161718192021// TL的set方法，如果是子类的实现，那么获取（getMap）和初始化赋值（createMap）都是ITL对象里的方法 // 其余操作不变（因为hash计算、查找、扩容都是TLMap里需要做的，这里子类ITL只起到一个为Thread对象里哪个TLMap属性赋值的作用） public void set(T value) { Thread t = Thread.currentThread(); ThreadLocal.ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); } // ITL里getMap方法的实现 ThreadLocal.ThreadLocalMap getMap(Thread t) { return t.inheritableThreadLocals; //返回的其实是Thread对象的inheritableThreadLocals属性 } // ITL里createMap方法的实现 void createMap(Thread t, T firstValue) { // 也是给Thread的inheritableThreadLocals属性赋值 t.inheritableThreadLocals = new ThreadLocal.ThreadLocalMap(this, firstValue); } 代码块6 而inheritableThreadLocals里的信息通过Thread的init方法是可以被传递下去的： 123456789101112131415161718192021222324252627282930313233343536373839404142// 初始化一个Thread对象时的代码段（Thread类的init方法） Thread parent = currentThread(); if (parent.inheritableThreadLocals != null){ //可以看到，如果父线程存在inheritableThreadLocals的时候，会赋值给子线程（当前正在被初始化的线程） // 利用父线程的TLMap对象，初始化一个TLMap，赋值给自己的inheritableThreadLocals（这就意味着这个TLMap里的值会一直被传递下去） this.inheritableThreadLocals = ThreadLocal.createInheritedMap(parent.inheritableThreadLocals); } // 看下TL里对应的方法 static ThreadLocal.ThreadLocalMap createInheritedMap(ThreadLocal.ThreadLocalMap parentMap) { return new ThreadLocal.ThreadLocalMap(parentMap); //这里就开始初始化TLMap对象了 } // 根据parentMap来进行初始化子线程的TLMap对象 private ThreadLocalMap(ThreadLocal.ThreadLocalMap parentMap) { ThreadLocal.ThreadLocalMap.Entry[] parentTable = parentMap.table; //拿到父线程里的哈希表 int len = parentTable.length; setThreshold(len); // 设置阈值（具体方法参考上一篇） table = new ThreadLocal.ThreadLocalMap.Entry[len]; for (int j = 0; j &lt; len; j++) { ThreadLocal.ThreadLocalMap.Entry e = parentTable[j]; //将父线程里的Entry取出 if (e != null) { @SuppressWarnings(\"unchecked\") ThreadLocal&lt;Object&gt; key = (ThreadLocal&lt;Object&gt;) e.get(); //获取key if (key != null) { Object value = key.childValue(e.value); //获取value ThreadLocal.ThreadLocalMap.Entry c = new ThreadLocal.ThreadLocalMap.Entry(key, value); //根据k-v重新生成一个Entry int h = key.threadLocalHashCode &amp; (len - 1); //计算哈希值 while (table[h] != null) h = nextIndex(h, len); //线性探查解决哈希冲突问题（具体方法参考上一篇） table[h] = c; //找到合适的位置后进行赋值 size++; } } } } // ITL里的childValue的实现 protected T childValue(T parentValue) { return parentValue; //直接将父线程里的值返回 } 代码块7 三、ITL所带来的的问题看过上述代码后，现在关于ITL的实现我们基本上有了清晰的认识了，根据其实现性质，可以总结出在使用ITL时可能存在的问题： 3.1：线程不安全 写在前面：这里讨论的线程不安全对象不包含Integer等类型，因为这种对象被重新赋值，变掉的是整个引用，这里说的是那种不改变对象引用，直接可以修改其内容的对象（典型的就是自定义对象的set方法） 如果说线程本地变量是只读变量不会受到影响，但是如果是可写的，那么任意子线程针对本地变量的修改都会影响到主线程的本地变量（本质上是同一个对象），参考上面的第三个例子，子线程写入后会覆盖掉主线程的变量，也是通过这个结果，我们确认了子线程TLMap里变量指向的对象和父线程是同一个。 3.2：线程池中可能失效按照上述实现，在使用线程池的时候，ITL会完全失效，因为父线程的TLMap是通过init一个Thread的时候进行赋值给子线程的，而线程池在执行异步任务时可能不再需要创建新的线程了，因此也就不会再传递父线程的TLMap给子线程了。 针对上述2，我们来做个实验，来证明下猜想： 123456789101112131415161718192021// 为了方便观察，我们假定线程池里只有一个线程 private static ExecutorService executorService = Executors.newFixedThreadPool(1); private static ThreadLocal tl = new InheritableThreadLocal&lt;&gt;(); public static void main(String[] args) { tl.set(1); System.out.println(String.format(\"线程名称-%s, 变量值=%s\", Thread.currentThread().getName(), tl.get())); executorService.execute(()-&gt;{ System.out.println(String.format(\"线程名称-%s, 变量值=%s\", Thread.currentThread().getName(), tl.get())); }); executorService.execute(()-&gt;{ System.out.println(String.format(\"线程名称-%s, 变量值=%s\", Thread.currentThread().getName(), tl.get())); }); System.out.println(String.format(\"线程名称-%s, 变量值=%s\", Thread.currentThread().getName(), tl.get())); } 代码块8 输出结果为： 1234线程名称-main, 变量值=1线程名称-pool-1-thread-1, 变量值=1线程名称-main, 变量值=1线程名称-pool-1-thread-1, 变量值=1 会发现，并没有什么问题，和我们预想的并不一样，原因是什么呢？因为线程池本身存在一个初始化的过程，第一次使用的时候发现里面的线程数（worker数）少于核心线程数时，会进行创建线程，既然是创建线程，一定会执行Thread的init方法，参考上面提到的源码，在第一次启用线程池的时候，类似做了一次new Thread的操作，因此是没有什么问题的，父线程的TLMap依然可以传递下去。 现在我们改造下代码，把tl.set(1)改到第一次启用线程池的下面一行，然后再看看： 12345678910111213141516public static void main(String[] args) throws Exception{ System.out.println(String.format(\"线程名称-%s, 变量值=%s\", Thread.currentThread().getName(), tl.get())); executorService.execute(()-&gt;{ System.out.println(String.format(\"线程名称-%s, 变量值=%s\", Thread.currentThread().getName(), tl.get())); }); tl.set(1); // 等上面的线程池第一次启用完了，父线程再给自己赋值 executorService.execute(()-&gt;{ System.out.println(String.format(\"线程名称-%s, 变量值=%s\", Thread.currentThread().getName(), tl.get())); }); System.out.println(String.format(\"线程名称-%s, 变量值=%s\", Thread.currentThread().getName(), tl.get())); } 代码块9 输出结果为： 1234线程名称-main, 变量值=null线程名称-main, 变量值=1线程名称-pool-1-thread-1, 变量值=null线程名称-pool-1-thread-1, 变量值=null 很明显，第一次启用时没有递进去的值，在后续的子线程启动时就再也传递不进去了。 尾声但是，在实际项目中我们大多数采用线程池进行做异步任务，假如真的需要传递主线程的本地变量，使用ITL的问题显然是很大的，因为是有极大可能性拿不到任何值的，显然在实际项目中，ITL的位置实在是尴尬，所以在启用线程池的情况下，不建议使用ITL做值传递。为了解决这种问题，阿里做了transmittable-thread-local（TTL）来解决线程池异步值传递问题，下一篇，我们将会分析TTL的用法及原理。","link":"/2019/02/19/ThreadLocal%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%8C%EF%BC%89-InheritableThreadLocal%E7%9A%84%E4%BD%BF%E7%94%A8%E5%8F%8A%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/"},{"title":"ThreadLocal系列（一）-ThreadLocal的使用及原理解析","text":"项目中我们如果想要某个对象在程序运行中的任意位置获取到，就需要借助ThreadLocal来实现，这个对象称作线程的本地变量，下面就介绍下ThreadLocal是如何做到线程内本地变量传递的， 一、基本使用先来看下基本用法： 1234567891011121314private static ThreadLocal tl = new ThreadLocal&lt;&gt;();public static void main(String[] args) throws Exception { tl.set(1); System.out.println(String.format(\"当前线程名称: %s, main方法内获取线程内数据为: %s\", Thread.currentThread().getName(), tl.get())); fc(); new Thread(ThreadLocalTest::fc).start();}private static void fc() { System.out.println(String.format(\"当前线程名称: %s, fc方法内获取线程内数据为: %s\", Thread.currentThread().getName(), tl.get()));} 代码块1 运行结果： 123当前线程名称: main, main方法内获取线程内数据为: 1当前线程名称: main, fc方法内获取线程内数据为: 1当前线程名称: Thread-0, fc方法内获取线程内数据为: null 可以看到，main线程内任意地方都可以通过ThreadLocal获取到当前线程内被设置进去的值，而被异步出去的fc调用，却由于替换了执行线程，而拿不到任何数据值，那么我们现在再来改造下上述代码，在异步发生之前，给Thread-0线程也设置一个上下文数据： 12345678910111213141516171819private static ThreadLocal tl = new ThreadLocal&lt;&gt;(); public static void main(String[] args) throws Exception { tl.set(1); System.out.println(String.format(\"当前线程名称: %s, main方法内获取线程内数据为: %s\", Thread.currentThread().getName(), tl.get())); fc(); new Thread(()-&gt;{ tl.set(2); //在子线程里设置上下文内容为2 fc(); }).start(); Thread.sleep(1000L); //保证下面fc执行一定在上面异步代码之后执行 fc(); //继续在主线程内执行，验证上面那一步是否对主线程上下文内容造成影响 } private static void fc() { System.out.println(String.format(\"当前线程名称: %s, fc方法内获取线程内数据为: %s\", Thread.currentThread().getName(), tl.get())); } 代码块2 运行结果为： 1234当前线程名称: main, main方法内获取线程内数据为: 1当前线程名称: main, fc方法内获取线程内数据为: 1当前线程名称: Thread-0, fc方法内获取线程内数据为: 2当前线程名称: main, fc方法内获取线程内数据为: 1 可以看到，主线程和子线程都可以获取到自己的那份上下文里的内容，而且互不影响。 二、原理分析ok，上面通过一个简单的例子，我们可以了解到ThreadLocal（以下简称TL）具体的用法，这里先不讨论它实质上能给我们带来什么好处，先看看其实现原理，等这些差不多了解完了，我再通过我曾经做过的一个项目，去说明TL的作用以及在企业级项目里的用处。 我以前在不了解TL的时候，想着如果让自己实现一个这种功能的轮子，自己会怎么做，那时候的想法很单纯，觉得通过一个Map就可以解决，Map的key设置为Thread.currentThread()，value设置为当前线程的本地变量即可，但后来想想就觉得不太现实了，实际项目中可能存在大量的异步线程，对于内存的开销是不可估量的，而且还有个严重的问题，线程是运行结束后就销毁的，如果按照上述的实现方案，map内是一直持有这个线程的引用的，导致明明执行结束的线程对象不能被jvm回收，造成内存泄漏，时间久了，会直接OOM。 所以，java里的实现肯定不是这么简单的，下面，就来看看java里的具体实现吧。 先来了解下，TL的基本实现，为了避免上述中出现的问题，TL实际上是把我们设置进去的值以k-v的方式放到了每个Thread对象内（TL对象做k，设置的值做v），也就是说，TL对象仅仅起到一个标记、对Thread对象维护的map赋值的作用。 先从set方法看起： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public void set(T value) { Thread t = Thread.currentThread(); //获取当前线程 ThreadLocal.ThreadLocalMap map = getMap(t); //获取到当前线程持有的ThreadLocalMap对象 if (map != null) map.set(this, value); //直接set值，具体方法在下面 else createMap(t, value); // 为空就给当前线程创建一个ThreadLocalMap对象，赋值给Thread对象，具体方法在下面 } ThreadLocal.ThreadLocalMap getMap(Thread t) { return t.threadLocals; //每个线程都有一个ThreadLocalMap，key为TL对象（其实是根据对象hash计算出来的值），value为该线程在此TL对象下存储的内容值 } private void set(ThreadLocal&lt;?&gt; key, Object value) { ThreadLocal.ThreadLocalMap.Entry[] tab = table; //获取存储k-v对象的数组（散列表） int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); //根据TL对象的hashCode（也是特殊计算出来的，保证每个TL对象的hashCode不同）计算出下标 for (ThreadLocal.ThreadLocalMap.Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) { //线性探查法解决哈希冲突问题，发现下标i已经有Entry了，则就查看i+1位置处是否有值，以此类推 ThreadLocal&lt;?&gt; k = e.get(); //获取k if (k == key) { //若k就是当前TL对象，则直接为其value赋值 e.value = value; return; } if (k == null) { //若k为空，则认为是可回收的Entry，则利用当前k和value组成新的Entry替换掉该可回收Entry replaceStaleEntry(key, value, i); return; } } //for循环执行完没有终止程序，说明遇到了空槽，这个时候直接new对象赋值即可 tab[i] = new ThreadLocal.ThreadLocalMap.Entry(key, value); int sz = ++size; if (!cleanSomeSlots(i, sz) &amp;&amp; sz &gt;= threshold) //这里用来清理掉k为null的废弃Entry rehash(); //如果没有发生清除Entry并且size超过阈值（阈值 = 最大长度 * 2/3），则进行扩容 } //直接为当前Thread初始化它的ThreadLocalMap对象 void createMap(Thread t, T firstValue) { t.threadLocals = new ThreadLocal.ThreadLocalMap(this, firstValue); } ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) { table = new ThreadLocal.ThreadLocalMap.Entry[INITIAL_CAPACITY]; //初始化数组 int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1); //计算初始位置 table[i] = new ThreadLocal.ThreadLocalMap.Entry(firstKey, firstValue); //因为初始化不存在hash冲突，直接new size = 1; setThreshold(INITIAL_CAPACITY); //给阈值赋值，上面已经提及，阈值 = 最大长度 * 2/3 } 代码块3 通过上述代码，我们大致了解了TL在set值的时候发生的一些操作，结合之前说的，我们可以确定的是，TL其实对于线程来说，只是一个标识，而真正线程的本地变量被保存在每个线程对象的ThreadLocalMap里，这个map里维护着一个Entry[]的数组（散列表），Entry是个k-v结构的对象（如图1-1），k为TL对象，v为对应TL保存在该线程内的本地变量值，值得注意的是，这里的k针对TL对象的引用是个弱引用，来看下源码： 123456789static class Entry extends WeakReference&lt;ThreadLocal&lt;?&gt;&gt; { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal&lt;?&gt; k, Object v) { super(k); value = v; } } 代码块4 为什么这里需要弱引用呢？我们先来看一张图，结合上面的介绍和这张图，来了解TL和Thread间的关系： 图中虚线表示弱引用，那么为什么要这么做呢？ 简单来说，一个TL对象被创建出来，并且被一个线程放到自己的ThreadLocalMap里，假如TL对象失去原有的强引用，但是该线程还没有死亡，如果k不是弱引用，那么就意味着TL并不能被回收，现在k为弱引用，那么在TL失去强引用的时候，gc可以直接回收掉它，弱引用失效，这就是上面代码里会进行检查，k=null的清除释放内存的原因（这个可以参考下面expungeStaleEntry方法，而且set、get、remove都会调用该方法，这也是TL防止内存泄漏所做的处理）。 综上，简单来说这个弱引用就是用来解决由于使用TL不当导致的内存泄漏问题的，假如没有弱引用，那么你又用到了线程池（池化后线程不会被销毁），然后TL对象又是局部的，那么就会导致线程池内线程里的ThreadLocalMap存在大量的无意义的TL对象引用，造成过多无意义的Entry对象，因为即便调用了set、get等方法检查k=null，也没有作用，这就导致了内存泄漏，长时间这样最终可能导致OOM，所以TL的开发者为了解决这种问题，就将ThreadLocalMap里对TL对象的引用改为弱引用，一旦TL对象失去强引用，TL对象就会被回收，那么这里的弱引用指向的值就为null，结合上面说的，调用操作方法时会检查k=null的Entry进行回收，从而避免了内存泄漏的可能性。 因为TL解决了内存泄漏的问题，因此即便是局部变量的TL对象且启用线程池技术，也比较难造成内存泄漏的问题，而且我们经常使用的场景就像一开始的示例代码一样，会初始化一个全局的static的TL对象，这就意味着该对象在程序运行期间都不会存在强引用消失的情况，我们可以利用不同的TL对象给不同的Thread里的ThreadLocalMap赋值，通常会set值（覆盖原有值），因此在使用线程池的时候也不会造成问题，异步开始之前set值，用完以后remove，TL对象可以多次得到使用，启用线程池的情况下如果不这样做，很可能业务逻辑也会出问题（一个线程存在之前执行程序时遗留下来的本地变量，一旦这个线程被再次利用，get时就会拿到之前的脏值）； 说完了set，我们再来看下get： 123456789101112131415161718192021222324252627282930313233343536373839404142434445public T get() { Thread t = Thread.currentThread(); ThreadLocal.ThreadLocalMap map = getMap(t); //获取线程内的ThreadLocalMap对象 if (map != null) { ThreadLocal.ThreadLocalMap.Entry e = map.getEntry(this); //根据当前TL对象（key）获取对应的Entry if (e != null) { @SuppressWarnings(\"unchecked\") T result = (T)e.value; return result; //直接返回value即可 } } return setInitialValue(); //如果发现当前线程还没有ThreadLocalMap对象，则进行初始化 } private ThreadLocal.ThreadLocalMap.Entry getEntry(ThreadLocal&lt;?&gt; key) { int i = key.threadLocalHashCode &amp; (table.length - 1); //计算下标 ThreadLocal.ThreadLocalMap.Entry e = table[i]; if (e != null &amp;&amp; e.get() == key) //根据下标获取的Entry对象如果key也等于当前TL对象，则直接返回结果即可 return e; else return getEntryAfterMiss(key, i, e); //上面说过，有些情况下存在下标冲突的问题，TL是通过线性探查法来解决的，所以这里也一样，如果上面没找到，则继续通过下标累加的方式继续寻找 } private ThreadLocal.ThreadLocalMap.Entry getEntryAfterMiss(ThreadLocal&lt;?&gt; key, int i, ThreadLocal.ThreadLocalMap.Entry e) { ThreadLocal.ThreadLocalMap.Entry[] tab = table; int len = tab.length; while (e != null) { ThreadLocal&lt;?&gt; k = e.get(); //继续累加下标的方式一点点的往下找 if (k == key) //找到了就返回出去结果 return e; if (k == null) //这里也会检查k==null的Entry，满足就执行删除操作 expungeStaleEntry(i); else //否则继续累加下标查找 i = nextIndex(i, len); e = tab[i]; } return null; //找不到返回null } //这里也放一下nextIndex方法 private static int nextIndex(int i, int len) { return ((i + 1 &lt; len) ? i + 1 : 0); } 代码块5 最后再来看看remove方法： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public void remove() { ThreadLocal.ThreadLocalMap m = getMap(Thread.currentThread()); if (m != null) m.remove(this); //清除掉当前线程ThreadLocalMap里以当前TL对象为key的Entry } private void remove(ThreadLocal&lt;?&gt; key) { ThreadLocal.ThreadLocalMap.Entry[] tab = table; int len = tab.length; int i = key.threadLocalHashCode &amp; (len-1); //计算下标 for (ThreadLocal.ThreadLocalMap.Entry e = tab[i]; e != null; e = tab[i = nextIndex(i, len)]) { if (e.get() == key) { //找到目标Entry e.clear(); //清除弱引用 expungeStaleEntry(i); //通过该方法将自己清除 return; } } } private int expungeStaleEntry(int staleSlot) { //参数为目标下标 ThreadLocal.ThreadLocalMap.Entry[] tab = table; int len = tab.length; tab[staleSlot].value = null; //首先将目标value清除 tab[staleSlot] = null; size--; // Rehash until we encounter null ThreadLocal.ThreadLocalMap.Entry e; int i; // 由目标下标开始往后逐个检查，k==null的清除掉，不等于null的要进行rehash for (i = nextIndex(staleSlot, len); (e = tab[i]) != null; i = nextIndex(i, len)) { ThreadLocal&lt;?&gt; k = e.get(); if (k == null) { e.value = null; tab[i] = null; size--; } else { int h = k.threadLocalHashCode &amp; (len - 1); if (h != i) { tab[i] = null; // Unlike Knuth 6.4 Algorithm R, we must scan until // null because multiple entries could have been stale. while (tab[h] != null) h = nextIndex(h, len); tab[h] = e; } } } return i; } 代码块6 目前主要方法set、get、remove已经介绍完了，包含其内部存在的弱引用的作用，以及实际项目中建议的用法，以及为什么要这样用，也进行了简要的说明，下面一篇会进行介绍InheritableThreadLocal的用法以及其原理性分析。","link":"/2019/02/15/ThreadLocal%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89-ThreadLocal%E7%9A%84%E4%BD%BF%E7%94%A8%E5%8F%8A%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/"},{"title":"利用ReentrantLock简单实现一个阻塞队列","text":"借助JUC里的ReentrantLock实现一个阻塞队列结构： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package demo.concurrent.lock.queue;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.ReentrantLock;/** * @author sunqinwen * @version \\: SimpleQueue.java,v 0.1 2019-01-16 14:47 * 利用重入锁和重入锁的线程调度实现的简单阻塞队列 */public class SimpleQueue { private static ReentrantLock lock = new ReentrantLock(); private T[] nodes; private int tail = 0; // 入元素下标 private int count = 0; // 元素个数 private int head = 0; // 出元素下标 public SimpleQueue(int size) { nodes = (T[]) new Object[size]; } private static Condition notFull = lock.newCondition(); private static Condition notEmpty = lock.newCondition(); public void put(T t) { try { lock.lock(); if (count == nodes.length) { // 队列已满，阻塞 System.out.println(\"目前队列已满，等待取值中\"); notFull.await(); } if (tail &gt; (nodes.length - 1)) { // 当前游标值已经大于数组游标最大值了，则从0开始计算 tail = 0; } nodes[tail] = t; // 给当前游标位赋值 count++; // 入元素元素个数+1 tail++; // 游标值+1 notEmpty.signalAll(); // 走到这里说明队列内至少有一个元素，则唤醒取值 } catch (Exception e) { e.printStackTrace(); } finally { lock.unlock(); } } public T take() { T t = null; try { lock.lock(); if (count == 0) { // 队列已空，等待加值 System.out.println(\"目前队列已空，等待入值中\"); notEmpty.await(); } if (head &gt; (nodes.length - 1)) { // 若取值游标大于游标最大值，则从0开始计算 head = 0; } t = nodes[head]; // 拿到元素值 nodes[head] = null; // 清空原有位置上的值 head++; // 取值游标+1 count--; // 元素个数-1 notFull.signalAll(); // 走到这里说明队列至少有一个空位，则唤醒入值 } catch (Exception e) { e.printStackTrace(); } finally { lock.unlock(); } return t; }} 代码块1 以上为主要代码，下面进行简单的测试： 12345678910111213141516171819202122232425262728293031@Testpublic void simpleQueueTest() throws Exception { executorService.execute(() -&gt; { simpleQueue.put(1); simpleQueue.put(2); simpleQueue.put(3); simpleQueue.put(4); simpleQueue.put(5); simpleQueue.put(6); simpleQueue.put(7); simpleQueue.put(8); simpleQueue.put(9); simpleQueue.put(10); simpleQueue.put(11); simpleQueue.put(12); }); Thread.sleep(5000L); executorService.execute(() -&gt; { Integer r; while ((r = simpleQueue.take()) != null) { System.out.println(r); } }); Thread.sleep(5000L);} 代码块2 运行结果： 123456789101112131415161718目前队列已满，等待取值中目前队列已满，等待取值中12目前队列已满，等待取值中3目前队列已满，等待取值中456789目前队列已空，等待入值中101112目前队列已空，等待入值中","link":"/2019/02/12/%E5%88%A9%E7%94%A8ReentrantLock%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E9%98%BB%E5%A1%9E%E9%98%9F%E5%88%97/"},{"title":"深入理解map系列-HashMap（一）","text":"Map系列之HashMap（源码基于java8） HashMap是我们最常用的map实现之一，这篇文章将会介绍HashMap内部是如何工作的，以及内部的数据结构是怎样的 一、数据结构简图 二、源码解析首先看下Map接口里常用的几个方法： 1234V put(K key, V value);V get(Object key);V remove(Object key);boolean containsKey(Object key); 代码块1 上面是常用的主要操作方法，下面来看下map的基本存储单位Entry： 12345678910111213interface Entry&lt;K,V&gt; { K getKey(); //返回当前存储数据里的key V getValue(); //返回当前存储数据里的value V setValue(V value); //给value赋值 boolean equals(Object o); //重写equals方法 int hashCode(); //重写hashCode方法 } 代码块2 然后我们来看下HashMap里对该接口的实现： 12345678910111213141516171819202122232425262728293031323334353637383940414243// 基本存储结构，可以看出来这是一个简单的链表结构，这里的实现类叫Nodestatic class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; { final int hash; //根据key计算出来的哈希值 final K key; //数据键 V value; //数据值 Node&lt;K,V&gt; next; //下一个数据节点 Node(int hash, K key, V value, Node&lt;K,V&gt; next) { this.hash = hash; this.key = key; this.value = value; this.next = next; } public final K getKey() { return key; } public final V getValue() { return value; } public final String toString() { return key + \"=\" + value; } public final int hashCode() { return Objects.hashCode(key) ^ Objects.hashCode(value); } public final V setValue(V newValue) { V oldValue = value; value = newValue; return oldValue; } // 判等，要求k，v必须满足相等才行 public final boolean equals(Object o) { if (o == this) return true; if (o instanceof Map.Entry) { Map.Entry e = (Map.Entry)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; } return false; } } 代码块3 我们再来看看hash值的计算，在哈希表中，哈希值取决了散列度，最终插入的数据会分布到哪个数组下标下，hash值起着至关重要的作用： 1234static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16); } 代码块4 下面我们来看看具体插入数据时做的操作，具体解释已经加上注释： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { HashMap.Node&lt;K,V&gt;[] tab; //存储链表的数组结构 HashMap.Node&lt;K,V&gt; p; //被插入的元素链表头部元素 int n, i; //n表示当前哈希表数组长度，i表示本次插入元素被分配的下标 if ((tab = table) == null || (n = tab.length) == 0) { //表示哈希表数组还未被初始化 n = (tab = resize()).length; //初始化，resize用来扩容 } //表示当前（下标由最大下标值和当前元素哈希值位运算得出）位置还没有任何链表结构，这时直接初始化即可 if ((p = tab[i = (n - 1) &amp; hash]) == null) { tab[i] = newNode(hash, key, value, null); } else { // 否则，需要进行链表数据插入的操作，注意现在p已经是计算出来的链表头元素了 HashMap.Node&lt;K,V&gt; e; K k; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) { e = p; // 若发现插入的数据跟p哈希值、key完全一致，则直接让新插入的数据等于p即可 } else if (p instanceof HashMap.TreeNode){ // 结合下面的代码，链表深度大于8后，就是个红黑树结构了，这时启用下面的代码加入新数据 e = ((HashMap.TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); } else { // 说明插入的是新元素 for (int binCount = 0; ; ++binCount) { // 遍历链表 if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); //插入链表尾部 if (binCount &gt;= TREEIFY_THRESHOLD - 1) // java8新引入的概念，当链表深度大于8时，就转换为红黑树结构了 treeifyBin(tab, hash); break; } if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) { break; // 若发现遍历过程中存在与插入值一致的，直接break } p = e; } } if (e != null) { // 说明未成功插入 V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; // 返回已存在的旧值 } } ++modCount; if (++size &gt; threshold) { //新插入值后，满足扩容条件则进行扩容 resize(); //扩容 } afterNodeInsertion(evict); return null; } 代码块5 由于java8做了根据元素数量，转换成红黑树结构的优化处理，所以上述代码中会掺杂一些相关的代码，这里先不用关心，我们按照最基本的哈希表结构来看就行，下一讲将会分析红黑树结构。 我们接下来来看下get方法： 1234public V get(Object key) { Node&lt;K,V&gt; e; return (e = getNode(hash(key), key)) == null ? null : e.value;} 代码块6 然后getNode方法： 1234567891011121314151617181920212223242526final HashMap.Node&lt;K,V&gt; getNode(int hash, Object key) { HashMap.Node&lt;K,V&gt;[] tab; //哈希表数组 HashMap.Node&lt;K,V&gt; first, e; //根据hash查找数组内的第一个元素 int n; K k; // n表示数组长度 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (first = tab[(n - 1) &amp; hash]) != null) { // 根据下标（下标由最大下标值和当前元素哈希值位运算得出）获取当前对应第一个元素（链表或者红黑树的根元素） if (first.hash == hash &amp;&amp; // 检查第一个节点的key是否等于当前查找的key，若等，直接返回 ((k = first.key) == key || (key != null &amp;&amp; key.equals(k)))){ return first; } // 否则继续遍历查找 if ((e = first.next) != null) { if (first instanceof HashMap.TreeNode) { //红黑树结构的查询 return ((HashMap.TreeNode&lt;K,V&gt;)first).getTreeNode(hash, key); } // 普通链表结构遍历查询，查到直接返回 do { if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))){ return e; } } while ((e = e.next) != null); } } return null; } 代码块7 ok,上面说完了put和get，现在我们来看下remove，也是先抛开红黑树不谈，只看链表部分，会很容易： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public V remove(Object key) { HashMap.Node&lt;K, V&gt; e; return (e = removeNode(hash(key), key, null, false, true)) == null ? null : e.value; }final HashMap.Node&lt;K, V&gt; removeNode(int hash, Object key, Object value, boolean matchValue, boolean movable) { HashMap.Node&lt;K, V&gt;[] tab; //哈希表数组 HashMap.Node&lt;K, V&gt; p; //需要被移除的元素所属的根元素 int n, index; //n表示数组长度，index表示需要被移除元素根元素位于数组的下标值 if ((tab = table) != null &amp;&amp; (n = tab.length) &gt; 0 &amp;&amp; (p = tab[index = (n - 1) &amp; hash]) != null) { HashMap.Node&lt;K, V&gt; node = null, e; // node表示最终需要被移除的元素 K k; V v; if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) { node = p; // 若根元素就等于需要被移除的元素，则直接将node赋值为p } else if ((e = p.next) != null) { // 否则继续往下查找，结构依然分为两种，红黑树暂不看 if (p instanceof HashMap.TreeNode) { node = ((HashMap.TreeNode&lt;K, V&gt;) p).getTreeNode(hash, key); } else { do { if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) { node = e; break; // 找到对应的元素，break } p = e; // 找不到对应元素时，让p一直下移（e.next） } while ((e = e.next) != null); } } if (node != null &amp;&amp; (!matchValue || (v = node.value) == value || (value != null &amp;&amp; value.equals(v)))) { if (node instanceof HashMap.TreeNode) { //红黑树移除 ((HashMap.TreeNode&lt;K, V&gt;) node).removeTreeNode(this, tab, movable); } else if (node == p) { // 待移除元素等于根元素时，直接让对应下标下的数组元素赋值为根元素的下一个值 tab[index] = node.next; } else { //否则，就进行链表正常删除逻辑，让被移除元素的前一个元素（为什么现在的p是前一个元素呢？因为在上述do while操作时已经重新赋值了）的下一个值指向被移除元素的下一个值 p.next = node.next; } ++modCount; --size; afterNodeRemoval(node); return node; } } return null; } 代码块8 好了，目前基本上把重要的一些操作给介绍完了，现在再看下containsKay这个方法，这个方法极度简单，直接调用getNode方法判空即可： 123public boolean containsKey(Object key) { return getNode(hash(key), key) != null;} 代码块9 本篇的侧重点在于HashMap在使用纯链表时的插入、移除、查找方式，下一篇将会介绍HashMap如何扩容数组、以及在启用红黑树结构下，会如何做插入、移除、查找这几种操作方式。","link":"/2019/02/12/%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3map%E7%B3%BB%E5%88%97-HashMap%EF%BC%88%E4%B8%80%EF%BC%89/"},{"title":"Redis小记-内存解析&内存消耗篇","text":"前置：redis内存指标 注：本文默认读者已初步学会使用redis了。 首先我们通过info命令查看相关指标，其中几个memory的重要指标整理出来如下： 属性 解释 used_memory redis内部存储的所有数据的内存总占用量（自身内存+对象内存+缓冲内存） used_memory_ress redis进程占用的总物理内存 mem_fragmentation_ratio used_memory_ress与used_memory的比值，即为内存碎片率 mem_allocator 内存分配器，默认为jemalloc 表1 一、碎片率 当内存碎片率 &gt; 1时，说明redis进程占用物理内存的总量大于Redis实际存储数据（表1第一行）的内存占用量，溢出来的部分内存被内存碎片消耗，如果溢出部分过大，则说明内存碎片率严重。 相反的，如果碎片率 &lt; 1时，则说明Redis存储的数据总量已经超出了redis进程占用内存的总量，造成这种情况是因为操作系统把Redis内存交换至硬盘导致（swap），由于硬盘读取速度远远慢与内存，因此这种情况下redis性能极差，可能出现僵死。 二、redis内存消耗的几个来源2.1：自身内存redis启动后自身运行所需内存； 2.2：对象内存内存占用最大的一部分，这里面存储的就是用户自身的数据（业务数据），数据以key-value类型存储，内存消耗可表示为：key内存+value内存。 2.3：缓冲内存主要由客户端缓冲区+复制积压缓冲区+AOF缓冲区组成，具体解释如下： 客户端缓冲区指的是所有接入redis服务器的TCP连接的输入和输出缓冲，输入缓冲无法被控制，最大空间为1G，超过立即断开连接，输出缓冲通过client-output-buffer-limit控制。 复制积压缓冲区指的是redis在2.8版本以后提供了一块可以重复利用的固定大小的缓冲区，用来实现部分复制功能，使用repl-backlog-size参数控制，默认1MB（主从结构下，主节点只存在一个该缓冲区，从节点共用，那时可以设置较大的缓冲区空间），该缓冲区可以避免全量复制。 AOF缓冲区用于存储在redis重写期间保存最近的写入命令，无法控制，通常取决于AOF重写时间以及写入命令量，一般情况下很小。 2.4：内存碎片redis默认的内存分配器是jemalloc，可选的还有glibc和tcmalloc；内存分配器为了更好的管理以及重复利用内存，分配策略一般采用固定范围的内存块进行分配；因此，我们在存储一块5kb的内容时，内存分配器可能会为我们分配8kb的块存储，剩下的3kb不能再次分配给其他对象存储，因而沦为了内存碎片；jemalloc对碎片化问题做了优化，一般来讲碎片化率保持在1.03左右。 可能造成内存碎片率过高的场景： 频繁的更新操作，例如频繁对已存在的键做append、setrange等操作； 大量过期键删除，键对象过期删除后释放的空间无法得到充分的利用，导致碎片率上升。 解决办法： 数据对齐，尽量采用数字类型或固定长度的字符串（大部分业务场景不满足这种方式）； 重启，重启节点可以使内存重整理，利用高可用的结构（节点集群+主从结构），将碎片率过高的节点主节点转换为从节点，然后进行安全重启。 2.5：子进程内存消耗子进程内存消耗指的是执行AOF/RDB重写时redis创建的子进程内存消耗；redis执行fork操作产生的子进程内存占用量对外表现为与父进程相同，理论上需要一倍的物理内存来完成重写的操作。但是linux具备写时复制技术（copy-on-write），父子进程会共享相同的物理内存页，当父进程处理写请求时会对需要修改的页复制出一份副本来完成写操作，而子进程依然读取fork时整个父进程的内存快照，总结： 子进程并不需要消耗一倍的父进程内存，实际消耗根据期间写入命令量决定，但依然要预留出一些内存防止溢出； 需要设置sysctl vm.overcommit_memory = 1允许内核可以分配所有的物理内存，防止redis进程执行fork时因剩余内存不足导致失败； 排查当前系统是否支持开启THP，如果开启建议关闭，防止copy-on-write期间内存过度消耗。","link":"/2017/08/12/Redis%E5%B0%8F%E8%AE%B0-%E5%86%85%E5%AD%98%E8%A7%A3%E6%9E%90&%E5%86%85%E5%AD%98%E6%B6%88%E8%80%97%E7%AF%87/"},{"title":"数据库事务的隔离级别","text":"一、数据库事务的几个特性1.1：原子性最基本的特性，意思是在一个事务内里所有关于数据库的操作，要么全部成功，要么全部失败；成功时意味着本次操作所有数据库相关的写操作全部持久化，无法更改，失败意味着本次操作相对于操作前对数据库没有任何影响和改变。 1.2：一致性指的是一次完整的事务必须将数据库的一个一致状态转变到另外一个一致状态。 一致性写 例如：事务A要做的操作是将A、B、C三个记录修改为D、E、F，那么A、B、C—–&gt;D、E、F的过程就满足了事务一致性，但是如果出现类似：A、B、C—-&gt;D、E、C（A、B修改成功，但是C未修改）则认定违背了事务的一致性，简单理解一致性就是指事务的“初始状态”到“修改完成状态”与“目标状态一致”。 一致性读 事务A在某一刻发起查询请求，那么查询结果是以那一刻为准，保证了数据在查询一刻的一致性。 1.3：持久性指一次事务的成功提交对数据库造成的修改是永久性的。 1.4：隔离性当多个用户并发访问数据库时，数据库为每一个用户开启的事务不可以被其他事务所影响，也就是说并发事务间要相互独立不受到干扰。关于隔离性分了集中隔离等级，本篇文章将详细介绍这几种隔离等级。 二、事务并发时的隔离级别2.1：Read Uncommitted（读未提交）这个隔离级别下未被提交的事务下所做的任何操作都可以被其他事务所读取到，这时候会造成数据的脏读、幻读、不可重复读问题。 2.2：Read Committed（读已提交）这个隔离级别下未被提交的操作不可以被其他事务所读取到，简单来讲就是单个事务里的内容在事务成功提交之前，是不会被其他事务所读取（发现）到的，但是这样同样会出现幻读、不可重复读现象。 举个栗子：事务T1要对C表做添加操作，同时事务T2里要读取C表，T2第一次读取C表时返回1条数据，这时T1执行完毕，那么T2如果再次取一次C表数据就会发现多出一条数据。 2.3：Repeatable Read（可重读）Mysql默认的隔离级别，这个隔离级别下同一事务读取到的数据一致（简单点说就是T1一旦开始，读取到数据如果中间被T2修改，那么T1再次读取该数据是和第一次读取时一样的），因此，在该隔离级别下，不会造成脏读、不可重复读，但依旧会造成幻读现象。 2.4：Serializable（串行）最高隔离级别，会为每个事务排序（为每条数据都加上锁），使之执行串行化，不可能产生冲突，因此解决了脏读、幻读、不可重复读问题，但是会造成锁竞争甚至超时，一般不会采用这种极端的隔离机制。 三、事务并发过程中产生的问题3.1：脏读一个事务读取到了另外一个事务中未提交的数据。 3.2：不可重复读一个事务读取到了另外一个事务中提交的修改掉的数据。 3.3：幻读一个事务读取到了另外一个事务中添加的数据。 Tip：不可重复读和幻读的区别在于着重点一个是update，一个是insert 四、各种隔离级别下对应的问题通过设置不同的事务隔离级别，可以避免事务并发时所造成的部分问题。 总结四种隔离级别所造成和避免的问题（请先看以上内容后再看此表）： 隔离级别 脏读 不可重复读 幻读 Read Uncommitted 是 是 是 Read Committed 否 是 是 Repeatable Read 否 否 是 Serializable 否 否 否 表1","link":"/2017/04/10/%E6%95%B0%E6%8D%AE%E5%BA%93%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/"},{"title":"简单实现生产者和消费者模式","text":"本实例中单独为生产者和消费者各开辟一个线程作为生产者和消费者的执行线程，在生产者消费者设计模式中存在一个数据缓冲区，使生产者和消费者的“生产”和“消费”动作都在该缓冲区进行，这样做的目的就是保证了生产者和消费者的完美解耦，试想一下如果没了这个缓冲区，生产者和消费者中的方法互调，那么两个类的关联度（耦合度）就会很高，一旦一个发生变化，势必会影响另外一个； 下面开始我们的实例： 首先是生产者的代码： 12345678910111213141516171819202122/** * 生产者 */public class Product implements Runnable{ private Queue queue; public Product(Queue queue){ this.queue = queue; } @Override public void run() { try{ for(int i = 0; i &lt; 10; i++){ queue.product(\"Product------\" + \"No.\" + i);//开始生产 } }catch (Exception e){ e.printStackTrace(); } }} 代码块1 这是消费者： 12345678910111213141516171819202122/** * 消费者 */public class Consumer implements Runnable{ private Queue queue; public Consumer(Queue queue){ this.queue = queue; } @Override public void run() { try{ for(int i = 0; i &lt; 10; i++){ System.out.println(\"already gone : \" + queue.consumer());//开始消费 } }catch (Exception e){ e.printStackTrace(); } }} 代码块2 这是缓冲区，几乎所有的逻辑都是在这里实现的： 123456789101112131415161718192021222324252627282930313233343536373839/** * 队列缓冲区 */public class Queue { private Object signal = new Object();//当前线程的挂起和执行标记 private boolean isFull = false;//队列是否已满 private List list = new ArrayList&lt;&gt;();//队列 public void product(String msg) throws Exception{ synchronized (signal){ if(!isFull){//如果没有满，执行如下代码 list.add(msg);//加进队列 isFull = true; System.out.println(\"Product One !\"); signal.notify();//唤醒当前消费者里面被挂起的线程 } signal.wait();//否则，如果当前满了，说明消费者正在消费，挂起当前生产线程 } } public String consumer() throws Exception{ synchronized (signal){ if(!isFull){ //不满，说明生产者正在生产，应当挂起consumer线程 System.out.println(\"Empty Product !\"); signal.wait(); } isFull = false;//已消费，队列被标记为不满状态 signal.notify();//通知生产者 } //消费（读取） String result = \"\"; if(list.size() &gt; 0){ result = this.list.get(list.size() - 1); this.list.remove(list.size() - 1); } return result; }} 代码块3 上面这个模式利用java现有的阻塞队列很容易实现，可以避免上述代码中很大一部分代码（线程的挂起、唤醒、队列弹出数据等）","link":"/2016/04/15/%E7%AE%80%E5%8D%95%E5%AE%9E%E7%8E%B0%E7%94%9F%E4%BA%A7%E8%80%85%E5%92%8C%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%BC%8F/"},{"title":"简单模拟spring的ioc和aop","text":"spring最核心的部分莫过于ioc和aop了，下面来简单模拟下这两种思想 ps：如果有哪里理解的不对或者代码上有瑕疵的地方欢迎大家指正，大家互相学习，还有就是这只是模仿一下spring思想，只是把事务管理和bean管理简单模仿一下，完全不代表spring，如果想深入理解请看spring源码 下面就开始进行简单的模拟。 这个项目不是web项目，只是一个简单的java项目，测试用junit，废话不多说了，下面上代码： 项目的目录结构： 说明：图中划红线的部分都是核心部分 红线部分说明： BeanFactory：所有bean的核心生成器（spring容器） ConnBean：jdbc连接生成器（没用连接池哦~） Transaction：事务管理的代理类 beans.properties：配置文件 其余的没划线的就是domain、dao、service、controller这些web基本层次结构，待会会说 主要几个类的代码： ① BeanFactory： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package sun.juwin.factory;import java.io.BufferedReader;import java.io.InputStreamReader;import java.util.HashMap;/** * 本类用来读取配置文件中的信息对每个接口对象生成具体的实现 * 主要是将接口作为key，实例作为value存储进去，这是个单例， * spring默认为每个层次生成实现也是单例，但可以通过@Scope * 来指定，我们简单模仿一下，只是单例 */public class BeanFactory { private static HashMap&lt;String, Object&gt; mapResult; public static HashMap&lt;String, Object&gt; getBean() { if (mapResult == null) { synchronized (BeanFactory.class) {//双重检查的单例，防止多线程访问时多次new对象 if (mapResult == null) { BufferedReader bf = null; String line = null; try { /** *下面这句代码通过流来读取资源包下面的配置文件，为了省去不必要的麻烦， * 我们没有用xml，而是用了properties */ InputStreamReader inputStreamReader = new InputStreamReader(BeanFactory.class.getClassLoader().getResourceAsStream(\"beans.properties\")); bf = new BufferedReader(inputStreamReader); mapResult = new HashMap&lt;&gt;(); while ((line = bf.readLine()) != null) {//每次仅读一行 if (\"\".equals(line)){//有可能读到换行时隔了一行（即只有一个换行符） continue; } String[] point = line.trim().split(\"=\");//按照等号拼接 if (point.length &gt; 2) { throw new Exception(\"beans文件格式不对！\"); } Object obj = Class.forName(point[1].trim()).newInstance();//反射实例化出目标对象 mapResult.put(point[0].trim(), obj);//然后以键值对的形式存入 } } catch (Exception e) { e.printStackTrace(); } } } } return mapResult; }} 代码块1 上面的类可以通过配置文件来实例化不同的对象，符合ioc最基本的思想，下面让我们来看看配置文件beans.properties的内容吧： 12userDao = sun.juwin.dao.impl.UserDaoImpluserDetailDao = sun.juwin.dao.impl.UserDetailDaoImpl 这里面只有两句话，指定dao层接口对象的实现类的路径，其实已经很接近spring的xml里对bean的配置了，只不过这里是properties文件，简化了许多 ② TransactionProxy代理类： 123456789101112131415161718192021222324252627282930313233343536package sun.juwin.proxy.transctional;import java.lang.reflect.InvocationHandler;import java.lang.reflect.Method;import java.lang.reflect.Proxy;import java.sql.Connection;/** * 事务代理类，通过这个类可以为要执行的方法加上事务管理 */public class TransactionProxy implements InvocationHandler { private Object targetObj; public Object getTargetObj(Object targetObj){ this.targetObj = targetObj; return Proxy.newProxyInstance(this.targetObj.getClass().getClassLoader(), this.targetObj.getClass().getInterfaces(), this); } /*下面这个方法会在被代理类执行方法时调用，拿到被代理类的要执行的method对象*/ @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { Object result = null; Connection connection = (Connection)args[0];//要求第一个参数必须是conn try{ connection.setAutoCommit(false);//开启事务 result = method.invoke(this.targetObj, args);//执行目标方法 connection.commit();//事务提交 System.out.print(\"commit success!\"); }catch (Exception e){ connection.rollback();//事务回滚 System.err.println(\"rollback!\"); e.printStackTrace(); }finally { connection.close();//关闭连接 System.out.println(\"connection closed!\"); } return result; }} 代码块2 说明：java在1.3版本的时候就为我们提供了一个用作代理类实现的接口InvacationHandler，通过实现这个接口可以很随意的写一个耦合度特别低的动态代理类（即这一个代理类可以代理任何类） ③ ConnBean，用来生成一个数据库连接对象，在不用连接池的情况下，我们用ThreadLocal进行封装，代码如下： 123456789101112131415161718192021222324package sun.juwin.db;import java.sql.Connection;import java.sql.DriverManager;/*原始产生数据库连接的类*/public class ConnBean { private static ThreadLocal conn = new ThreadLocal&lt;&gt;(); private ConnBean(){} public static Connection getConn(){ Connection connection = conn.get(); if(connection == null){ synchronized (ConnBean.class){//由于用到了ThreadLocal，因此该单例仅仅相对于当前线程是单例的 if(connection == null){ try{ Connection realConn = DriverManager.getConnection(\"jdbc:mysql://localhost:3306/db_useradd\", \"root\", \"\"); conn.set(realConn); }catch (Exception e){ e.printStackTrace(); } } } } return conn.get();//返回给当前线程一个Connection对象 }} 代码块3 以上就是核心的一些实现代码，下面让我们来看一下我们的业务吧： 实体类：User，UserDetail，要求添加一个User的同时要添加一个UserDetail User： 1234private Long id;private String userName;private String address;private int money; 代码块4 UserDetail： 123private Long id;private int age;private String realname; 代码块5 dao层的接口和实现： UserDao： 123public interface UserDao { public void save(User user, Connection conn)throws Exception;} 代码块6 UserDaoImpl： 123456789101112public class UserDaoImpl implements UserDao{ @Override public void save(User user, Connection conn) throws Exception { Statement statement = conn.createStatement();//为了省去不必要的麻烦，我们不用预编译语句 String sql = \"insert into tb_user (userName, address, money) values ('\" + user.getUserName() + \"', '\" + user.getAddress() + \"', \" + user.getMoney() + \")\"; statement.executeUpdate(sql); statement.close(); }} 代码块7 UserDetailDao： 123public interface UserDetailDao { public void save(UserDetail userDetail, Connection connection) throws Exception;} 代码块8 UserDetailDaoImpl： 12345678910public class UserDetailDaoImpl implements UserDetailDao { @Override public void save(UserDetail userDetail, Connection connection) throws Exception { Statement statement = connection.createStatement(); String sql = \"insert into user_detail (age, realname) values (\" +userDetail.getAge()+\", '\" +userDetail.getRealname()+\"')\"; statement.executeUpdate(sql); }} 代码块9 UserService： 123public interface UserService { public void saveService(Connection connection, User user) throws Exception;} 代码块10 UserServiceImpl： 123456789101112131415161718192021222324/** * 业务层 * juwin * 2015-12-04 */public class UserServiceImpl implements UserService { //下面的dao层实例由BeanFactory通过properties配置文件帮我们生成对应的实例对象 private UserDao userDao = (UserDao) BeanFactory.getBean().get(\"userDao\"); private UserDetailDao userDetailDao = (UserDetailDao) BeanFactory.getBean().get(\"userDetailDao\"); @Override public void saveService( Connection connection, User user)throws Exception { /** * 这个业务层方法执行了两个dao层方法，可以看做一个事务， * 任意一个dao层调用过程中如果发生异常，整个业务方法进行的所有dao层操作就会回滚 */ userDao.save(user, connection); /*要求在添加user的同时生产一个对应的detail，这里偷个懒，就自己new一个UserDetail对象吧*/ UserDetail userDetail = new UserDetail(); userDetail.setAge(22); userDetail.setRealname(\"juwin\"); userDetailDao.save(userDetail, connection); throw new Exception(\"拦-路-虎\");//这个异常是用来测试事务会不会回滚的，正常情况下不加这个 }} 代码块11 UserController： 12345678910111213141516171819202122/** * 程序入口，类似于controller层 */public class UserController { public void SaveUser(User user)throws Exception{ /** * 这一步很关键，为每一个执行这个操作的线程分配一个connection连接对象 * 说明：在实际web开发中客户端通过发送http请求到业务后台，这时候tomcat会为这次请求分配一个线程 * 因此就出现了并发甚至并行的现象，假象一下，我们如果只是利用单例写一个生成connection对象的方法， * 那么多线程并发访问的时候就有可能出现：线程1利用完connection对象将其状态修改为close，而此时线程2 * 也要用connection，这时候就会报“connection已经关闭”的异常 * 因此我们采用ThreadLocal，为单独一个线程生成一个单例的connection对象 */ Connection connection = ConnBean.getConn(); /** * 下面这个实例要加一层事务代理，就是让TransactionProxy这个代理类搅合一下， * 这样我们再利用service层对象调用任何方法时，都会加上事务管理了 */ UserService userService = (UserService) new TransactionProxy().getTargetObj(new UserServiceImpl()); userService.saveService(connection,user); }} 代码块12 测试类： 123456789101112public class UserAddTest { @Test public void Test1() throws Exception{ User user = new User(); user.setUserName(\"weixiaojie1993\"); user.setAddress(\"beijing\"); user.setMoney(1); UserController userController = new UserController(); userController.SaveUser(user); System.out.print(\"Done !\"); }} 代码块13 ok，大功告成了，现在让我们用junit来测试一下吧： service层不加： 1throw new Exception(\"拦-路-虎\"); 代码块14 执行结果： 可以看出来事务已经提交了，我们来看看数据库里面的变化： tb_user表： user_detail表： 然后在业务层加上： 1throw new Exception(\"拦-路-虎\"); 代码块15 运行结果： 仔细观察划绿色线的部分就能发现，事务已经回滚了，看数据库表也是没有记录的 我们主键id由于是递增的，因此我们还要确定一下事务是不是真的回滚了，我们把异常代码去掉，然后再往里面插入成功一次数据，运行后的数据库表记录如下： tb_user： user_detail： 大家仔细看id，已经是3了，说明原来事务成功回滚了 说明：其实connection对象不必每次都作为参数传递给方法，这里只是为了更清楚的展示connection的流向，其实我们用ThreadLocal封装成一个单例的时候就已经注定了本次访问（即当前线程从controller层调用到dao层）所有get到的connection对象都是同一个； 最后，个人感觉这个程序有个非常要命的地方，就是我要给service层加事务代理，这样就导致了sevice层的对象不能通过配置文件来实例化，正在纠结中。。以后还会优化，这只是简单实现以下，真正的spring要复杂的多得多，第一次在开源中国发表博客，以后也会多发一些，大家互相学习~","link":"/2015/12/04/%E7%AE%80%E5%8D%95%E6%A8%A1%E6%8B%9Fspring%E7%9A%84ioc%E5%92%8Caop/"},{"title":"java实现二叉树","text":"定义一个节点下面最多拥有两个子节点，并且两个子节点分为左值和右值，左值比父节点要小，右值比父节点要大 java实现下面，我们来利用java实现一棵如下图中的二叉树： 大家可以根据我的描述分析一下这棵二叉树 下面就来写代码实现这棵二叉树： 首先是要建立一个节点类Node： 123456789101112131415161718192021222324252627282930313233343536package Tree;/** * 节点类 * @author javadaodechengxuyuan * */public class Node { private long value; private Node leftNode;//节点下面的左节点 private Node RightNode;//节点下面的右节点 //构造器 public Node(long value){ this.value=value; } public long getValue() { return value; } public void setValue(long value) { this.value = value; } public Node getLeftNode() { return leftNode; } public void setLeftNode(Node leftNode) { this.leftNode = leftNode; } public Node getRightNode() { return RightNode; } public void setRightNode(Node rightNode) { RightNode = rightNode; }} 代码块1 这是二叉树类，就是这个类用来操作节点类的： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051package Tree;/** * @author javadaodechengxuyuan * 二叉树：每个节点有最多两个分叉， * 分别作为父节点的左值和右值，遵循左小右大的规则进行分叉 */public class Tree { private Node root; private Node current; private Node parent; /** * @author javadaodechengxuyuan * 为一颗二叉树添加节点 */ public void insert(long value){//为二叉树插入新的节点 //创建新的节点 Node newNode=new Node(value); //创建完后就该考虑把这个节点放在哪里了，下面这些代码就是用来判断将这个节点放在哪里的 if(root==null){ this.root=newNode;//如果root为空，那么第一次调用添加时应给root初始化 }else{ this.current=root;//初始化current while(true){//进入死循环，一直等到给newNode找到合适的位置时进行终止死循环 if(this.current.getValue()&gt;value){//比root小，放在左侧 this.parent=this.current;//让parent一直保留本次的current this.current=this.current.getLeftNode(); if(this.current==null){//如果当前的左值为空，那么就终止循环并赋值给这个左值 this.parent.setLeftNode(newNode);//将这个新节点放在这个位置 return;//最终找到合适位置，死循环终止 } }else{//比root大，放在右侧 this.parent=this.current;//让parent一直保留本次的current this.current=this.current.getRightNode();//将当前的节点重新赋值给下一次需要比较的节点 if(this.current==null){//如果当前的右值为空，那么就终止循环并赋值给这个左值 this.parent.setRightNode(newNode);//将这个新节点放在这个位置 return;//最终找到合适位置，死循环终止 } } } } } public Node getRoot() { return root; } public void setRoot(Node root) { this.root = root; }} 代码块2 这是测试类： 12345678910111213141516171819202122package Tree;/** * 测试类 * @author javadaodechengxuyuan * */public class Test { public static void main(String args[]){ Tree t=new Tree(); t.insert(10);//根节点 t.insert(20); t.insert(15); t.insert(9); t.insert(35); System.out.print(t.getRoot().getValue()+\"、\");//第0层：根节点 System.out.print(t.getRoot().getLeftNode().getValue()+\"、\");//第一层左值 System.out.print(t.getRoot().getRightNode().getValue()+\"、\");//第一层右值 System.out.print(t.getRoot().getRightNode().getLeftNode().getValue()+\"、\");//第二层左值 System.out.print(t.getRoot().getRightNode().getRightNode().getValue());//第二层右值 //输出结果应为：10、9、20、15、35 }} 代码块3 输出结果应该为： 110、9、20、15、35 这只是简单的插入功能，下一节我会写如何查找二叉树的节点以及删除节点、还有如何遍历一棵二叉树","link":"/2014/07/04/java%E5%AE%9E%E7%8E%B0%E4%BA%8C%E5%8F%89%E6%A0%91/"},{"title":"利用java实现一个简单的链表结构","text":"定义所谓链表就是指在某节点存储数据的过程中还要有一个属性用来指向下一个链表节点，这样的数据存储方式叫做链表 链表的优缺点优点：易于存储和删除 缺点：查询起来较麻烦 java实现下面我们用java来实现如下链表结构： 首先定义节点类： 123456789101112131415161718192021222324252627282930package LinkTest;/** * 链表节点类 * @author admin * */public class Node { private int value;//存储数据 private Node next;//下一个节点 /** * 定义构造器 * @param vlaue * @param value */ public Node(int value){ this.value=value; } public int getValue() { return value; } public void setValue(int value) { this.value = value; } public Node getNext() { return next; } public void setNext(Node next) { this.next = next; }} 代码块1 然后定义一个链表类： 注意：遍历链表定义了两个方法，一个是普通方法，一个是递归方法，都可以遍历出来 12345678910111213141516171819202122232425262728293031323334353637383940414243444546package LinkTest;/** * 链表 * @author admin * */public class Link { private Node current; private Node root; public void insert(int vlaue){ Node newNode=new Node(vlaue); if(this.current==null){ this.current=newNode; this.root=this.current; }else{ this.current.setNext(newNode); this.current=this.current.getNext(); } } //普通遍历 public void getList(){ this.current=this.root; while(this.current!=null){ System.out.print(this.current.getValue()); this.current=this.current.getNext(); if(this.current!=null){ System.out.print(\"-------&gt;\"); } } } //递归遍历 public void getList2(){ DG(this.root); } //递归方法 public void DG(Node node){ System.out.print(node.getValue()+\"-----&gt;\"); if(node.getNext()!=null){ DG(node.getNext()); }else{ return; } }} 代码块2 测试类： 123456789101112131415161718package LinkTest;/** * 测试类 * @author admin * */public class Test { public static void main(String[] args){ Link l=new Link(); l.insert(1); l.insert(4); l.insert(5); l.insert(6); l.insert(9); l.insert(8); l.getList(); }} 代码块3 测试类运行结果： 11-------&gt;4-------&gt;5-------&gt;6-------&gt;9-------&gt;8 这样我们就用java实现了一个简单的链表结构。","link":"/2014/07/04/%E5%88%A9%E7%94%A8java%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E9%93%BE%E8%A1%A8%E7%BB%93%E6%9E%84/"},{"title":"济源游记-20200517","text":"⏱ 旅行时间线：2020-05-15 ~ 2020-05-17 🌏 地点：河南省 · 济源市 🌿 景点：黄河三峡、王屋山 ⚜️ 级别：AAAA 出发（2020.05.15） 留宿（2020.05.15） 黄河三峡照片墙（2020.05.16） 老街照片墙（2020.05.16） 王屋山照片墙（2020.05.17）","link":"/2020/06/08/%E6%B5%8E%E6%BA%90%E6%B8%B8%E8%AE%B0/"},{"title":"JVM基础回顾记录（四）：类文件","text":"一、JAVA的平台无关性我们最开始学习java的时候了解到java代码文件必须经过javac的编译，生成字节码文件才可以被JVM识别并运行，而JVM则拥有多种操作系统的版本： 正是因为这些不同操作系统下的JVM版本，使得我们被编译后的字节码文件可以被不同操作系统上的JVM识别并运行，继而实现跨平台。 二、字节码文件组成字节码文件是经过编译器编译后的class文件，所以也可以称为Class类文件，这个文件内的内容你是看不懂的，它是以某种顺序堆叠的二进制流组成的文件，以字节（byte）为基本单位，我们学习java基础时知道，单字节占有8个bit位，虽然你很难看懂，但对于JVM来说却可以很容易的解析这类文件，这里先不谈它如何解析，我们本次只说明这个文件里都放了些什么东西，接下来会结合实例来详细说明。 上面刚说过，文件内部的基本单位是字节（8bit），利用这些字节数据所处的先后顺序，来决定Class文件里不同的数据项，JVM在做字节码加载分析的时候也是按照这个顺序来进行的，这就像是一种序列化的方式，而JVM加载解析它的时候，相当于在做反序列化，这个在本节不做探讨，留到类加载章节会详细说明。 字节码文件里存储的数据类型分为两大类，一种是基本的无符号数字类型，另外一种则是类似C语言中结构体的一种东西，我们管它叫表，表可能包含无符号数字，也可能包含另外一个表，也可能由无符号数字和其他表共同组成（其实也很像java里的类对吧？可能只包含基本类型的属性，也可能包含另外一个类的对象属性，虽然它跟类完全不是一回事，但为了便于理解，可以这样想） 由于字节码文件的“反序列化”过程非常的单纯，就是从前往后读，那么Class文件中的无符号数和表的堆叠顺序就显得非常重要，这其实是一种java虚拟机约定好的协议，比如Class文件读进来的二进制流，前4个字节一定是某个具体的字段，紧接着往后2个字节一定是另外一个字段，就这样，这个二进制字节流被协议切分成了具体的组，每一个组都代表着不同含义的字段。 三、前三个字节组：魔数、次版本号、主版本号通过第二节的了解，我们知道了Class文件就是字节与字节的顺序堆叠排列，然后按照字节码约定协议进行以字节为单位分组，每一组的数据代表着不同的含义，接下来让我们看下字节码文件的头部三个组都分别代表什么吧： 上图展示的就是Class文件开头几个字节被约定分组的情况，其实后面更复杂的表也是按照类似的方式做的。 无符号数按照不同数量的字节分组，形成了不同的数据类型，需要占用4个字节才能存下的字段的类型被称为u4，而只需要占用2个字节的字段类型被称为u2，图2中“魔数”就可以被说成是一个u4类型的数字，而次版本号和主版本号则为u2。 魔数（magic）：确定当前文件是否是一个可以被jvm加载的Class文件（像mp3、pdf等文件，开头一样也会有类似的魔数） 主版本号（major_version）&amp;次版本号（minor_version）：用来记录当前Class文件的版本号，每个版本的jdk编译过的Class文件，会保有其版本号信息，学习java基础的时候都了解过，java是自上向下兼容的，比如jdk1.8编译出来的Class文件不可以被1.7版本的虚拟机加载运行，但jdk1.7编译出来的Class文件却可以被1.8版本的虚拟机加载运行，至于能不能被加载的第一道坎，就是按照这俩版本号进行判断的。 四、常量池图2里我们至少已经知道了魔数、次版本号、主版本号这三个字段，往后的则被省略了。往后是什么呢？它是非常复杂且庞大的一个分组集合，被称为常量池（注意，这个常量池是指Class文件内的常量池，他们会被Class文件内一些索引项给索引到，准确的说并不是运行期的那个JVM方法区内的运行时常量池，但随着JVM的类加载，类里的这个常量池会被加载进运行时常量池，顺便说下，到了那个时候，很多符号引用也会转变为直接引用）它们位于主版本号后面，第一项是一个u2类型的数字，表示有几个常量表如图： 4.1：池里常量的分类常量池里的常量按照类型被分为了两大类：字面量和符号引用，而符号引用又往下细分了几个分类，如图： 4.1.1：字面量很简单的概念，类似于常量，下面的分节将会详细介绍，主要是类似语言层面的基本类型，比如int、float、double这些，都有对应的字面量常量表：CONSTANT_Integer_info、CONSTANT_Float_info、CONSTANT_Double_info 4.1.2：符号引用符号引用都是以字符串的方式存储在常量池里的，它们通常用来描述类的全限定名、方法和字段的名称以及描述符，因此被分了三类. 类和接口的全限定名第一类叫全限定名，例如，一个类叫：com.bilibili.test.Test，则它的全限定名为：com/bilibili/test/Test 字段、方法的简单名称和描述符相对来说，字段和方法的描述符更为复杂，我们得通过实际的例子来说明问题。 我们知道，一个类里面有字段以及方法，字段有它自己声明的类型以及名称，而方法则更加复杂，它存在入参、返回、名称，因此要想用描述符这样的字符串来描述一个字段或者一个方法，往往需要一定的格式，让我们来看看其格式吧。 先说字段，假如我的类里有个字段是int型，名称为num，名称没什么好说的就是num，但它的类型描述符却是：I 为什么int型的数据的类型描述符是I？那double呢？下面我们就列出所有类型对应的描述符的映射关系表，后面更复杂的方法描述符也可以用到这个表： 类型 描述符 byte B char C double D float F int I long J short S boolean Z reference（引用类型） L，格式为L类型全限定名;，例如Object类型的引用变量可表示为：Ljava/lang/Object; array（数组） [，格式为：[类型描述符，比如int[]可以表示为[I，再比如Object[]可以表示为[Ljava/lang/Object;，高维数组，只需要多加一个[即可，比如int[][]可以表示为[[I void（方法描述符独有，用于形容方法的无参数返回类型） V 表1 ok，了解完各种类型的描述符规则，再来一遍开始那个字段描述符为I就可以很容易理解了，下面来继续了解下方法的描述符，这个就更为复杂，其格式为： 1(参数1类型描述符 参数2类型描述符...)返回值类型描述符 下面通过几个例子来说明方法描述符，希望可以帮助你理解： 方法定义 描述符 int getId() ()I double getPrice(Object[] o, double price) ([Ljava/lang/Object;D)D void setId() ()V void setId(int id) (I)V 表2 那么字段或方法的名称及描述符存在哪个地方呢？接下来要介绍的一个叫做CONSTANT_NameAndType_info的常量表就是专门存放这俩数据的，它的内部结构请参考``里对它的介绍。 常量池里的每一项都是个表，下面，我们来深层次探讨下常量池里的每一个表的结构~ 4.2：CONSTANT_Utf8_info（UTF-8编码的字符串）这个表的详细结构如下： 即便是表，也是按照类似的方式堆叠的，后续几种类型也是类似。 基本上常量池里的表第一个字节都是tag，tag在图4中已经解释过了，它用来区分当前表属于什么类型的表，比如本节里tag=1，规范里tag为1的表就是CONSTANT_Utf8_info，当虚拟机知道了表为CONSTANT_Utf8_info，那么很自然的后面两个字节肯定是length，至于其内容，肯定就是length后面的字节了，然后截取length长度的字节，就是这个CONSTANT_Utf8_info所存放的实际内容。 4.3：CONSTANT_Integer_info（整型字面量）直接上图： 学过java的人都知道int型数据占4字节，32位，现在通过上图字节码的分配可以证实，下面的字面量也大体相同。 4.4：CONSTANT_Float_info（浮点型字面量） 4.5：CONSTANT_Long_info（长整型字面量） 4.6：CONSTANT_Double_info（双精浮点型字面量） 到这里，基本类型就介绍完了，通过对2.3.2 ~ 2.3.5的了解，可以发现，基本类型是很简单的类型，而且其实际内容符合java里基本类型所占位数。 4.7：CONSTANT_Class_info（类、接口的符号引用） 这里要详细介绍下index这个属性，因为在后面的表中，这个字段出现的频率会非常高。 这个字段代表了一个内容的索引，它索的谁的引呢？答案还是常量池，举个例子吧，Class_info是一个用来描述类或者接口全限定名的表，既然是全限定名，那肯定是个字符串，那么index指向的肯定是常量池里一张CONSTANT_Utf8_info的表，现在让我们假设一个常量池，里面已经排列好了各种表数据，按照常量池索引值从1开始，因此按照规则，绘制出下图： 当Class_info的index属性就是用来指向常量池中某一个表的，例如上图里index=2，则意味着常量池里索引下标为2的表的内容，就是Class_info的内容，开头说过，就是类或接口的全限定名。 4.8：CONSTANT_String_info（字符串类型字面量） 4.9：CONSTANT_NameAndType_info（字段、方法的部分符号引用）之前介绍了常量池里存放的字段、方法都是存在名称以及描述符的，本节介绍的这个结构就是用来存放这两项内容的（对字段或方法的名称及描述符不熟悉的话，建议加强理解下图4、表1、表2里的内容） 该结构存储了某个字段或方法的名称索引和描述符索引，那肯定有具体的某个字段或方法的表里会索引向它，接下来要介绍的CONSTANT_Fieldref_info和CONSTANT_Methodref_info，均有指向它的索引字段。 4.10：CONSTANT_Fieldref_info（类中引用字段的符号引用）这个常量专门用来描述类内被引用到的属性，包含你自定义的出现在该类的属性，也包含该类里方法调用时使用的别的类的属性。 字段的符号引用表，除了要描述声明自己的类或接口，还需要索引到具体的CONSTANT_NameAndType_info，这个表里上面介绍过，内含一个名称和具体的描述符，下方的方法符号引用也是同样的结构，即一个字段或方法的符号引用等于：声明该字段或方法的类或接口的全限定名 + 该字段或方法的名称 + 该字段或方法的描述符 4.11：CONSTANT_Methodref_info（类中引用方法的符号引用）这个常量专门用来描述类内被引用到的方法，比如你在A类里定义了一个叫test()的方法，这个test方法并不会在常量池存在一个CONSTANT_Methodref_info，如果你再定义一个方法test2()，让它调用test方法，这时test方法相当于被引用，这时就拥有了对应的Methodref_info. 再比如，你方法里经常会调用一些依赖包的类方法，比如最常用的System.out.println，这个过程相当于你使用了out对象的println方法，此时当前类的字节码常量池便会有一个println的Methodref_info出现，同时也会有一个叫out的Fieldref_info出现。（这些均会在实战篇讲解） 纠错，图里第一个info块里的内容应该是“Methodref_info的tag值为10” 4.12：CONSTANT_InterfaceMethodref_info（接口中方法的符号引用） 截止到目前，常见的11种常量已经介绍完了，下面再介绍3种JDK1.7引入的新常量。 4.13：CONSTANT_MethodHandle_info（表示方法句柄）这个常量是1.7新增的特性，即方法句柄，可以和下方MethodType结合使用，用法这里暂时不说，你可以简单理解，它是类似反射的功能，可以指定调用哪个对象的哪个方法。但不同于反射，它可以在编译期就指定好方法调用，而不是运行期，这相比反射要安全的多。 4.14：CONSTANT_MethodType_info（标识方法类型）待补全😂 4.15：CONSTANT_InvokeDynamic_info（动态方法调用点）待补全😂 五、访问标志紧挨着常量池后面的两个字节，代表访问标志（access_flags），因为其具备2个字节，所以它有16个bit位可以利用，每个位置的0或1代表不同的含义，当前只定义了8个，如下： 六、继承关系接着access_flag后面，有描述本类继承关系的几个变量，分别如下： 学过java的人都知道，java类允许单继承和多实现，因此图中super_class只有一个，interface却对应了一个集合，当然，它们都只是u2类型的索引而已，指向常量池里的CONSTANT_Class_info（参考4.7） 七、字段表集合我们离开了继承关系后，紧挨着的就是字段表集合。字段表，即类（或接口）里声明的变量，变量分为静态变量（类变量）以及成员变量（实例变量）。 现在详细介绍下图中虚线部分，这四个u2类型的字段以及属性表（attribute_info）集合代表了一个field_info表，它的这些字段解释如下： 7.1：access_flags这个字段也叫access_flags，跟之前Class本身的access_flags所具备的意义一样，它是用来描述字段的访问标志，让我们来看看它每一位代表什么意思吧： 7.2：name_index &amp; descriptor_indexname_index和descriptor_index都是索引值，前者指向常量池里一个CONSTANT_Utf8_info，用来表示该字段的简单名称，后者也是指向常量池里一个CONSTANT_Utf8_info，但它用来表示字段的描述符（什么是简单名称，什么是描述符？请参考4.1.2） 7.3：attributes_count &amp; attribute_infos这段信息很复杂，代表属性表，attributes_count代表后面跟几个属性表，attribute_infos代表属性表集合，属性表是很复杂的一块内容，Class文件、字段表、方法表，甚至Code属性表都可以携带自己的属性表集合，属性表的种类很繁多，在9.1节会列举一个与本节（方法表）相关的属性表ConstantValue的结构。 八、方法表集合离开字段表后，紧挨着的就是方法表集合，它几乎跟字段表结构一致。方法表，即类（或接口）里声明的方法，同样的，方法分为静态方法（类方法）以及成员方法（实例方法）。 同样的，来介绍下图中虚线部分，这四个u2类型的字段以及属性表（attribute_info）集合代表了一个method_info表，它的这些字段解释如下： 8.1：access_flags不多说了，前面遇到很多次了，它用来描述方法的访问标志，让我们来看看它每一位代表什么意思吧： 8.2：name_index &amp; descriptor_index跟字段表对应的字段作用相同，name_index和descriptor_index都是索引值，前者指向常量池里一个CONSTANT_Utf8_info，用来表示该方法的简单名称，后者也是指向常量池里一个CONSTANT_Utf8_info，但它用来表示方法的描述符（什么是简单名称，什么是描述符？请参考4.1.2） 8.3：attributes_count &amp; attribute_infos属性表，在7.3节提到过，这里的俩属性跟字段表里的意思一样，但在本节你可能会好奇，方法表定义了方法的名称和描述符，那么方法的代码跑哪里去了？其实方法本身的代码会被编译成字节码指令，存放在每个方法后面的某个属性表里，而这个属性表就是Code，在后续的9.2里会详细介绍，这里只需要记住它是出现在本节（方法表）这个位置的即可。 九、属性表前面两节或多或少提到过属性表，这是很复杂也很重要的一个表结构，下面来看下它的基本组成： 不管属性表多么复杂，它开始的两个属性总是attribute_name_index和attribute_length，具体含义如图所示。 本节只少量介绍几个很重要的属性表，更详细的属性表可以查阅https://docs.oracle.com/javase/specs/jvms/se7/html/jvms-4.html#jvms-4.7 9.1：ConstantValue先回看下7.3，该属性表仅作用在字段表上，用来表示一个static final字段的初始值，且要求该字段必须是基本类型或String类型才可以用该表表示初始值，为什么？我们来看下该表的结构： 因为constantvalue_index指向的一定常量池里某字面量，因此它只能是CONSTANT_Long_info、CONSTANT_Float_info、CONSTANT_Double_info、CONSTANT_Integer_info、CONSTANT_String_info里的一种，所以只有被声明为static final的基本类型和String类型的字段，才可能存在此属性表，用来代表其初始化值。 9.2：Code这绝对是最高能的一个属性表，我们详细来解析下，从它的名字就可以看出来，它代表的是一个方法的代码属性，属于方法表（这点请牢记），这在8.3里提到过，我们写的程序代码最终会被javac编译成字节码指令，而这些指令就存储在Code属性表内，它的结构是怎样的呢？看图： 这里面有很多细节，我们来逐一介绍下，就从max_stack开始吧~ 9.2.1：max_stack这个值跟操作数栈有关系，代表它的最大深度，当jvm处于运行期的时候，会根据这个值来分配栈帧中操作数栈的深度。（栈帧是后续文章将会介绍的概念，运行期时每一个Code模块，都会与之对应一个栈帧结构，可通过10.2.5.1节了解其基本概念） 9.2.2：max_locals这个值也是围绕着运行期栈帧来生效的，它代表栈帧里局部变量表所需的存储空间。什么是局部变量表？早期接触java时就知道，每个方法内产生的对象、变量我们都称之为局部变量，它们会在运行期被暂存入方法所属栈帧里的局部变量表中，方便操作数栈存取与操作，而它需要多少存储大小呢？就是靠这个值来决定的，结合9.2.1可以知道，一个java方法的局部变量表大小、操作数栈最大深度都是在编译期就已经确定了的。 那么，存储大小的单位又是什么呢？是字节（byte）吗？比如max_locals为5，代表的是5byte的存储空间吗？ 并不是，它是一个叫做Slot的东西，max_locals=5，代表需要分配5个Slot。下面来介绍下Slot。 Slot有32个bit位，意思就是说每个Slot占4字节，因此，如果你有一个局部变量是int型，那么它就占1个Slot空间，如果是float或者long之类的，那就需要两个Slot。 那现在我们就可以很容易推算出一个方法的局部变量表的容量到底需要多大了： 总Slot量 = 所有局部变量所用的字节量 / 4 但是，这样是不对的！因为根据每个局部变量的作用域不同，Slot是可以被复用的，这点很重要，让我们来看个例子： 代码块112345678910111213public int slot(int a) { //入参a，为a分配一个Slot int b = 1; //为b分配一个Slot if (a == 1) { int c = a + b; //为c分配一个Slot，但是注意，c的作用域只有if域 b = c; } //离开if域，为c分配那个Slot会被完全闲置出来，因为c在之后的逻辑中再也无法使用了 int d = b; //反正c的那个Slot闲着也是浪费，不如给接下来的变量用，这里d就可以复用c的Slot return d;} 这段代码一共有a、b、c、d四个int型变量，理论上需要分配4个Slot存储，但实际只分配了3个，具体原因请参考代码中的注释，这种利用java语法来节省Slot的做法还是相当聪明的。 9.2.3：字节码指令参考图25，离开了max_stack和max_locals，紧接着就进入了一个非常冗长的区域，那就是code区域，它是你写的java程序被翻译成的一个个指令码，是最重要的一部分，也是直接可以被java程序员操控的部分。 字节码指令大全：https://luisstruggle.github.io/blog/javaSE7-JVM.html 后续会以某个实例的字节码指令集来分析这些指令是如何在栈帧里运行的（虽然是运行期的东西，但还是想在这一节做下简单的说明） 十、实战10.1：快速回顾终于把字节码的详细内容讲完了，太多太长，上面是详细介绍，下面就通过一张图简单回顾一下字节码文件的堆叠顺序和内容种类： 10.2：字节码分析实战10.2.1：源代码本篇会通过以下实例来具体解析字节码组成： 代码块212345678910111213141516171819202122232425262728293031323334353637383940package sun.juwin.test;public class Test { public static final int j; //声明一个类变量j，其在静态块里进行初始化 public static final int k = 5; //再声明一个类变量k，并直接初始化为5 private int i; //包含一个成员变量 static { //静态块，相当于类本身的构造器 j = 6; } public Test() { //包含两个重载构造器 System.out.println(\"Test init !\"); //注意，这里调用了out对象的println方法 } public Test(int i) { this(); //在这里主动触发一次无参构造器 this.i = i; //给i属性赋值 } public static int getK() { //类方法，返回类属性k的值（静态方法的入参不会隐式传入this作为参数） return k; } public int sum_i(int a, int b) { //成员方法，这个方法传俩值，返回i和这俩值之和的和（成员方法的入参会隐式传入this作为参数） return i + sum(a, b); } public int sum(int x, int y) { //成员方法，计算两个数的和 return x + y; } public int sum2(int x, int y) { //成员方法，作用跟上面的sum一样，但这个方法会多出一个Slot来存result这个局部变量 int result = x + y; return result; }} 10.2.2：字节码信息首先我们利用javac将其编译成字节码文件（我这里使用jdk11版本做的编译），然后利用javap指令，输出其字节码信息： javac&amp;javap指令12javac -g Test.java //编译(-g会帮忙把调试信息也编译进去，为了说明Slot，我们需要这个)javap -v -p Test //输出字节码详细信息，-v表示输出详细信息，-p表示输出所有的字段和方法(不加这个的话public以下的字段方法无法展示) javap这个指令是干嘛的呢？它实际上是将编译好的字节码文件反编译，然后输出字节码文件里各数据项的详细信息，相比直接去读二进制的字节码文件，javap输出的字节码详细信息更加通俗易读。 现在来看下代码块2的字节码详细信息（太冗长了，后面会拆解说明）： 代码块3123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199 Compiled from \"Test.java\"public class sun.juwin.test.Test minor version: 0 major version: 55 flags: (0x0021) ACC_PUBLIC, ACC_SUPER this_class: #7 // sun/juwin/test/Test super_class: #10 // java/lang/Object interfaces: 0, fields: 3, methods: 7, attributes: 1Constant pool: #1 = Methodref #10.#39 // java/lang/Object.\"&lt;init&gt;\":()V #2 = Fieldref #40.#41 // java/lang/System.out:Ljava/io/PrintStream; #3 = String #42 // Test init ! #4 = Methodref #43.#44 // java/io/PrintStream.println:(Ljava/lang/String;)V #5 = Methodref #7.#39 // sun/juwin/test/Test.\"&lt;init&gt;\":()V #6 = Fieldref #7.#45 // sun/juwin/test/Test.i:I #7 = Class #46 // sun/juwin/test/Test #8 = Methodref #7.#47 // sun/juwin/test/Test.sum:(II)I #9 = Fieldref #7.#48 // sun/juwin/test/Test.j:I #10 = Class #49 // java/lang/Object #11 = Utf8 j #12 = Utf8 I #13 = Utf8 k #14 = Utf8 ConstantValue #15 = Integer 5 #16 = Utf8 i #17 = Utf8 &lt;init&gt; #18 = Utf8 ()V #19 = Utf8 Code #20 = Utf8 LineNumberTable #21 = Utf8 LocalVariableTable #22 = Utf8 this #23 = Utf8 Lsun/juwin/test/Test; #24 = Utf8 (I)V #25 = Utf8 getK #26 = Utf8 ()I #27 = Utf8 sum_i #28 = Utf8 (II)I #29 = Utf8 a #30 = Utf8 b #31 = Utf8 sum #32 = Utf8 x #33 = Utf8 y #34 = Utf8 sum2 #35 = Utf8 result #36 = Utf8 &lt;clinit&gt; #37 = Utf8 SourceFile #38 = Utf8 Test.java #39 = NameAndType #17:#18 // \"&lt;init&gt;\":()V #40 = Class #50 // java/lang/System #41 = NameAndType #51:#52 // out:Ljava/io/PrintStream; #42 = Utf8 Test init ! #43 = Class #53 // java/io/PrintStream #44 = NameAndType #54:#55 // println:(Ljava/lang/String;)V #45 = NameAndType #16:#12 // i:I #46 = Utf8 sun/juwin/test/Test #47 = NameAndType #31:#28 // sum:(II)I #48 = NameAndType #11:#12 // j:I #49 = Utf8 java/lang/Object #50 = Utf8 java/lang/System #51 = Utf8 out #52 = Utf8 Ljava/io/PrintStream; #53 = Utf8 java/io/PrintStream #54 = Utf8 println #55 = Utf8 (Ljava/lang/String;)V{ public static final int j; descriptor: I flags: (0x0019) ACC_PUBLIC, ACC_STATIC, ACC_FINAL public static final int k; descriptor: I flags: (0x0019) ACC_PUBLIC, ACC_STATIC, ACC_FINAL ConstantValue: int 5 private int i; descriptor: I flags: (0x0002) ACC_PRIVATE public sun.juwin.test.Test(); descriptor: ()V flags: (0x0001) ACC_PUBLIC Code: stack=2, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object.\"&lt;init&gt;\":()V 4: getstatic #2 // Field java/lang/System.out:Ljava/io/PrintStream; 7: ldc #3 // String Test init ! 9: invokevirtual #4 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 12: return LineNumberTable: line 15: 0 line 16: 4 line 17: 12 LocalVariableTable: Start Length Slot Name Signature 0 13 0 this Lsun/juwin/test/Test; public sun.juwin.test.Test(int); descriptor: (I)V flags: (0x0001) ACC_PUBLIC Code: stack=2, locals=2, args_size=2 0: aload_0 1: invokespecial #5 // Method \"&lt;init&gt;\":()V 4: aload_0 5: iload_1 6: putfield #6 // Field i:I 9: return LineNumberTable: line 20: 0 line 21: 4 line 22: 9 LocalVariableTable: Start Length Slot Name Signature 0 10 0 this Lsun/juwin/test/Test; 0 10 1 i I public static int getK(); descriptor: ()I flags: (0x0009) ACC_PUBLIC, ACC_STATIC Code: stack=1, locals=0, args_size=0 0: iconst_5 1: ireturn LineNumberTable: line 25: 0 public int sum_i(int, int); descriptor: (II)I flags: (0x0001) ACC_PUBLIC Code: stack=4, locals=3, args_size=3 0: aload_0 1: getfield #6 // Field i:I 4: aload_0 5: iload_1 6: iload_2 7: invokevirtual #8 // Method sum:(II)I 10: iadd 11: ireturn LineNumberTable: line 29: 0 LocalVariableTable: Start Length Slot Name Signature 0 12 0 this Lsun/juwin/test/Test; 0 12 1 a I 0 12 2 b I public int sum(int, int); descriptor: (II)I flags: (0x0001) ACC_PUBLIC Code: stack=2, locals=3, args_size=3 0: iload_1 1: iload_2 2: iadd 3: ireturn LineNumberTable: line 33: 0 LocalVariableTable: Start Length Slot Name Signature 0 4 0 this Lsun/juwin/test/Test; 0 4 1 x I 0 4 2 y I public int sum2(int, int); descriptor: (II)I flags: (0x0001) ACC_PUBLIC Code: stack=2, locals=4, args_size=3 0: iload_1 1: iload_2 2: iadd 3: istore_3 4: iload_3 5: ireturn LineNumberTable: line 37: 0 line 38: 4 LocalVariableTable: Start Length Slot Name Signature 0 6 0 this Lsun/juwin/test/Test; 0 6 1 x I 0 6 2 y I 4 2 3 result I static {}; descriptor: ()V flags: (0x0008) ACC_STATIC Code: stack=1, locals=0, args_size=0 0: bipush 6 2: putstatic #9 // Field j:I 5: return LineNumberTable: line 12: 0 line 13: 5}SourceFile: \"Test.java\" 太长了，我们来将上面的字节码信息拆解一下进行详细分析。 10.2.3：字节码-头部代码块4 头部12345678 Compiled from \"Test.java\"public class sun.juwin.test.Test minor version: 0 //这是次版本号 major version: 55 //这是主版本号 flags: (0x0021) ACC_PUBLIC, ACC_SUPER //这是访问标志，将0x0021转换成二进制，再跟图17一一对应，就可以得出ACC_PUBLIC和ACC_SUPER的结论 this_class: #7 //#7是个常量池索引，其值为sun/juwin/test/Test，代表该类的全限定名 super_class: #10 //#10也是个常量池索引，其值为java/lang/Object，代表该类的父类全限定名，默认为Object，如果真的继承了其他父类，则该值为对应父类的全限定名 interfaces: 0, fields: 3, methods: 7, attributes: 1 //该类实现了0个接口，共有3个字段（i、j、k），7个方法（静态块、Test()、Test(int i)、getK、sum_i、sum、sum2），1个attributes？？这个我自己没搞懂，如果指代属性表，那远不止1个，如果不是指属性表，那是什么意思呢？ 10.2.3：字节码-常量池代码块5 常量池123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657Constant pool: //下面就是大杂烩的常量池了，#xx代表一个索引，所以看得时候需要找序号的按照这个值找就行了。//这里说一下XXXref，在4.10和4.11介绍过，它们是指在该类内被引用到的字段或方法，因此像System.out就是一个FieldRef，而println就是一个MethodRef，同样的在sum_i方法里有对sum方法的调用，因此sum方法也是一个MethodRef #1 = Methodref #10.#39 //由于该类构造器被调用时会触发其父类构造器的调用，因此父类构造器是一个MethodRef，其值为：java/lang/Object.\"&lt;init&gt;\":()V（前面为该方法所属类的全限定名，后面为该方法的名称和描述符，#39可以往下找，是个NameAndType） #2 = Fieldref #40.#41 //一个被该类引用的属性，这个找法类似上面的MethodRef，其值为：java/lang/System.out:Ljava/io/PrintStream;即：Test()方法里的System.out属性 #3 = String #42 //String字面量，指向内容为\"Test init !\"的Utf8_info #4 = Methodref #43.#44 //值为java/io/PrintStream.println:(Ljava/lang/String;)V，因为println被我们所引用，所以它也成为了该类的一个MethodRef #5 = Methodref #7.#39 //该类的一个无参构造器，值为sun/juwin/test/Test.\"&lt;init&gt;\":()V，因为我们的有参构造器引用了它，所以它也是MethodRef，下面的各类Ref以此类推 #6 = Fieldref #7.#45 //值为sun/juwin/test/Test.i:I #7 = Class #46 //该类的全限定名，指向的值为sun/juwin/test/Test #8 = Methodref #7.#47 //值为sun/juwin/test/Test.sum:(II)I #9 = Fieldref #7.#48 //值为sun/juwin/test/Test.j:I #10 = Class #49 //父类的全限定名，它已经被#1引用过一次了，值为java/lang/Object #11 = Utf8 j //字段j的简单名称文本值，应被某个字段表索引，参考7.2 #12 = Utf8 I //字段的描述符文本值，同样应被某个字段表索引，参考7.2，只需要一个I即可，因为本类所定义的属性只有int一个类型 #13 = Utf8 k //字段k的简单名称文本值 #14 = Utf8 ConstantValue //属性表名称文本值，应被某个属性表索引，参考9.1 #15 = Integer 5 //某Integer字面量的值，本例中被属性k的ConstantValue索引，参考9.1 #16 = Utf8 i //字段i的简单名称文本值 #17 = Utf8 &lt;init&gt; //该类构造方法的简单名称文本值，应被某个方法表索引，参考8.2 #18 = Utf8 ()V //某方法的描述符文本值，()V代表无参无返回值（描述符规则参考4.1.2），应被某个方法表索引，参考8.2 #19 = Utf8 Code //属性表名称文本值，应被某个属性表索引，参考9.2 #20 = Utf8 LineNumberTable //属性表名称文本值，应被某个属性表索引，LineNumberTable是Code属性表的一部分，参考9.2 #21 = Utf8 LocalVariableTable //属性表名称文本值，应被某个属性表索引，LocalVariableTable是Code属性表的一部分，参考9.2 #22 = Utf8 this //字段this的简单名称（this属于隐式字段） #23 = Utf8 Lsun/juwin/test/Test; //字段描述符文本值，所有类型为sun/juwin/test/Test类型的字段，都可以使用该值作为自己的描述符 #24 = Utf8 (I)V //某个入参为一个int型无返回值的方法描述符文本值，本例子指Test(int i)方法 #25 = Utf8 getK //getK方法的简单名称文本值 #26 = Utf8 ()I //某个无参返回int数据的方法的描述符文本值，本例可以指代getK()方法 #27 = Utf8 sum_i //sum_i方法的简单名称文本值 #28 = Utf8 (II)I //某个入参为两个int型，返回一个int型值的方法的描述符文本值，在本例中，可以用来作sum_i、sum、sum2的描述符 #29 = Utf8 a //这种位于局部变量表的变量名，都是在开启调试模式时（javac -g）编译的class文件才有，普通javac是没有的，下方b、x、y、result同理 #30 = Utf8 b #31 = Utf8 sum //sum方法的简单名称文本值 #32 = Utf8 x #33 = Utf8 y #34 = Utf8 sum2 //sum2方法的简单名称文本值 #35 = Utf8 result #36 = Utf8 &lt;clinit&gt; //类初始化方法的简单名称，就是静态块，静态块被认为是类本身的\"构造器\"，如果没有静态块，那么也不会有这个方法 #37 = Utf8 SourceFile #38 = Utf8 Test.java //本类源文件名称 #39 = NameAndType #17:#18 //已经被#1和#5的MethodRef引用，其值为：\"&lt;init&gt;\":()V #40 = Class #50 //类限定名：java/lang/System，被#2引用 #41 = NameAndType #51:#52 //被#2引用，其值为：out:Ljava/io/PrintStream;即：System.out这个属性的名称和描述符 #42 = Utf8 Test init ! //Test()方法里的字符串文本值，被#3引用 #43 = Class #53 //类限定名：java/io/PrintStream，被#4引用 #44 = NameAndType #54:#55 //被#4引用，其值为：println:(Ljava/lang/String;)V，即：out.println这个方法的名称和描述符 #45 = NameAndType #16:#12 //被#6引用，其值为：i:I，即该实例中i这个成员变量的名称和描述符 #46 = Utf8 sun/juwin/test/Test //类全限定名的文本值，被#7引用 #47 = NameAndType #31:#28 //被#8引用，其值为：sum:(II)I，即该实例中sum方法的简单名称和描述符 #48 = NameAndType #11:#12 //被#9引用，其值为：j:I，即该实例中j这个类变量的名称和描述符 #49 = Utf8 java/lang/Object //类全限定名的文本值，被#10引用 #50 = Utf8 java/lang/System //类全限定名文本值，被#40引用 #51 = Utf8 out //属性简单名称文本值，被#41引用 #52 = Utf8 Ljava/io/PrintStream; //属性的描述符文本值，被#41引用 #53 = Utf8 java/io/PrintStream //类全限定名的文本值，被#43引用 #54 = Utf8 println //方法简单名称文本值，被#44引用 #55 = Utf8 (Ljava/lang/String;)V //方法描述符的文本值，被#44引用 10.2.4：字节码-字段表看完了最为复杂的常量池，下面来看下字段表部分，这部分简单许多： 代码块6 字段表123456789101112public static final int j; //类变量j descriptor: I //描述符，指向#12 flags: (0x0019) ACC_PUBLIC, ACC_STATIC, ACC_FINAL //参考7.1,将十六进制的0x0019转成二进制后看看哪些bit位是1哪些就是它的修饰符，显然是ACC_PUBLIC、ACC_STATIC、ACC_FINALpublic static final int k; //类变量k descriptor: I flags: (0x0019) ACC_PUBLIC, ACC_STATIC, ACC_FINAL ConstantValue: int 5 //参考9.1，如果一个static final的常量有直接的初始值，那么必定跟一个ConstantValue的属性表private int i; //成员变量i descriptor: I flags: (0x0002) ACC_PRIVATE 10.2.5：字节码-方法表 ps：想用更加简洁的方式看方法指令的话，建议使用javap -c。本节只挑选代码块2实例中的几个具有代表性的方法来说明。 下面就是我们的方法表了，内部会包含Code这个涵盖方法主要逻辑的指令码，我们这节会一点点分析这些指令是如何在操作数栈里运行的（操作数栈是运行期要学的一个结构，先做个了解，它位于栈帧内，主要负责执行Code指令码），这里要注意：栈是一种先进后出的数据结构，既然操作数栈是栈，那么必然满足这一点特性。 10.2.5.1：无参构造器-Test()javap输出的详细信息： 代码块7 方法表-Test()123456789101112131415161718public sun.juwin.test.Test(); //Test方法，Code区域的指令集是如何在操作数栈运行的？请看图27 descriptor: ()V //方法描述符，指向#18 flags: (0x0001) ACC_PUBLIC //参考8.1 Code: stack=2, locals=1, args_size=1 //依次为：最大栈深、逻辑Slot数量（实际上可能少于这个，因为Slot可复用）、参数size，成员方法默认传入this，所以即便Test()里没有参数，这里args_size也是1 0: aload_0 //取this 1: invokespecial #1 //调用方法：java/lang/Object.\"&lt;init&gt;\":()V 4: getstatic #2 //获取类属性：java/lang/System.out:Ljava/io/PrintStream; 7: ldc #3 //ldc是指从常量池中取值，放入操作数栈，值为：Test init ! 9: invokevirtual #4 //调用方法：java/io/PrintStream.println:(Ljava/lang/String;)V 12: return //返回指令，用于终止程序 LineNumberTable: line 15: 0 //源代码15行对应指令码第0行，剩下的类推即可（这里源代码以代码块2为准） line 16: 4 line 17: 12 LocalVariableTable: Start Length Slot Name Signature //这里只有一个Slot，它的下标为0 0 13 0 this Lsun/juwin/test/Test; 先来看下其源代码： 代码块81234public Test() { //隐式传入this作参数 super(); //隐式调了父类构造器 System.out.println(\"Test init !\"); //输出一段话} 它对应的指令码参考代码块7-Code部分 当一个线程运行到此方法，会为其创建一个栈帧。 栈帧的基本概念 栈帧是用于支持jvm进行方法调用和方法执行的数据结构，它是运行期保存在虚拟机栈中的栈元素，是线程运行的基本单元，由执行引擎触发执行其内部保存的字节码指令（这里执行引擎在执行代码时还分为解释执行和编译执行）。栈帧由局部变量表、操作数栈、动态连接、方法返回地址、附加信息组成 Test()方法栈帧结构如图： 现在，让我们看一下这个方法在运行时，栈帧内部是如何变化的： 栈帧初始状态：入参首先填充进局部变量表 解释：首先Test方法是一个非静态方法，其方法第一个入参就是this这个reference类型的变量，因此aload_0就是将该变量推向操作数栈的栈顶。 第一步：aload_0，代表将局部变量表内第0个Slot内保存的变量入栈 第二步：invokespecial，代表一次构造方法的调用，根据代码块7可知这里调用的是其父类的构造方法 随着Object构造方法触发，随之又产生了一个Object构造器的栈帧，用来执行父类构造器逻辑，传入的是子类对象指针this，这点要记住。为了优化内存，jvm开发者们经常会将新开栈帧的局部变量表和上一栈帧的操作数栈共享一些数据，共享是完全按照下一栈帧接收的参数个数来的，比如下一栈帧需要5个参数，那么当前栈帧操作数栈从栈顶开始往下数5个，全部会共享进下一栈帧的局部变量表里。 还有一点需要说明一下，我们知道在一个对象的构造器被触发时，它的父类构造器首先会被触发，此时很多人认为父类跟子类一样，肯定也是被new了一次，其实不然，集合图29和图30可知，在Test类的构造器被触发之前，就已经存在this指针了，那意味着类对象已经产生了，因此构造器是在对象产生后主动触发的，而父类也没有被new，仅仅是触发了一下它的构造器而已，你可以理解，一个子类对象，其实就是包含了其父类公共方法的类，不存在对象嵌套的情况（即一个子类的对象是不会包含一个父类对象的），这个在第六节会详细讲，本节做个预热即可。 最后随着新开栈帧运行结束，线程又会再次回到当前栈帧，并将当前栈帧内的操作数栈共享部分清空，若下一栈帧有返回值，则将该值存入当前栈帧的栈顶，否则不进行其他操作。说到这里也知道为什么栈帧也是栈了，先进入的栈帧最后才会执行完，越是新开的栈帧越先结束，这似乎也是栈的特性。 第三步：getstatic，用来获取某个类的静态属性，由代码块7可知，这里是从类常量池里获取到的PrintStream类型的System类的out静态属性，即System.out。 提示：getstatic的执行流程应为：从常量池获取常量的描述符，然后根据描述符里指示的类和属性名，去对应的类里拿到该属性的值。 第四步：ldc，由代码块7可知，这是从常量池里加载String类型字面量“Test init !”入栈 第五步：invokevirtual，代表了一次虚方法的调用，本例指的是对out对象的println方法的调用 结合上图来思考下println的两个参数，首先我们需要明确一点，println不是静态方法，因此它传入的第一个参数一定是“this”，即对象引用，其实任何非静态方法的访问都是如此，需要知道自己的对象是哪位，否则没意义，那么针对println这个方法的“this”，又是谁呢？答案是out，我们是用out对象调用的它，所以这里第一个入参一定是out对象，第二个入参则是“Test init !”，所以上图中共享给下游栈帧的一共有俩，已用红框标出。 第六步：return，方法执行结束，栈帧出让，就不画图了。 到这里，我们已经完成了一组指令码的执行过程解析。 10.2.5.2：sum_i(int a, int b)再来分析一个方法的执行流程，我们把javap的信息丢出来（这里就不加注释了）： 代码块91234567891011121314151617181920public int sum_i(int, int); descriptor: (II)I flags: (0x0001) ACC_PUBLIC Code: stack=4, locals=3, args_size=3 0: aload_0 1: getfield #6 // Field i:I 4: aload_0 5: iload_1 6: iload_2 7: invokevirtual #8 // Method sum:(II)I 10: iadd 11: ireturn LineNumberTable: line 29: 0 LocalVariableTable: Start Length Slot Name Signature 0 12 0 this Lsun/juwin/test/Test; 0 12 1 a I 0 12 2 b I 它的源代码如下： 代码块10123public int sum_i(int a, int b) { return i + sum(a, b);} 它有3个参数，还发生过一次方法调用，最后还要将方法返回的值再加上i属性，然后将结果返回出去，我们来看下这个方法的运行过程。 老样子，来看下栈帧初始状态：入参首先填充进局部变量表 这是个非静态方法，第一个参数依然是该方法对应类的某个对象的引用指针（即this），剩下两个参数就是源代码里的a和b。 第一步：aload_0，代表将局部变量表内第0个Slot内保存的变量入栈，即this入栈，因为接下来的getfield指令要用到，这跟非静态方法需要this的道理一样，获取某个非静态的属性，也需要知道是哪个对象，才能取到对应的值，这就是非静态属性和方法与静态属性方法最大的区别。 第二步：getfield，这个指令用来获取某对象的某个属性值，本例中为i 提示：getfield执行流程：从常量池获取属性的描述符，然后根据描述符里指示的类和属性名，去对应的类对象里拿到该属性的值。（图中第2步就是利用当前栈顶的this，来打入其内部，将其对应属性i的值取出，并设置入栈顶，替换掉原来的this） 第三步：aload_0，再次将局部变量表内第0个Slot内保存的变量入栈 第四步：将入参a和b入栈 第五步：invokevirtual，虚方法调用，本例指的是对sum方法的调用，通过实例代码可知，sum方法拥有三个参数，即this、a、b，因此开栈帧时需要共享当前栈帧自栈顶往下数3个栈位作为新栈帧的入参 注意，sum方法是有返回值的，返回值为a和b相加的结果，最终sum栈帧执行完毕再次回到sum_i栈帧时，会将其返回结果入栈，最终sum执行完毕后的情况如下： 第六步：iadd，将栈顶两int型数值相加，并且其运算结果入栈 第七步：ireturn，将现在位于栈顶的数据返回出去给上游的栈帧，就不画图了。 到目前，sum_i方法的执行流程就剖析完毕。 十一、小结本章介绍了Class文件本身的组成以及它内部数据的堆叠方式，还介绍了运行期指令码是怎么被执行引擎所运行的，可以尝试利用这些知识，分析日常中较为复杂的类的字节码文件，熟悉字节码指令对于我们理解日常java代码的一些“特殊性质”有一定的积极作用。","link":"/2020/05/06/JVM%E5%9F%BA%E7%A1%80%E5%9B%9E%E9%A1%BE%E8%AE%B0%E5%BD%95%EF%BC%88%E5%9B%9B%EF%BC%89%EF%BC%9A%E7%B1%BB%E6%96%87%E4%BB%B6/"},{"title":"LV5-6：java并发包-juc的简单介绍","text":"一、线程池1.1：简单介绍前面简单提了一嘴线程池，回想下我们前几节内容提到的线程创建： 代码块112Thread t1 = new Thread();t1.start(); //毫无意义的一个线程被创建并触发了，这个线程不会执行任何逻辑 这段简单的代码，会在运行的时候向操作系统申请开启一个线程，当我们利用这个线程把我们的业务代码运行完，线程就会被销毁，再次会议一下线程的生命周期那张图。 但是线程是稀缺资源，当我们的程序里存在大量开启-关闭线程这种操作时，对于系统本身是一种浪费，其次，开启线程是有性能开销的，所以有没有一种方法，可以让线程创建之后，就不再死亡，而是用的时候直接去一个地方拿，用完了也不再销毁它，而是重新放回去，等待别的地方使用呢？ 线程池诞生了！ 跟它的名字一样，它是个用来存放线程对象的池子，最开始就会往里面放一定量的线程，别人需要使用线程的时候，它便从池子里取到一个闲置的线程用来给别人使用，别人用完了，就回收这个线程，接下来给别人用。 1.2：初始化线程池简单记一下，有下面三种快速初始化线程池对象的方法： Executors.newCachedThreadPool()：无限线程池。 Executors.newFixedThreadPool(nThreads)：创建固定大小的线程池。 Executors.newSingleThreadExecutor()：创建单个线程的线程池。 我们目前用newCachedThreadPool的情况比较多，它可以无限创建线程，每个线程被创建后都可以活60s的时间，期间可以被反复使用。 例子： 代码块2123456789101112131415161718public class Test { //全局声明一个线程池 private static ExecutorService threadPool = Executors.newCachedThreadPool(); public static void main(String[] args) { //直接启动一个线程来运行async方法 Thread t = new Thread(Test::async); t.start(); //使用线程池提供的线程运行async方法 threadPool.execute(Test::async); //利用execute方法触发线程运行 } public static void async() { System.out.println(\"线程名为\" + Thread.currentThread().getName() + \"正在运行...\"); }} 二、ReentrantLock2.1：简介前面讲了使用synchronized同步块来解决多线程下操作同一块资源的安全性问题，现在，来认识juc下同样可以实现同步控制的锁：ReentrantLock ReentrantLock实现了Lock接口，除此之外，相比synchronized只支持非公平锁的特性，它支持灵活配置锁竞争的公平和非公平模式，什么是锁竞争的公平与非公平呢？我们继续来拿之前的流程图举例： 2.2：用法代码块31234567891011121314151617181920212223242526272829public class Word { //全局声明一个重入锁作为成员变量，这样，使用这个变量控制的同步块就跟原先的对象锁功能相同，只在同一个对象下生效 private ReentrantLock lock = new ReentrantLock(false); //参数代表启用公平或者非公平的锁竞争模式，我们这里设置为非公平 //声明为static的情况下，跟之前的类锁相似，只要属于这个类，这个锁则全生效 private static ReentrantLock lockAll = new ReentrantLock(false); //对象锁，相同对象被多个线程访问才会生效 public void A() { lock.lock(); try { System.out.println(\"A方法被调用\"); } finally { lock.unlock(); //释放锁，为了保证锁释放一定会被执行，一般放到finally块里 } } //类锁，只要是Word对象，被多个线程访问时都会上锁 public void B() { lockAll.lock(); try { System.out.println(\"B方法被调用\"); } finally { lock.unlock(); } } } 2.3：await、signal、signalAll同样的ReentrantLock也支持线程通信，我们可以利用ReentrantLock来改写下前面的那个阻塞队列： 代码块41234567891011121314151617181920212223242526272829303132333435//阻塞队列类public class BlockQueue { private List&lt;String&gt; msgs = new LinkedList&lt;&gt;(); private ReentrantLock lock = new ReentrantLock(false); //不同于synchronized可以利用对象里的wait和notify，想要利用ReentrantLock实现线程通信，需要使用lock对象生成一个condition对象 private Condition condition = lock.newCondition(); public void put(String msg) { lock.lock(); //将synchronized同步块换成lock的方式进行 try { msgs.add(msg); //存放进 condition.signalAll(); //效果等于notifyAll } finally { lock.unlock(); } } public String get() { lock.lock(); try { if (msgs.size() == 0) { try { condition.await(); //效果等于wait } catch (InterruptedException e) { e.printStackTrace(); } } return msgs.remove(msgs.size() - 1); } finally { lock.unlock(); } }} 运行效果跟之前是一样的。 三、利用线程池的Future模式做到跟join一样的效果还是前面优化大首页的例子，我们不在将Dao层的代码列出来，我们仅看service层的异步实现，对比下不同： 代码块5123456789101112131415161718192021222324// 异步获取，通过多线程优化性能public WebModule getWebModuleMsgSimpleAsync() throws InterruptedException { WebModule webModule = new WebModule(); Thread topTask = new Thread(() -&gt; webModule.setTop(moduleDao.getTop())); Thread leftTask = new Thread(() -&gt; webModule.setLeft(moduleDao.getLeft())); Thread rightTask = new Thread(() -&gt; webModule.setRight(moduleDao.getRight())); Thread userTask = new Thread(() -&gt; webModule.setUser(moduleDao.getUser())); //触发各个异步任务 topTask.start(); leftTask.start(); rightTask.start(); userTask.start(); //等待所有的任务均执行完毕 topTask.join(); leftTask.join(); rightTask.join(); userTask.join(); return webModule;} 这是我们使用普通方式创建线程，然后使用join来完成的异步任务调度，结合最开始说的有关线程池的优点来看这个流程，会发现我们一下子就创建了4个线程，这是有开销的，那么线程池是否也能完成类似这种操作呢？ 代码块612345678910111213//线程池Future模式完成异步操作public WebModule getWebModuleMsgAsync() throws ExecutionException, InterruptedException { Future&lt;String&gt; top = threadPool.submit(moduleDao::getTop); Future&lt;String&gt; left = threadPool.submit(moduleDao::getLeft); Future&lt;String&gt; right = threadPool.submit(moduleDao::getRight); Future&lt;String&gt; user = threadPool.submit(moduleDao::getUser); WebModule webModule = new WebModule(); webModule.setTop(top.get()); webModule.setLeft(left.get()); webModule.setRight(right.get()); webModule.setUser(user.get()); return webModule;} 上面的代码就是我们利用线程池改造后的代码块5，它的运行效果是和代码块5一致的。 四、异步任务编排：CompletableFuture有时候，我们需要拿到某个方法的结果后再去拿着这个结果去调用别的方法，这时就不能盲目使用join或者线程池来做异步，juc提供了更高级的类：CompletableFuture 详细用法请查看附件：利用CompletableFuture优化程序的执行效率","link":"/2020/04/02/LV5-6%EF%BC%9Ajava%E5%B9%B6%E5%8F%91%E5%8C%85-juc%E7%9A%84%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/"},{"title":"LV5-5：线程调度：join、yield、sleep、interrupt","text":"一、简介这些方法属于线程对象里的方法，属于线程本身的操作。 join：用于等待一个线程的终止，等待期间将会进入阻塞状态，直到被等待的线程终止结束。 yield：用于线程让步，触发了此方法的线程会进入就绪状态，也就是说会让出CPU的调度一下，让CPU转去其他线程。 sleep：强制当前正在运行的线程进入阻塞状态，直到休眠期结束，才会再次进入运行状态。 interrupt：终止当前正在运行的线程。 二、用法2.1：join前面说过它简单的用法，来看个例子： 代码块1123456789101112131415161718192021222324252627282930313233public static void main(String[] args) throws Exception { System.out.println(\"main线程开始运行\"); Thread t1 = new Thread(() -&gt; { try { Thread.sleep(1000L); //让当前线程阻塞1s System.out.println(\"线程1运行结束\"); } catch (InterruptedException e) { e.printStackTrace(); } }); Thread t2 = new Thread(() -&gt; { try { Thread.sleep(5000L); //让当前线程阻塞5s System.out.println(\"线程2运行结束\"); } catch (InterruptedException e) { e.printStackTrace(); } }); //线程1和2启动 t1.start(); t2.start(); //利用join，可以让main线程必须等到它们俩线程运行完毕后才会继续往下执行 t1.join(); t2.join(); System.out.println(\"main线程运行结束\"); } 运行结果： 最终运行结果告诉我们，由于t1和t2在main线程里join，因此main线程在join处必须要等到t1和t2都运行结束后才会继续往下运行。 它的流程像是下面这样： 此时main方法总耗时约等于5s。 基于这种流程理解，我们让t1和t2调用join前，让main线程阻塞6s（超过t1和t2并发运行的总耗时），此时join便不再阻塞main线程了，因为t1和t2已经执行结束了： 相比图2，main线程阻塞的时间甚至还多出了一截，此时总运行时间约为6s。 2.2：【实例】利用join优化一个大首页接口的效率假设现在有一个网站，首页有顶部Banner位、左边栏、右边栏、用户信息几大模块需要加载，现在出一个接口，要求包装并吐出这几大模块的内容。 先来抽象一个首页接口对象： 代码块21234567891011121314151617public class WebModule { private String top; //顶部Banner位 private String left; //左边栏 private String right; //右边栏 private String user; //用户信息 //...get...set... @Override public String toString() { return String.format(\"top: %s; left: %s; right: %s; user: %s\", top, left, right, user); }} 定义我们的dao层，我们利用sleep来模拟实际的方法耗时： 代码块3123456789101112131415161718192021222324252627282930313233343536373839public class ModuleDao { public String getTop() { // 这里假设getTop需要执行200ms try { Thread.sleep(200L); } catch (InterruptedException e) { e.printStackTrace(); } return \"顶部banner位\"; } public String getLeft() { // 这里假设getLeft需要执行50ms try { Thread.sleep(50L); } catch (InterruptedException e) { e.printStackTrace(); } return \"左边栏\"; } public String getRight() { // 这里假设getRight需要执行80ms try { Thread.sleep(80L); } catch (InterruptedException e) { e.printStackTrace(); } return \"右边栏\"; } public String getUser() { // 这里假设getUser需要执行100ms try { Thread.sleep(100L); } catch (InterruptedException e) { e.printStackTrace(); } return \"用户信息\"; } } 我们再定义一个service层，提供两个方法，它们的目的都是通过dao层提供的数据，封装成WebModule对象返回给网关层（controller层）： 代码块412345678910111213141516171819202122232425262728293031323334353637383940public class ModuleService { private ModuleDao moduleDao = new ModuleDao(); // 同步获取 public WebModule getWebModuleMsgSync() { WebModule webModule = new WebModule(); webModule.setTop(moduleDao.getTop()); webModule.setLeft(moduleDao.getLeft()); webModule.setRight(moduleDao.getRight()); webModule.setUser(moduleDao.getUser()); return webModule; } // 同步获取，通过多线程优化性能 public WebModule getWebModuleMsgSimpleAsync() throws InterruptedException { WebModule webModule = new WebModule(); Thread topTask = new Thread(() -&gt; webModule.setTop(moduleDao.getTop())); Thread leftTask = new Thread(() -&gt; webModule.setLeft(moduleDao.getLeft())); Thread rightTask = new Thread(() -&gt; webModule.setRight(moduleDao.getRight())); Thread userTask = new Thread(() -&gt; webModule.setUser(moduleDao.getUser())); //触发各个异步任务 topTask.start(); leftTask.start(); rightTask.start(); userTask.start(); //等待所有的任务均执行完毕 topTask.join(); leftTask.join(); rightTask.join(); userTask.join(); return webModule; } } 我们用controller层来测试下： 代码块51234567891011121314151617181920212223242526//只是模拟，其实网关层应该是servlet或者spring boot的controller（后续会认识到）public class ModuleController { private ModuleService moduleService = new ModuleService(); public WebModule getWebModuleMsgSync() { return moduleService.getWebModuleMsgSync(); } public WebModule getWebModuleMsgSimpleAsync() throws InterruptedException { return moduleService.getWebModuleMsgSimpleAsync(); } public static void main(String[] args) throws Exception { ModuleController moduleController = new ModuleController(); long start = System.currentTimeMillis(); //获取系统当前时间戳，ms moduleController.getWebModuleMsgSync(); //同步获取各个模块的方法 System.out.println(\"同步获取各个模块，所需时间为：\" + (System.currentTimeMillis() - start) + \"ms\"); long start2 = System.currentTimeMillis(); moduleController.getWebModuleMsgSimpleAsync(); //异步获取各个模块的方法 System.out.println(\"异步获取各个模块，所需时间为：\" + (System.currentTimeMillis() - start2) + \"ms\"); }} 运行结果如下： 代码块612同步获取各个模块，所需时间为：442ms异步获取各个模块，所需时间为：207ms 我们利用多线程异步+join的方式，将代码性能优化了足足1倍多。 我们再使用执行流程图来说明下它们的运行流程： 同步方法getWebModuleMsgSync的运行流程 异步方法getWebModuleMsgSimpleAsync的运行流程 请结合代码和流程图，仔细理解下join。截止到目前，你已经会用多线程优化一个慢接口了~ 2.3：sleep就不想说啥了，我们前面的例子在不断的用这个方法，只需要知道，你在任何地方写上Thread.sleep(xxx);这段代码，就是在让运行该段代码的线程阻塞（休眠）。 2.4：interrupt&amp;stop都用于主动终止一个线程，区别在于，interrupt比较温和，stop属于暴力关停，我们下面通过一个例子来看下它们的区别。 线程本身调用这俩方法后，视为已终止状态，下面来用interrupt和join来做个实验： 代码块7123456789101112131415161718192021222324252627282930313233343536373839404142434445public class JoinTest { private boolean isStop = false; public static void main(String[] args) throws Exception { JoinTest test = new JoinTest(); Thread loopT = new Thread(test::loopTask); //test::loopTask等于test.loopTask() loopT.start(); sleep(2000L); //2s后终止线程 test.setStop(true); long s = System.currentTimeMillis(); loopT.join(); System.out.println(\"线程终止后，join阻塞时间为：\" + (System.currentTimeMillis() - s)); System.out.println(\"end~\"); } public void setStop(boolean stop) { isStop = stop; } public void loopTask() { while (!isStop) { //若状态为false，则继续执行下面的逻辑，每隔1s打印一次 sleep(1000L); System.out.println(\"loop trigger ~\"); } Thread.currentThread().interrupt(); //在这里终止掉当前线程 //事实上，在终止掉线程后，还有接下来的逻辑要执行 long s = System.currentTimeMillis(); for (int i = 0; i &lt; 1000000; i++) { int[] a = new int[100]; //模拟耗时操作，这里不能用sleep了，因为当前线程已经被终止了 } System.out.println(\"线程终止后，逻辑块运行时间：\" + (System.currentTimeMillis() - s)); } public static void sleep(long time) { try { Thread.sleep(time); } catch (InterruptedException e) { e.printStackTrace(); } } } 运行结果如下： 代码块812345loop trigger ~loop trigger ~线程终止后，逻辑块运行时间：98线程终止后，join阻塞时间为：114end~ 通过interrupt终止线程，即便线程被终止了，后面的逻辑也会触发，join依旧会选择阻塞，直到后续逻辑执行完毕，事实上，大部分任务都可以及时的终止，比如第一个例子，异步出去的任务，最终都会执行完成，线程变为终止状态，join都可以顺利结束，但是反观上例，如果没人及时的设置isStop的值，程序会一直执行下去，没有终止态，join会无止境的终止下去。 而stop，线程的stop方法已被官方标记为“不建议使用”的方法，如果把上例的interrupt的调用换成stop，来看看其运行结果： 代码块91234loop trigger ~loop trigger ~线程终止后，join阻塞时间为：0end~ 可以看到，线程终止后的后续逻辑均没有触发，前面说过，stop是一种很粗暴的终止线程的方式，一旦被stop，那么里面的业务逻辑将直接断掉，因此官方并不推荐使用该方法来终止线程。 而interrupt，仅仅是对目标线程发送了了一个中断信号（改变了线程的中断状态而已），当目标线程再次通过obj.wait、thread.sleep、thread.join方法进入阻塞状态时，接收到该信号，就会抛出InterruptedException异常，这时候需要业务方自行处理或者直接抛出，以结束线程阻塞状态（这里需要注意的是被obj.wait方法阻塞时，抛出该异常需要目标线程再次获得实例对象obj的锁才行）。 上述三个“需要花费时间”的方法均抛出了InterruptedException异常，针对这些特性，想要完成以下操作就非常方便了： 取消wait方法等待notify/notifyAll的处理 取消在sleep方法指定时间内停止的处理 取消join方法等待其他线程终止的处理 取消之后所做的处理，取决于需求，可能会终止线程，或者通知用户已取消，或者终止当前处理进入下一个处理阶段。 2.5：yield通过前面的理解，我们知道线程是靠CPU通过来回切换执行的方式来执行多个线程的，那么你知道你的线程通过代码调用start后，会发生哪些状态变化吗？ 首先，start触发线程后，线程状态进入运行状态，而我们前面讲过CPU是来回切换运行线程的，所以我们针对”被调度到“和”等待被调度“两种情况又对运行状态又做了细分，即运行中和就绪： ⚠️ 请务必仔细跟着图中的序号顺序梳理一遍这个流程，对之后的线程生命周期理解起来很有帮助 为什么要说这个呢？因为它可以帮你理解yield，yield被当前线程触发后，首先当前线程会直接进入”就绪状态“，你可以这么理解，当前线程做出yield调用之后，会将本该调度到它的CPU”让出去“，让CPU重新调度一次，注意我这里说的是重新调度一次，所以即便调用了yield，下次CPU仍然有概率调度到它，来看下这个过程： 看到了吗？任意线程只要自行触发自己的yield方法，就会立马让自己变成就绪态，然后让CPU重新选择线程调度，重新调度意味着可能会调度到其它线程，也可能再次调度到自己，如果再次调度到自己（如图中的情况2），那么就只能继续往下执行咯，等于yield无用，如果调度到别人（如图中的情况1）则yield有意义，因为你确实让出了CPU让别人进入运行态了。 就不举例子了，你也看到了，yield的意义是这样的，本就不好举例，但是这个过程一定要梳理清楚，至少你要知道线程处于运行状态的时候，又根据CPU调度状态细分了就绪和运行中状态。 三、线程状态迁移图截止目前，我们知道一个线程对象可以通过下面的方法改变自己的运行状态： 通过start让自己变成运行中状态。而运行中状态又根据有没有被CPU调度到分成了就绪态和运行中两个状态 通过sleep让自己陷入阻塞状态，阻塞状态意味着CPU不会再调度到它，除非它解除阻塞变为就绪状态 通过调用其它线程的join方法让自己在其它线程结束前陷入阻塞状态，其它线程结束后，自己再次由阻塞变为就绪状态 通过某个对象的wait方法，让自己处于阻塞状态，然后再次通过同一对象的notify触发，让自己变成就绪状态 通过yield让自己让出正在调度自己的CPU，让自己直接进入就绪状态 通过interrupt或者stop让自己变成终止状态 我们结合上面的六条信息，整理下一个线程的生命周期，应该包含哪几个状态： 运行状态（又细分为就绪和运行中两个小状态）、阻塞状态、终止状态 让我们来画下线程的生命周期，或者叫线程状态转换图：","link":"/2020/04/02/LV5-5%EF%BC%9A%E7%BA%BF%E7%A8%8B%E8%B0%83%E5%BA%A6%EF%BC%9Ajoin%E3%80%81yield%E3%80%81sleep%E3%80%81interrupt/"},{"title":"LV5-4：线程通信：wait、notify、notifyall","text":"一、Object类里对线程实施控制的方法前面说过，Object是所有类的父类，它自带equals、hashCode等方法，我们这次来介绍几个新的成员方法： 这节要讲的wait、notify、notifyall都是Object里自带的方法，通过这些方法可以控制当前正在运行这个对象的线程，如何控制？往下看~ 二、利用线程控制方法完成线程通信上述几个方法是属于每个实例对象的，所有实例都拥有一个“等待队列”（虚拟概念，实例里并不存在该字段），它是在实例的wait方法调用后存放停止操作线程的队列。执行wait方法后，线程进入当前实例的“等待队列”，以下几种情况可以让线程退出“等待队列”： 其他线程调用notify、notifyAll方法来将其唤醒 其他线程调用interrupt来将其唤醒 wait方法本身超时 2.1：wait当执行了下面的代码： 代码块11obj.wait(); 我们可以说当前线程在obj上发生了等待，当前线程进入了obj的“等待队列”，此时当前线程会让出锁，让其他线程继续竞争获得该实例的锁（因此这里有个规则，调用wait的线程必须持有当前实例对象的锁） ⚠️ 普通不加synchronized的方法也可以使用wait，这在编译器是没问题的，但是运行期会报IllegalMonitorStateException异常。 还是以前面提过的Word对象为例，但是现在A这个方法里调用了wait方法，这时多线程访问时过程就变成了下图： 2.2：notify现在先来介绍下notify，该方法会将等待队列里的线程取出，让其退出等待并参与锁竞争然后继续执行上次wait后没有执行完的语句。整体过程如下图所示： 可以看到，t1在被挂起后，会因为t2调用了同实例的notify方法，而让t1被从等待队列里释放，重新加入到所得竞争力，t2执行完毕后释放锁，锁又再次被t1竞争到，t1将继续执行上次被挂起时后面未执行完的语句。 需要指出的是，如果等待队列里的线程是多个，那么被唤醒的那一个，将会是等待队列里所有线程随机的一个，不会特定哪一个线程会被唤起。 2.3：notifyAll接下来介绍notifyAll方法，顾名思义，就是将等待队列里的线程全部唤起，然后这些线程将全部加入到锁竞争，竞争到，继续完成上次被挂起时未执行完毕的操作，流程图如下： 说明，当线程调用实例的wait、notify、notifyAll方法有个大前提，就是必须要求该线程拥有该实例的锁，否则会抛IllegalMonitorStateException异常。 在编写程序时，是该选择notify还是选择notifyAll？这个可以指出的是，notifyAll往往更加健壮，而notify由于唤起的线程少，因此效率会更高，但是存在程序停止的风险。 三、实例3.1：wait+notify的简单例子以图4里的Word对象为例，我们将这个Word类定义出来： 代码块212345678910111213public class Word { public synchronized void A() throws Exception { System.out.println(\"A方法 wait触发前\"); this.wait(); //将竞争到对象锁后运行到这里的线程，放入到自己的wait队列，让它处于\"挂起\"的状态 System.out.println(\"A方法 wait触发后\"); } public synchronized void B() { System.out.println(\"B方法被触发~\"); this.notifyAll(); }} 测试类，我们开启两个线程来访问同一个word对象里的同步方法： 代码块31234567891011121314151617181920212223public static void main(String[] args) throws Exception { Word w1 = new Word(); Thread t1 = new Thread(()-&gt;{ try { w1.A(); //第一个线程触发w1对象的A方法 } catch (Exception e) { e.printStackTrace(); } }); Thread t2 = new Thread(()-&gt;{ try { w1.B(); //第二个线程触发w1对象的B方法 } catch (Exception e) { e.printStackTrace(); } }); t1.start(); Thread.sleep(1000L); //t1启动后main线程睡眠一秒，确保t1可以抢到w1的对象锁 t2.start();} 这个运行结果为： 代码块4123A方法 wait触发前B方法被触发~A方法 wait触发后 看到了吗？明明t1抢到了对象锁，但是只运行了A方法wait前面的部分，这时因为运行到wait时被挂起，释放锁，然后1s后t2抢到锁，运行B方法，然后触发B方法里的notifyAll后，所有放到w1对象等待队列里的线程被唤起，重新加入到锁竞争里去，此时t1再次竞争到w1的对象锁，然后把自己没执行完的方法栈执行完。 3.2：利用线程通信实现阻塞队列大体流程如下： 简单来说，就是有一个公共缓冲带，我们管它叫做消息队列，可以由多个生产者往它里面生产数据，可以由多个消费者获取，当队列内无消息可读时，所有消费者陷入等待，等待新的消息到来： 根据这个结构的特性，我们可以利用本节所学wait和notify来编写。 定义阻塞队列类，根据我们上面的描述，一个组的队列应该具备生产和获取的能力： 代码块512345678910111213141516171819202122//阻塞队列类public class BlockQueue { private List&lt;String&gt; msgs = new LinkedList&lt;&gt;(); //用于存放生产者生产的消息的结构 //因为支持多个生产者往里面生产数据，之前说过ArrayList线程不安全，所以这里需要给这个方法加锁 public synchronized void put(String msg) { msgs.add(msg); //存放进 this.notifyAll(); //生产完就通知消费者们，让它们知道有消息可以消费了~ } public synchronized String get() { if (msgs.size() == 0) { try { this.wait(); //如果生产者生产的消息都被消费者拿完了，那么消费者想再拿的时候就只能陷入等待，等待生产者再次生产信息 } catch (InterruptedException e) { e.printStackTrace(); } } return msgs.remove(msgs.size() - 1); }} 我们来开启几个独立的线程来当成是生产者程序和消费者程序，分别对BlockQueue对象进行生产-消费的操作： 代码块6123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051public static void main(String[] args) throws Exception { BlockQueue queue = new BlockQueue(); Thread p1 = new Thread(() -&gt; { queue.put(\"1号生产者生产的消息\"); }); Thread p2 = new Thread(() -&gt; { queue.put(\"2号生产者生产的消息\"); }); Thread p3 = new Thread(() -&gt; { queue.put(\"3号生产者生产的消息\"); }); p1.start(); p2.start(); p3.start(); //此时这三个代表着生产者的线程被启动，意味着队列里的list已经有了三条数据 Thread c1 = new Thread(() -&gt; { System.out.println(\"消费者1消费到的消息：\" + queue.get()); //消费者线程从阻塞队列里获取消息 }); Thread c2 = new Thread(() -&gt; { System.out.println(\"消费者2消费到的消息：\" + queue.get()); //消费者线程从阻塞队列里获取消息 }); Thread c3 = new Thread(() -&gt; { System.out.println(\"消费者3消费到的消息：\" + queue.get()); //消费者线程从阻塞队列里获取消息 }); c1.start(); c2.start(); c3.start(); //此时这三个代表着消费者的线程被启动，若执行完毕，则意味着队列里的list已变成空值 //此时队列里的消息已经被消费完，现在我们再开启一个消费者，让它继续获取消息（此时便会陷入等待） Thread c4 = new Thread(() -&gt; { System.out.println(\"消费者4消费到的消息：\" + queue.get()); //由于消息空了，因此该消费者线程会陷入queue对象的等待队列，挂起自己 }); c4.start(); Thread.sleep(5000L); //等待5s后，我们再开启一个生产者线程，让它产生一条消息 Thread p4 = new Thread(() -&gt; { queue.put(\"4号生产者生产的消息\"); }); p4.start();} 运行结果： 这个流程，如果对线程通信还不是很熟悉的话就需要仔细分析一下😁","link":"/2020/04/01/LV5-4%EF%BC%9A%E7%BA%BF%E7%A8%8B%E9%80%9A%E4%BF%A1%EF%BC%9Await%E3%80%81notify%E3%80%81notifyall/"},{"title":"LV5-3：java里的同步锁","text":"一、为什么会存在线程安全问题？1.1：用户如何访问我的程序？我们前面了解了java如何开启一个线程做异步处理，也知道了在实际的项目里，我们写的的web程序也被tomcat安排成了多线程调用的程序。 如果现在只有一个用户访问我们的web程序，这个过程再针对tomcat细化下，如下： tomcat接收客户端请求的流程大致如上图所示，但是我们也可以同时接收和响应多个用户的请求和响应，当请求数据包进入我们的tomcat时，它会从它自带的线程池里取出一个空闲的线程，用于执行具体的servlet逻辑，也就是你写的业务程序。 线程池：事先申请好一批线程，存放到一个结构里（可以是数组，也可以是集合，能存线程对象就行），这样就不用每次进来一个请求就去向操作系统申请一个线程，申请线程开销是比较大的，事先开启一批放到一个结构里，用的时候直接拿来用即可，这种技术叫池化技术，之后的数据库连接池也属于一种池化技术，本质上都是避免开销大的操作，让它们事先准备好，不至于用的时候现用现申请，可以提升程序效率。 注意，线程是操作系统层面的东西，只是每种语言都支持申请，像java之前的new Thread语句在创建一个Thread对象的同时，也会向操作系统申请创建一个实际的线程出来，Thread类其实只是对申请的线程的抽象，你可以通过这个类来操纵具体的线程，就像你可以利用HttpServletRequest类在Servlet场景里操作HTTP协议信息一样。 图1里tomcat的处理模式只有一个客户端一次请求，试想一下，你这个tomcat程序放到服务器上是要给全国人民访问的，我们来结合压测来理解，我们单机一个接口的压测成绩在500qps我们都会觉得超级低，而我们的接口平均响应时间在20ms，如果就是一个用户访问我们的tomcat，完成后再让第二个用户访问，以此类推，这样显然是不合理的，让我们做个数学题，每个用户访问我们这个接口都消耗20ms，1s是1000ms，如果是按照排队访问的方式，1s只能处理（1000/20 = 50）个用户的请求，然而我们觉得这种接口，不压到1000qps都算是非常非常低的成绩。既然是这样，tomcat在处理用户请求的时候，就不可能是处理完一个再处理一个，一定是一起处理的，基于此，我们引出多线程下的tomcat处理模式，来看下多线程模式下的tomcat运行方式，拿之前的例子StudentInfoController来说事： 通过图2可以更清晰的看到，多个用户同时访问你的web程序时的样子，可以看到进来的请求都是被从线程池里取到的线程并发执行的，换句话说，下面的代码是会被多线程并发执行的： 代码块11234567891011121314151617public class StudentInfoController extends HttpServlet { private StudentDao studentDao = new StudentDao(); @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { resp.setContentType(\"application/json\"); try { Student result = studentDao.getStudentInfo(); resp.getOutputStream().write(JSON.toJSONBytes(result)); resp.setStatus(200); } catch (Exception e) { e.printStackTrace(); resp.setStatus(500); } }} 1.2：线程安全问题先记住1.1关于tomcat的访问分发模式。我们现在来看下线程安全问题主要是指什么。 线程安全问题主要是指多条线程对同一个公共资源的操作，导致的一系列”违反常规“的事情，举个例子，现在来定义一个类： 代码块2123456789101112public class Adder { private int i = 0; public void incre() { i++; } public int getI() { return i; }} 类很简单，有一个叫i的属性，incre方法用来给自己这个属性做+1操作，getI用于返回这个属性值。 来编写下main方法，我们让同一个Adder对象被两个不同的线程做incre的调用： 代码块3123456789101112131415161718192021222324public static void main(String[] args) throws Exception { Adder adder = new Adder(); Thread t1 = new Thread(() -&gt; { for (int j = 0; j &lt; 1000; j++) { adder.incre(); //第一个线程让Adder对象里的i自增1000次 } }); Thread t2 = new Thread(() -&gt; { for (int j = 0; j &lt; 2000; j++) { adder.incre(); //第二个线程让Adder对象里的i自增2000次 } }); //启动线程 t1.start(); t2.start(); //利用线程睡眠，让main线程睡眠2s，等待上面两个线程执行完毕 Thread.sleep(2000L); System.out.println(adder.getI()); } 第一个线程让i自增1000次，第二个线程让i自增2000次，我们这时来猜一下最终运行结果应该是多少？3000吗？是的，看逻辑是3000没错！可是运行下，多运行几遍，就会发现诡异的事情发生了： 代码块4123456216026983000300028633000 我运行了6次，得到的结果让人惊讶，不仅不完全正确，甚至很”随机“，这便是线程安全问题。 还记得一开始说的吗？造成线程安全问题的原因主要是因为多个线程同时操作同一公共资源造成的，在本例中，Adder的对象就是个公共资源，自然它内部的i属性也是个公共资源了。那这种线程安全问题到底是如何导致的呢？我们根据本例进行分析一下： 如图所示，仿照incre的逻辑，做了一个流程图，incre的逻辑一定是先从Adder对象里拿到i的值，然后做+1计算，然后将计算结果重新写回对象的i属性，这没问题，在单线程下运行良好，但是现在线程2也做类似的操作，这时就有问题了，因为某一时刻，从内存取到的i值可能是相同的，例如第一次调用incre方法时，线程1和线程2同时取i的值，此时都是0，然后各自完成incre的调用，再将i写回对象，发现问题了吗？虽然线程1和线程2都执行了incre方法，但最终俩线程写回内存的i的值都是1，这就是为什么结果会不准的原因，不过你会发现，结果除了不准，它还随机，这就牵扯到多线程调用的一个随机性，线程是独立的、同时运行的，所以我前面描述的问题有可能发生，也有可能不发生，但只要发生一次，就足以影响结果的准确性，而发生的次数又是随机的，这也是结果为什么随机的原因。 二、synchronize关键词的作用2.1：synchronize是干啥的？首先它是个方法的修饰词，可以修饰成员方法，也可以修饰静态方法，也可以单独作为域来修饰一个代码块，被它修饰的方法或代码块，相当于加了个同步锁，在被多线程访问时，线程会发生”排队“，也就是后面会说的锁竞争，从而保证一个方法在多线程方法下有且仅有一个线程可以运行它。 2.2：对象锁2.2.1：利用对象锁解决Adder类的累加问题作用在对象上的同步锁，以对象实例为单位给访问它的线程进行”排队“，我们上面的例子就符合这种规则，我们现在让上面的incre方法变成一个同步方法（被synchronize修饰的方法，被称作同步方法）： 代码块5123456789101112public class Adder { private int i = 0; public synchronized void incre() { //改成同步方法 i++; } public int getI() { return i; }} 现在再来测试下代码块3里的内容： 代码块6123456300030003000300030003000 事实上，incre方法加了锁之后，不管运行几遍，程序始终都是正确的3000次。 为什么呢？再试着理解下图3，看看不加锁的时候是怎么导致最终结果不正确的？就是因为两个线程同一时刻访问统一资源导致的，那么再结合我们2.1中所描述的，加了synchronize的方法，在有多个线程同时访问自己的时候，只放行一个，另外一个排队，直到第一个线程执行完毕释放掉锁后，另外一个线程才可能进入： 由于锁的控制，我们如图3里的线程安全性问题被解决了~ 本例中的锁属于同步锁里的对象锁，它的作用域仅限于对象本身，一个线程要想访问它内部被synchronize修饰了的方法、代码块，则必须要获取当前对象的锁，就像图4中那样。 代码块7123456789101112131415161718192021public class ObjectLock { private int i = 0; public synchronized void incre() { i++; } public synchronized void decre() { i--; } public void setI(int i) { System.out.println(\"我不在同步块内，我可以被多个线程同时触发运行\"); //这时同步块，只有线程竞争到对象锁时才能访问 synchronized (this) { //同步代码块，括号里声明为this，代表这个代码块上的锁是对象锁 this.i = i; } } } 如上，任何线程要想调用incre、decre、setI的同步代码块，都必须要竞争到当前ObjectLock对象的对象锁，否则排队等待别的线程释放。 2.2.2：对象锁的作用域通过前面的了解，对象锁指的是当前线程获得了某个实例的锁，然后对其内部”上了锁“的资源的访问，举个例子，有个Word类，有A、B两个同步方法，C属于普通方法，如图所示： 可以发现，对象锁的作用域只针对当前对象生效，就像w1和w2里的A方法可以被不同的线程同时执行，但是同一个对象内的同步块，却只允许持有当前对象锁的线程执行，如t2、t3均被挡在了外面，当t1释放锁以后，t2、t3才会重新竞争锁，竞争到锁以后就会执行自己想要执行的同步逻辑。 这跟我们后面要说的类锁很不一样。 2.3：类锁跟对象锁的目标是一致的，就是控制线程同时访问，与对象锁不同的是，它是按照类来进行上锁的，就像普通方法和static方法一样，一个属于对象范畴的，一个属于类范畴的。 一般使用synchronize修饰的static方法，或者这样声明同步块： 代码块8123synchronized (XXX.class){ //跟对象锁用this做声明不一样，类锁使用class本身做声明 ...略} 类锁的作用域跟对象锁不太一样，改造下Word类，让其方法都变成静态的，图5里的访问就要变成下面这样： 跟上面相比较，这里的t5受到了t1的影响，因为t1获得了Word类的锁，w1和w2共属一个类，因此t1获得类锁以后，其他线程想要访问这个类里的同步块，就得等到t1释放锁以后才可以继续竞争锁然后执行自己想要执行的同步逻辑。 2.4：死锁死锁就是指两个线程互相等待对方释放锁资源的现象，举个例子：线程1持有对象a的对象锁，线程2持有对象b的对象锁，此时a对象里需要调用b对象的同步方法，而对象b也需要调用对象a的同步方法，这时线程1在等着获取对象b的对象锁，线程2等着获取对象a的对象锁，互不撒手，此时就导致程序发生了死锁，我们通过代码来说明下这个问题： 定义一个DeadLock类： 代码块912345678910111213141516171819202122232425262728public static void main(String[] args) { Object a = new Object(); //为了便于说明问题，我们随便new个a对象 Object b = new Object(); //为了便于说明问题，我们随便new个b对象 Thread t1 = new Thread(() -&gt; { synchronized (a) { System.out.println(\"线程1已经获取到了a对象的对象锁，进入同步块~\"); //假设接下来需要获取b对象的对象锁才能正常运行 synchronized (b) { System.out.println(\"线程1已经获取到了b的对象锁，进入同步块~\"); } } }); Thread t2 = new Thread(() -&gt; { synchronized (b) { System.out.println(\"线程2已经获取到了b对象的对象锁，进入同步块~\"); //假设接下来需要获取a对象的对象锁才能正常运行 synchronized (a) { System.out.println(\"线程2已经获取到了a的对象锁，进入同步块~\"); } } }); t1.start(); t2.start();} 这个程序输出： 代码块1012线程1已经获取到了a对象的对象锁，进入同步块~线程2已经获取到了b对象的对象锁，进入同步块~ 然后阻塞，一直阻塞，这俩线程算是废了，这就是死锁，线程1和线程2各自等着对方释放自己需要的锁，可是这是不可能的，因为同步块不执行完锁是没办法释放的。 三、锁的种类有哪些？synchronize属于哪一种？锁的分类太多太杂，感兴趣的话可以通过美团技术团队这篇文章来了解：不可不说的Java“锁”事 synchronized在实现上属于一种可重入的非公平锁，之后会结合java底层有关synchronized的实现来给具体的说一说同步锁。","link":"/2020/04/01/LV5-3%EF%BC%9Ajava%E9%87%8C%E7%9A%84%E5%90%8C%E6%AD%A5%E9%94%81/"},{"title":"LV5-2：并发&并行","text":"两张图来理解并发和并行： 如果只有一个CPU，通过CPU调度切换线程上下文，我们认为，在任意一个时刻，各个线程有且只能运行一个，这个叫并发。 相反的，如果有两个或者更多的CPU，那么就会是下面这样： 两个CPU一起调度的时候，总会出现一些”同时被调度运行“的线程，比如上图的红色⭐️标记的部分，它们确实在同一时刻都被调度到并且开始运行了，我们称这种因多CPU调度而产生的线程运行时间重叠的现象叫并行。 并行必须要有多个CPU，能并行的一定能并发，如果你只有一个CPU，那么你只能是并发了，不可能并行。","link":"/2020/04/01/LV5-2%EF%BC%9A%E5%B9%B6%E5%8F%91&%E5%B9%B6%E8%A1%8C/"},{"title":"LV5-1：进程是什么？线程又是什么？","text":"一、线程的简单认识还记得自己写的main方法吗？它在被运行的那一刻就等于你启动了一个java线程，只不过它结束的很快，因为我们的main方法里的逻辑执行完它就结束掉了，简单来说就是这个线程完成了它的使命，可以当场去世了。 再来发散下思维，还记得我们写过的xxx管理系统吗？为什么你可以一直输指令，它一直运行？我们看康康代码： 代码块112345678910111213141516171819202122public static void main(String[] args) { StudentSystemView view = new StudentSystemView(); while (true) { //这里是一个死循环！ int opNum = view.home(); if (opNum == 0) { //终止程序 break; } else if (opNum == 1) { //需要新增学生信息 view.addStudent(); } else if (opNum == 2) { view.updateStudent(); } else if (opNum == 3) { view.delStudent(); } else if (opNum == 4) { view.getStudentById(); } else if (opNum == 5) { view.getAllStudents(); } else { System.out.println(\"请输入有效的指令！\"); } }} 看第三行，什么是死循环呢？死循环就是指一个循环体本身并不会主动结束，比如例子里的while(true)，我们前面讲过while的语法含义，当一个条件满足时，就继续循环，当这个条件恒等于true，那么意味着这个循环程序会一直执行下去，直到你输入了指令0，通过break关键词终止循环体才算真正离开了这个死循环。 为什么要说这个例子？因为它更能直观的表达“线程”这个东西，我们来解释一下线程： 线程就是运行代码逻辑的最基本单位，每一段代码逻辑，都需要线程推动它被运行。 当你运行main方法时，jvm就已经自动开启一个线程帮你运行你main方法里的代码了，因为while(true)不会轻易结束，所以这个线程不会轻易结束，如果你输入0，break意味着跳出循环体，那么代码逻辑也就运行完毕了，此时线程自动销毁。 你之前写过的任何代码都是，只是它们不像上面的死循环那样由于自身逻辑特点不会让这个线程立刻销毁，程序一旦运行完毕，线程也就随之销毁了。 二、多线程一个main方法被运行后开启运行它的线程，我们管这个线程叫”主线程“，其实没必要分那么细，名称不重要，只需要记住，在任意线程里都可以产生子线程来帮助你做一些”异步化“的操作。 2.1：如何开启子线程？在java里开启子线程的方法有很多，我们只列举现在最常用的： 代码块2123456789public static void main(String[] args) { Thread t1 = new Thread(); t1.start(); //毫无意义的一个线程被创建并触发了，这个线程不会执行任何逻辑 //通过下面的方式可以加上你的逻辑代码，让程序变得有意义 new Thread(() -&gt; { //这里加上你需要异步的逻辑块 }).start();} 这样说太虚了，我们来举个例子： 代码块31234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public static void main(String[] args) { Thread t1 = new Thread(() -&gt; { int i = 1; //线程内计数器 while (true) { try { Thread.sleep(100L); //线程睡眠100ms System.out.println(\"线程1第\" + i + \"次打印\"); i++; if (i &gt; 10) { System.out.println(\"线程1打印完毕~\"); break; } } catch (InterruptedException e) { e.printStackTrace(); } } }); t1.start(); //启动线程1 Thread t2 = new Thread(() -&gt; { int i = 1; //线程内计数器 while (true) { try { Thread.sleep(100L); //线程睡眠100ms System.out.println(\"线程2第\" + i + \"次打印\"); i++; if (i &gt; 15) { System.out.println(\"线程2打印完毕~\"); break; } } catch (InterruptedException e) { e.printStackTrace(); } } }); t2.start(); int i = 1; while (true) { try { Thread.sleep(100L); //线程睡眠100ms System.out.println(\"main方法线程第\" + i + \"次打印\"); i++; if (i &gt; 12) { System.out.println(\"main方法线程打印完毕~\"); break; } } catch (InterruptedException e) { e.printStackTrace(); } }} 让我们来看看打印： 代码块412345678910111213141516171819202122232425262728293031323334353637383940线程1第1次打印main方法线程第1次打印线程2第1次打印线程1第2次打印线程2第2次打印main方法线程第2次打印线程2第3次打印线程1第3次打印main方法线程第3次打印main方法线程第4次打印线程2第4次打印线程1第4次打印main方法线程第5次打印线程1第5次打印线程2第5次打印线程2第6次打印线程1第6次打印main方法线程第6次打印线程2第7次打印main方法线程第7次打印线程1第7次打印main方法线程第8次打印线程2第8次打印线程1第8次打印线程2第9次打印main方法线程第9次打印线程1第9次打印线程1第10次打印线程1打印完毕~main方法线程第10次打印线程2第10次打印线程2第11次打印main方法线程第11次打印线程2第12次打印main方法线程第12次打印main方法线程打印完毕~线程2第13次打印线程2第14次打印线程2第15次打印线程2打印完毕~ 是不是觉得杂乱无章？没关系，我们来试图使用流程图来说明下它的运行过程： 我们这个程序里，一共包含三个线程： 运行main方法的线程 运行线程1逻辑块的线程 运行线程2逻辑块的线程 它们一起运行，然后各自执行自己的逻辑： 如图可知，线程是运行程序块的最基本单位，一旦线程启动后，它是跟其他线程一起运行的，这也是为什么你看到的处理结果那么杂乱无章的原因，但是你会发现，它就只是杂乱而已，仔细看每个线程的打印结果，它是没有问题的，多线程并发执行，仍然可以保证每个线程里的逻辑没有任何问题。 线程是如何看起来”同时“运行的呢？让我们来看张图： 看起来在一起执行的原因，是因为我们运行程序的核心，也就是CPU，它在不断的切换线程上下文，让这三个线程”看起来在同时运行“，你只需要关注绿线部分即可，因为不管CPU如何切换自己到不同的线程运行程序，但实际人肉眼看到的就是，它们都在同时运行，但你要知道，这是CPU切换着线程的上下文来运行的，我们常说的CPU利用率不高的问题主要就是我们利用单线程或较少的线程完成庞大复杂的程序，没有充分利用多线程的优势去解决问题。 2.2：多线程与java web我们刚刚讲了main方法，回忆一下我们前面说过的tomcat，我们的servlet程序是如何被触发运行的呢？我们再来贴一下之前servlet的代码，这次我们打印一下运行它的线程的名字： 代码块5123456789101112131415161718public class StudentInfoController extends HttpServlet { private StudentDao studentDao = new StudentDao(); @Override protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { resp.setContentType(\"application/json\"); try { Student result = studentDao.getStudentInfo(); resp.getOutputStream().write(JSON.toJSONBytes(result)); resp.setStatus(200); System.out.println(Thread.currentThread().getName()); //使用线程类的currentThread静态方法可以取到执行当前代码的线程对象 } catch (Exception e) { e.printStackTrace(); resp.setStatus(500); } }} 我们在接口发生调用时尝试打印一下执行该接口的线程的名字。 运行打印如下： 代码块61http-nio-8081-exec-5 它的名字叫http-nio-xxxx，这是tomcat运行时，用来分配执行业务逻辑的线程，也就是说，我们写的业务代码被放进tomcat身体后，就全部由tomcat触发了，所以运行我们servlet程序的线程肯定都由tomcat分配。 这跟执行main方法还不一样，main方法是直接由jvm分配的，而tomcat主程序肯定也是jvm分配线程运行的（之前说过tomcat本身也是java写的），然后tomcat自己作为独立运行的程序，就像我们开始举的那个main方法的例子一样，也可以产生线程，用来运行我们的servlet。那么我们是不是也可以在我们的接口里开线程异步处理一些事情呢？比如我们现在接口里有一段代码，假设需要耗时200ms，但是我们这个接口的响应时间要求不超过100ms，怎么办呢？异步就是最好的办法： 代码块712345678910111213141516@Overrideprotected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { resp.setContentType(\"application/json\"); try { Student result = studentDao.getStudentInfo(); //下面这段代码耗时很久，一下子要200ms，但我们这个接口需要在100ms就返回，而且这段耗时较久的逻辑还不影响你的主业务流程交互 Thread.sleep(200); //这里用线程睡眠的方式模拟运行时间200ms的程序段 resp.getOutputStream().write(JSON.toJSONBytes(result)); resp.setStatus(200); } catch (Exception e) { e.printStackTrace(); resp.setStatus(500); }} 如上，利用sleep模拟了一段运行超慢的代码块，注意这里只是模拟（⚠️ 注：类似这种代码逻辑特别复杂的代码只可能出现在Service层，这里只是为了说明问题写在了网关也就是servlet层）。怎么优化呢？首先上面已经说了，这段代码是每次都会运行，但是它不影响student_info的输出，那么就很适合把它异步出去，这样改即可： 代码块812345678910111213141516171819202122@Overrideprotected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { resp.setContentType(\"application/json\"); try { Student result = studentDao.getStudentInfo(); //异步 new Thread(() -&gt; { try { Thread.sleep(200); } catch (Exception e) { e.printStackTrace(); } }).start(); resp.getOutputStream().write(JSON.toJSONBytes(result)); resp.setStatus(200); } catch (Exception e) { e.printStackTrace(); resp.setStatus(500); }} 这样，我们这个接口的那200ms的复杂逻辑就会在不影响我们主题逻辑的响应速度的情况下，通过别的线程去运行了~ 三、进程前面讲完了线程，你就了解了，它是运行程序时的最小单位，由CPU来负责触发执行，它是有状态的，它可以被start触发进入运行状态，也可以通过sleep来由运行状态变成睡眠状态。那么进程是什么呢？ 简单来说，tomcat是一个进程，一个main方法在运行时是一个进程，你的QQ是一个进程，你的浏览器也是；进程就是我们所说的运行着的一个个程序，程序包含需要执行的指令码（我们写的java源代码被编译后也是一个个的指令，你可以理解它就是你编写的程序），包含多个线程（线程是运行这些指令码的最小单位，由CPU不断切换线程上下文来完成运行）。 你可以利用线程做很多神奇的事情，我们在后面的文章里将会介绍到。","link":"/2020/03/31/LV5-1%EF%BC%9A%E8%BF%9B%E7%A8%8B%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%E7%BA%BF%E7%A8%8B%E5%8F%88%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F/"},{"title":"LV4-5：网络通信、协议、序列化、程序之间的联系","text":"⚓️ 前排提示： 本篇第一部分的大部分内容来源： https://www.bilibili.com/video/BV1EW411u7th?p=28 https://www.bilibili.com/video/BV1EW411u7th?p=29 https://www.bilibili.com/video/BV1EW411u7th?p=30 本篇第一部分仅做计算机网络的简单介绍，旨在帮助你梳理你写的java业务程序在全局充当一个怎样的地位，另外一个目的是为了引入tomcat与java web程序，还有http是怎么回事，常说的建连和三次握手四次挥手又是指什么（如果你是计算机科班的话，对这些应该会比较熟悉，如果忘了也不要紧，这里会再重新帮你梳理一下）。看懂第一部分后，你会很容易的理解tomcat是什么，浏览器是什么，然后你甚至可以发散下思维，我们常说的RPC框架又是啥。看懂第一部分后，第二部分的序列化以及为什么要序列化以及json是什么就更加容易理解了，之后利用SDK连接操作mysql、redis等服务端，你也可以很容易理解，看懂第一部分后，你可以尝试自学下java的套接字Socket部分的内容，当然没兴趣也可以不学，因为幸运的是，tomcat等web服务器程序已经帮你规避掉这部分的开发了，你要做的是业务代码的开发 一、网络发展史你在计算机上请求一个网站的数据，需要发起一个http请求，然后你传输的参数以及path信息漂洋过海跑到该网站的服务器上（request），服务器收到你的请求后，找到对应的程序，解析出来你传的参数和path，这个时候该程序内部对应path的逻辑将被触发，到此为止，你已经成功进入服务端程序的内部了，经过一系列内部逻辑的处理，服务端将处理结果再次通过http协议将数据包发送到你的电脑上（response）。这个过程在网络畅通和对方服务端程序没有瓶颈的条件下，可能在极短的时间内就会完成（以毫秒甚至纳秒计），即便真实的服务器可能离你几百公里。 这个过程里发生了什么呢？ 1.1：局域网1.1.1：以太网简介毫无疑问，用户在全球网络中发送和接收信息的能力，永远改变了这个世界。150年前，发一封信件从伦敦到加州要花2~3周，而且还是特快邮件，如今，电子邮件只要几分之一秒，时延改变了上百万倍，振兴了全球经济，帮助现代世界在遍布全球的光纤中快速发展。你可能觉得计算机和网络密切相关，但事实上，1970年以前，大多数计算机是独立运行的，然而，因为大型计算机开始随处可见，廉价机器开始出现在书桌上，分享数据和资源渐渐变得有用起来，首个计算机网络出现了。 第一个计算机网络出现在1950~1960年代，通常在公司或研究室内部使用，为了方便信息交换，比把纸卡或磁带送到另一栋楼里更快速可靠，这叫“球鞋网络”，第二个好处是能共享物理资源，举个例子，与其每台电脑配一台打印机，大家可以共享一台联网的打印机，早期网络也会共享存储空间，因为每台电脑都配存储器太贵了，计算机近距离构成的小型网络叫做局域网，简称LAN，局域网能小到是同一个房间里的两台机器，或大到校园里的上千台机器，尽管开发和部署了很多不同的LAN技术，其中最著名和成功的是“以太网”（英文名称：Ethernet），开发于1970年代，在施乐的“帕洛阿尔托研究中心”诞生，今天仍被广泛使用。以太网的最简单形式是：一条以太网电线连接数台计算机： 1.1.2：数据传输当一台计算机要传数据给另一台计算机时，它以电信号形式，将数据传入电缆，当然，由于电缆是共享的，连在同一个网络里的其他计算机也能看得到数据： 如图2由A发出的数据会被BCDEF接到，正因如此，它们不知道A发出的这些数据是给它们中的哪一个的，为了解决这个问题，以太网需要每台计算机有唯一的媒体访问控制地址，简称MAC地址，这个地址放到数据的头部，作为数据的前缀发送到网络中，所以计算机只需要监听以太网电缆，只有匹配到自己的MAC地址才去处理数据，这个过程如下： 这运作的很好，现在制造的每一台计算机都自带唯一MAC地址用于以太网和无线网络。 多台电脑共享一个传输媒介，这种方法叫做“载波侦听多路访问”，简称“CSMA”，这里的载体指的就是运输数据的共享媒介，以太网的载体是铜线，WIFI的载体是传播无线电波的空气。很多计算机同时侦听载体，所以叫“侦听”和多路访问，而载体传输数据的速度叫“带宽”。 1.1.3：冲突域不幸的是，使用共享载体有个很大的弊端，当网络流量较小时，计算机可以等待载体清空，然后传送数据，但随着网络流量的上升，两台计算机想要同时写入数据的概率也会上升，比如A机和F机同时发出一个数据包： 看，共享载体冲突了。就像两个人打电话，他们同时在电话里讲话一样。幸运的是，计算机能够监听电线中的信号检测这些冲突，最明显的解决办法是停止传输，等待网络空闲，然后再试一次，问题是，其他计算机也打算这样做，其他等待着的计算机可能在任何停顿间隙闯入，导致越来越多冲突，很快每个人都一个接一个的讲话，而且有一堆事要说，就像在家庭聚餐时和男朋友分手一样，这可不是个好主意。以太网有个超简单有效的方法，当计算机检测到冲突，就会在重传之前等待一小段时间，因为要举例，假设是一秒好了，当然，如果所有的计算机都用同样的等待时间是不行的，因为它们会在1s再次冲突，所以这个时间是随机的，这样就保证了每台机器下次重试的时间都不一样，然后大幅度降低冲突概率，这有用，但不能完全解决问题，所以要用另一个小技巧，正如我刚才说的，如果一台计算机在传输数据期间检测到冲突，会等1秒+随机时间，然而如果再次发生冲突，则表明有网络拥塞，这次不再等1s，而是2s，如果再次发生冲突，则等4s、8s、16s以此类推，直到成功传输为止。因为计算机的这种退避策略，冲突次数减少了，数据再次流动，网络再次畅通了，家庭晚餐有救了~这种指数级增长等待时间的方法叫做：指数退避。以太网和WiFi都用这种方法，很多其他传输协议也用。 但即便有了指数退避这种技巧，想用一根网线连通整个大学的计算机是不可能的，为了减少冲突+提升效率，我们需要减少同一载体中设备的数量，载体和设备统称“冲突域”（上面的例子就是一个冲突域，因为6个设备共享一个载体） 1.1.4：利用交换机降低冲突域大小仍然以上面的例子为基础，例子中冲突域设备数量为6，我们利用交换机把它拆为两个冲突域，这样，单位冲突域内设备数量就会降低，冲突发生的概率也会大幅下降： 交换机位于两个更小的网络之间，必要时才在两个网络间传数据，交换机会记录一个MAC地址列表，记录哪个MAC地址位于哪边的网络，所以，我们现在让A发送数据到C，交换机会控制不再让DEF接收到数据： 如果同时E想要发送数据到F，网络就依然是空的，不会跟A→C这个网络传输冲突。那如果F想要传数据给A呢？这时仍然跟之前是一样的，整个通道都会被占用： 大型的计算机网络也是这样构建的，包括大的网络：互联网，也是由多个连在一起的稍小的网络组成，使不同网络间信息互相传递。 这些大型网络比较有趣的一点是从一个地点到另一个地点通常有多条线路，这就带出了另一个话题：路由。 1.1.5：路由连接两个相隔遥远的计算机或者网络，最简单的办法就是分配一条专用的通信线路，早期电话系统就是这样运作的，比如如果在1910年的北京和上海，北京的小明想要打电话给上海的小红，而北京到上海的电话线只有5条： 这叫“电路交换”，因为是要把电路连接到正确的目的地，能用是能用，但是不灵活，而且价格昂贵，因为总有闲置线路，好处是，如果有一条专属于自己的线路，你可以最大限度的随意使用，无需共享，因此军队、银行和其他一些机构依然会购买专用线路来连接数据中心。 传输数据的另一种方法是“报文交换”，报文交换就像邮政系统一样，不像之前A和B有一条专有线路，而是信息会经过好几个站点，比如从北京发一封信到上海可能会中转以下线路： 每一个站点都知道下一个站点将会发往哪里，因为站点有表格，记录到各个目的地，信件该怎么传。报文交换的好处是可以使用不同的路由使信件传输更可靠更能容错，比如让图9中的济南站点断路，那么此时换另外一条传输线路仍然是ok的，虽然不是最优解，但确实解决了现在济南站点无法工作的问题： 看，转郑州线路也是个不错的选择，至少你的信件最终还是顺利到达了上海。 在上面送邮件这个例子中，每个城市邮局站点就像路由器一样，消息沿着路由跳转的次数叫“跳数”，记录跳数很有用，因为可以分辨出路由问题。 现在假设北京传输信息到上海，假设转至郑州，郑州认为去往上海的最快线路应该是中转济南这条，但是济南认为去往上海的最佳路线应该是郑州，这就糟糕了，因为两个城市看到的目的地都是上海，它们互相认为对方是“最优解”，然后这个信息在济南和郑州之间转来转去： 图中的问题不仅浪费带宽，而且这种路由错误需要被修复。这种错误会被检测到，因为跳数被记录在消息中，传输时会更新跳数，如果看到某条消息的跳数很高，就可以知道路由肯定哪里出错了，这叫做“跳数限制”。 报文交换的缺点之一是有时报文比较大，会堵塞网络，因为要把整个报文从一站转到下一站后，才能继续传递其他报文，传输一个大文件时，整条路都堵塞了，这时即便你有一个只有1kb的电子邮件要传输，也只能等大文件传完，或是选另一条效率稍低的线路，这就糟了，解决方法是将大报文分成很多小块，这些小数据块就是“数据包”，就像报文交换，每个数据包都有目标地址，因此路由器知道下一站该发往哪里，报文具体格式由“互联网协议”定义，简称IP，这个标准创建于1970年代，每台联网的计算机都需要有一个自己的IP地址，你可能见过，以点分割的4组数字，例120.92.174.135就是哔哩哔哩一个服务器的IP地址。 数百万计算机在网络上不断交换数据，瓶颈的出现和消失是毫秒级的，路由器会平衡和其他路由器之间的负载，以确保传输可以快速可靠，这叫做“阻塞控制”，有时同一个报文的不同数据包会经过不同的线路： 最终上海收到了全部由北京发出的数据，但是四个数据包传过来时经历的路由路径可能不一样（图中线路颜色对应具体数据包颜色）。 但是拆包意味着有另外一个问题：到达顺序问题。这对于一些软件来说这是个大问题，比如图12里最终到达顺序变成了下面这样： 这该怎么办呢？幸运的是，在IP协议之上还有其他的协议，例如TCP/IP，就可以解决乱序问题（TCP/IP下节会说）。 将数据拆分成多个小数据包，然后通过灵活的路由传递，非常高效且可容错，如今互联网就是这么运行的，这叫做”分组交换“，有个好处是，它是去中心化的，没有中心权威机构，没有单点失败问题，事实上，在冷战时期有核攻击威胁，所以创造了分组交换，如今，全球的路由器协同工作，找出高效的线路，用各种标准协议运输数据，比如”因特网控制消息协议“（ICMP）和”边界网关协议“（BGP），世界上第一个分组交换网络以及现代互联网的祖先是”ARPANET“。 互联网非常快速的发展，如今不再只有几十台计算机联网，据估计，有接近100亿台联网设备，而且互联网会持续快速发展，特别是如今智能设备层出不穷，比如联网冰箱、恒温器，以及其他智能家电，它们组成了”物联网“。 1.2：广域网&amp;高级协议1.2.1：广域网计算机想要从网络里获取一段信息，首先要连到局域网，也叫LAN，你家WiFi连接着的所有设备，组成了局域网，局域网再到广域网，广域网也叫WAN，WAN的路由器一般属于你的”互联网服务提供商“，简称ISP，中国这种提供商主要有：中国电信、中国移动、中国联通等，广域网里，先连接到一个区域性路由器，这个路由器可能覆盖了一个街区，然后再连到一个更大的WAN，可能覆盖了整个城市，可能会再跳几次，但最终到达互联网主干，互联网主干由一群超大型、带宽超高的路由器组成。 为了从一个网站获取到html信息或者接口信息，请求数据包要先到互联网主干，沿着主干到达有对应内容的目标网站服务器，请求数据包到从你的电脑跳到目标网站服务器，可能要跳个十几次，让后找到对应的程序，将程序内对应路径里目标html文件或json接口数据再按照类似的方式传送回你的电脑上. 上节讲到，大的数据会被拆分成一个个的小数据包，然后经过不同的路由线路，最终抵达目的地，数据包要想在互联网上传输就要符合”互联网协议“的标准，简称IP，就像邮快递一样，快件的打包、重量都是要符合快递公司标准的，比如每个邮包必须要有发件地址和目标地址，而且地址是独一无二的（就像中国地名中按照省/市/县/镇/村的规则，没有任何一个地名是完全重复的），并且要求邮包的大小和重量也符合标准，违反这些标准和规范，邮包将无法被邮寄，事实上，IP数据包也是如此，因为IP是一个非常底层的协议。 数据包的头部只有目标地址： 仔细看上图的包结构，这意味着这个数据包抵达目标机器后，对方并不知道要把这个服务包给自己的哪个程序（是给tomcat这个http服务程序呢？还是给自己的文件处理程序呢？不知道，因为你只有一个IP头，它只能保证你的数据可以到达目标机器），因此需要在IP之上，开发更加高级的协议。 1.2.2：UDP协议这些高级协议里最简单最常见的叫”用户数据报协议“，简称UDP，有了这个协议，我们的数据包进化成了下面这样： UDP协议头位于数据的前面，这个协议头里包含一些很重要的信息： 看，我们熟悉的端口号出现了。每个想访问网络的程序都要向操作系统申请一个端口号，比如tomcat这个java开发的http程序默认端口号就是8080，对外提供http服务的nginx程序的默认端口号就是80，当一个数据包到达时，接收方的操作系统就会读取UDP头里的端口号，如果看到PORT=8080，那么就把数据包交给tomcat，如果是80，则交给nginx，看，到现在我们就可以利用这俩协议互相配合，完成一个客户端到一个服务端上某个程序的访问了。 ⭐️ 总结： IP负责把数据包送到正确的计算机 UDP负责把数据包送到正确的程序 我们看到图16里UDP头除了PORT，我还列出了一个叫CHECK_NUM的东西，它叫做”校验和“，用于检查数据是否正确，正如校验和这个名字所暗示的，检查方式是把数据求和来对比，以下是个简单的例子： 值得注意的是，校验和只是一个16位的数（bit位），一旦累加和超过16位所能表达的最大范围，跟我们前面讲位运算时的位溢出一样，舍弃高位，保留低位。校验和用来判断数据的准确性，如果接收方收到的数据计算出的校验和跟发送方不一致，则说明数据被篡改过或由于电缆波动导致数据损坏过。不幸的是，UDP不提供数据修复或者数据重发的机制（划重点），接收到对方数据损坏后，一般只是扔掉（丢包），而且UDP无法得知数据包是否到达，发送方发送数据后无法感知对方是否已经接收到，这些特征听起来很糟糕，但有些程序其实并不在意这些问题，因为UDP又简单又快，对于那些有着高传输性能低完整性的系统（比如我们的日志系统和链路追踪系统），使用UDP协议开发服务端是个不错的选择，但是针对业务服务，对数据有着高精准性，不允许丢包的发生，对数据完整性要求极高，则不建议使用UDP，比如java开发的tomcat http服务程序，它就不是基于UDP协议完成通信传输的。 如果需要保证数据的完整性，即：所有数据必须到达，那么就可以使用”传输控制协议“，简称TCP。 1.2.3：TCP/IP协议TCP和UDP一样，头部也在存数据前面，就像UDP一样，TCP头部也有”端口号“和”校验和“，但TCP有更高级的功能，我们这里只介绍重要的几个： TCP数据包有序号，15号后是16号，16号后是17号，以此类推，发上百万个数据包也是有可能的（事实上，TCP在建连时会确定初始序列号，被称作Initial Sequence Number，简称ISN，这个序号是根据系统时间来的，下面建连三次握手会讲到），还记得上面图13里的拆包后的到达顺序错乱的问题吗？序号可以使接收方把数据包排列成正确的顺序，即使到达顺序不同，TCP协议也能把顺序排对。 TCP要求接收方的电脑收到数据包并且”校验和“检查无误后（数据没有损坏）给发送方发一个确认码，代表收到了，确认码简称ACK，得知上一个数据包成功抵达后，发送方会发下一个数据包： 数据包2在第一次发送时可能由于网络延迟，可能中途发送失败，这不重要，重要的是发送方确实没有收到接收方的ACK消息，此时发送发会重试。注意，这样有可能发送重复的数据包，因为没有收到ACK并不意味着接收方100%没有收到数据包2，好在我们有TCP序列号，如果收到重复序列号的数据包就会删掉，只保留一个。图18里看到的是一个个的数据包发送，但事实上，TCP是支持多个数据包同时发送的然后收取多个ACK码的，这大大增加了效率，不用浪费时间等待确认码： 有意思的是，确认码的成功率和来回时间可以推测网络的拥堵程度，TCP利用这个信息，调整同时发包的数量，解决拥堵问题（可以了解下TCP的窗口滑动协议），简单来说，TCP可以处理乱序和丢失数据包问题，丢了就重发，此外还可以根据拥堵情况自动调整传输率，相当厉害。你可能会奇怪，既然TCP那么厉害，为什么还有人用UDP呢？TCP最大的缺点是那些ACK确认码，交互的数据包的数量翻了一倍，但并没有传输更多信息，有时候这种代价是不值得的，特别是对时间要求很高的程序，比如前面提到的日志系统和链路追踪系统，它们可能对数据完整性要求没有业务系统那么高，但是传输性能一定要保证高速，这时候就特别适合UDP传输协议。图18和图19都是在TCP建连后的数据传输阶段，在这之前，你要想在TCP/IP上传输数据首先要做的就是建连，建连需要三次握手，如果数据传输完毕，数据发送方不会再和数据接收方有任何交互，这是其中一方会断开这个连接，这时又需要通过四次挥手来断开连接：三次握手、四次挥手 1.2.4：DNS服务这个大家应该很熟悉了，简单来说就是互联网的一个特殊服务，负责把域名和某个IP地址对应起来，我们并不希望在访问网站时输入一大串的数字，往往我们希望每个网站都有个自己的名字，毕竟bilibili.com记起来可比什么xxx.xxxx.xxx.xx容易多了。DNS就是做这个的，它可以将你记下来的域名解析出来它对应的IP地址，一般DNS服务器是互联网供应商提供的，DNS会查表，如果域名存在，就返回IP地址。你在浏览器输入bilibili.com就会解析到具体的一个IP地址，然后浏览器会给这个IP地址发请求，每个请求的数据传输都会基于一个或多个TCP连接完成，上层解析数据的工作则交给HTTP这个应用层协议去做。 1.2.5：网络协议分层物理层：之前说过的电缆、无线电信号这些属于物理层 数据链路层：负责操控物理层，数据链路层有包含媒体访问控制地址（MAC地址），碰撞检测（之前说过的传输冲突），指数退避以及一些其他的底层协议，在往上一层是网络层。 网络层：负责各种报文交换和路由 传输层：上面刚讲的TCP和UDP都属于传输层协议，具体来说传输层负责在计算机之间进行点对点的传输（port to port），而且还会检测和修复错误。 会话层：利用TCP/UDP来创建连接，传递信息，然后关掉连接，这一整套叫”会话“ 表示层&amp;应用程序层：HTML解码、浏览器等。 让我们来看看详细的分层结构图，目前有两种分层方式，我列举出一些具体的实现协议： 这个概念性框架，把网络通信划分成多层，每一层处理各自的问题，如果不分层，直接从上到下捏在一起实现网络通信，是完全不可能的，看，这个过程也是抽象&amp;分层，跟java里的抽象分层一样，软件分层也是同样的道理，通过抽象和分层让科学家和工程师们能各自分工同时改进多个层，不被整体复杂度难倒。 但我们目前会以TCP/IP协议为中心分的粒度更加粗，只有四层： 应用层：最接近我们软件开发的一层，需要软件工程师亲自设计实现的一层，我们常见的有HTTP、HTTP2、FTP。 传输层：利用应用层协议打包好的数据，通过传输层传输，就跟我们前面解析TCP和UDP工作流程时是一样的，它们只用来发送数据包。 网络层：负责将数据包运送至互联网，传输到目标机器。 网际接口层：包含了数据链路层和物理层。 1.3：万维网1.3.1：万维网简介万维网不是互联网！万维网不是互联网！万维网不是互联网！尽管人们经常混用这两个词，万维网在互联网之上运行，互联网之上还有A站B站、知乎微博、王者荣耀等，互联网是传递数据的管道，各种程序都会用，其中传输最多数据的程序是万维网，分布在全球上百万个服务器上，你可以用浏览器访问万维网，本节会详细介绍万维网。 1.3.2：超链接万维网的最基本单位，是单个页面，页面有内容，也有去往其他页面的链接，这些链接叫做”超链接“，大家都见过：可以点击的文字或图片，点击后把你送往另一个页面，这些超链接行成巨大的互联网络，这就是万维网名字的由来，现在说起来觉得很简单，但在超链接做出来之前，计算机上每次想看另一个信息时，你需要在文件系统中找到它，或是把地址输入搜索框，但有了超链接，你可以在相关主题间轻松切换，比如这样：（对，点一下我，你就可以离开我去百度了）。 超链接的价值早在1945年就被Vannevar Bush意识到了，他解释道：将两样东西联系到一起的过程十分重要，在任何时候，当其中一件东西进入视线，只需要点一下按钮，立马就能回忆起另一件，1945年的时候计算机连个显示屏都没有，所以这个想法在当时是非常超前的，因为文字超链接是如此的强大，它得到了一个同样厉害的名字：超文本。 如今超文本最常指向的，是另一个网页，然后网页由浏览器渲染（下面会详细讲），为了使网页能相互连接，每个网页需要一个唯一的地址，这个地址叫”统一资源定位器“，简称URL（这东西很熟悉吧）。 1.3.3：定位一个超文本页面结合我们前面介绍的，我们来看看浏览器如何定位到一个超文本信息： 假如你要访问：https://www.bilibili.com/anime 这个时候会再次帮你巩固下TCP/IP的作用，我们常说的”建连“其实就是指TCP/IP连接的建立，没有TCP/IP连接，你的数据就无法和服务端进行准确的点对点的交互，而负责把你数据输送到对端又是通过基于IP协议的路由功能完成的，看，又串起来了。每一层网络协议都在我们上网时承载着不同的作用。 ⚠️ 注意：图里虽然管TCP叫长连接，因为TCP在大部分应用层协议下都是不断开的，但是！但是！但是！由于HTTP协议的实现，TCP会很快断掉，你知道吗？TCP每次建连需要三次握手，下次访问时发现TCP已断开，访问前就会再次建连经历一遍三次握手，简单来说就是影响性能，这也是HTTP协议的一个弊端，所以后来HTTP协议通过keep-alive的方式进行优化，让长连接在断开前尽可能多的被复用。 依照这个流程，我们重点讲一下位于最上层的协议：HTTP超文本传输协议。 1.3.4：HTML、HTTP协议首先，它是一个应用层协议，位于网络通信的最上层，下层仍然通过TCP/IP协议完成网络传输，那么HTTP是什么样的一种协议呢？它在通信过程里起到的作用是什么呢？ HTTP的第一个标准，HTTP0.9，创建于1991年，当时只有一个指令：GET，幸运的是，这对当时来说也够用，如图21，因为我们想要的是b站的/anime页面，所以我们向服务器发送指令：GET /anime，该指令以ASCII码发送至Web服务器，服务器会返回该地址对应的网页，然后浏览器将其渲染到屏幕上，此时在该页面用户再次点了另一个链接，计算机会重新发送一个GET请求，你在浏览网站时，这个过程会不断重复。在之后的版本，HTTP添加了状态码，状态码放在服务端返回消息体的前面，举例，状态码200代表”网页找到了，给你“，状态码400~499代表客户端出错，比如网页不存在，也就是可怕的404错误（-_-||）。 超文本的存储和发送都是以普通文本形式进行，比如，编码有可能是ASCII码或者UTF-16，因为如果只有纯文本，就无法表明什么是链接，什么不是链接，所以有必要开发一种标记方法，于是超文本标记语言HTML就这样诞生了（看，我们甚至讲到了前端），HTML第一版的版本号是0.8，创建于1990年，有18种HTML指令，仅此而已，我们来做一个网页吧！ 代码块112&lt;h1&gt;这是标题&lt;/h1&gt;这是内容，&lt;a href=\"http://www.bilibili.com\"&gt;这是超链接&lt;/a&gt; h1就是一个HTML指令，它可以告诉浏览器，被它包裹的内容的字体需要变成1号标题大小，而a指令可以令被包裹的内容跳到其他页面。 这样，我们的HTML页面就完成了，把它保存到一个记事本里面，然后把记事本的后缀改成.html，它就可以运行在浏览器上了，说白了，你请求一个网站也是类似的道理，只不过需要走网络，将目标网站对应的HTML页面拿到。 最新版的HTML、HTML5，有100多种标签指令，图片标签、表格标签、表单标签、按钮标签等等，还有其他相关技术就不说了，比如层叠样式表（CSS）和JavaScript，这俩可以加进网页，嵌入进你的HTML代码里，做一些更加厉害的事情。 让我们回到浏览器，网页浏览器可以和网页服务器（HTTP服务器）通信，浏览器不仅获取网页和媒体，获取后还负责显示。 1.3.5：浏览器、web服务器的发展史浏览器：基于HTTP协议完成和web服务器的网络通信，并将服务端返回的超文本内容进行渲染展示。 web服务器：支持HTTP协议的一个服务端程序，对，它只是一个服务器上运行的一个拥有端口号的程序而已，就像前面讲的网络通信一样，因为它支持HTTP协议的解析和封装传输，因此浏览器可以和它进行无障碍通信，这个流程简单画一下就是下面这样： 上面省去了DNS这一步，对浏览器和web服务器程序通信的整个逻辑流程，仔细分析下，为什么web server会知道浏览器想要访问哪个文件呢？以及浏览器如何知道服务端的响应结果的？万一服务端找不到该咋办？这就是HTTP协议起到的作用，按照HTTP协议打包的数据，会告诉服务端当前请求资源的方式（GET、POST），还会告诉服务端请求的路径和参数，根据路径和参数找到对应的文件，然后包装成HTTP响应数据，返回给浏览器，如果没找到对应的路径，则返回404错误码，这时浏览器就知道了服务端找不到它要请求的界面。 看到这里，知道什么是应用层协议了吧？所谓应用层协议，其实就是软件内部处理对端数据时约定好的一种格式，便于符合同一种通信协议的两种软件相互通信（比如浏览器和web服务器程序都属于某种软件，或称应用，它们为什么可以顺利处理彼此通过TCP/IP传来的数据呢？因为它们都支持HTTP这种应用层协议，解析规则一致，所以它们都可以理解对方，比喻成人的话，就是他们俩虽然不是同一类人，但是他们俩是一个国家的人，能够听懂彼此的语言，而语言相对于人类来讲，也是一种”协议“性的东西）。 第一个浏览器和服务器，是Tim Berners-Lee在1990年写的，一共花了2个月，那时候他在瑞士的”欧洲核子研究所“工作，为了做出来，他同时建立了几个最基本的网络标准：URL（概念）、HTML（语言）、HTTP（协议），两个月。。就。。过于优秀！不过公平点说，他研究超文本系统已经有十几年了，和同事在CERN内部使用了一阵子后，在1991年发布了出去，万维网就此诞生！重要的是，万维网有开放标准，大家都可以开发新的服务器和新的浏览器，因此”伊利诺伊大学香槟分校“的一个小组，在1993年做了Mosaic浏览器，它是第一个可以在文字旁边显示图片的浏览器，之前的浏览器要单开一个页面专门展示图片，还引进了书签等新功能，界面友好，使它一度很受欢迎，它长这样： 尽管以现在的眼光看上去很呆板，但和如今的浏览器长得也差不多。 1990年代末有许多浏览器面世：IE、Opera、OmniWeb、Mozilla，当然也有很多基于HTTP协议的web服务器面世，比如Apache（重点）和微软互联网信息服务（IIS）。 基于这些成果，每天都有新网站冒出来，如今的网络巨头，比如亚马逊、eBay，创建于1990年代中期，而在中国，1998年诞生了腾讯，1999年诞生了阿里巴巴，那是个黄金时代！ 1.3.6：第一个爬虫程序&amp;第一个搜索引擎随着万维网日益繁荣，人们越来越需要搜索，如果你知道网站地址，比如bilibili.com，直接输入浏览器就行，但如果不知道地址呢？比如想找可爱猫咪的图片，现在就要！去哪里找呢？起初人们会维护一个目录，链接到其它网站，其中最有名的叫”Jerry和David的万维网指南“，1994年改名为Yahoo（没错，就是早期的雅虎搜索），随着网络越来越大，人工编辑的目录变得不便利，所以开发了搜索引擎。 长得最像现代搜索引擎的最早搜索引擎，名叫JumpStation，由Jonathan Fletcher于1993年在斯特林大学创建，它有3个部分： 第一个是爬虫，一个跟着链接到处跑的软件，每到看到新链接，就加进自己的列表里 第二部分是不断扩张的索引，记录访问过的网页上，出现过哪些词 最后一部分，是查询索引的搜索算法，比如我输入”猫“，那么每个有”猫“这个词的网页都会出现 早期搜索引擎的排名方式非常简单，取决于搜索词在页面上的出现次数，刚开始还行，直到有人开始钻空子，比如在页面上写几百个”猫“，把人们吸引过来，谷歌的成名的很大一个原因是创造了一个聪明的算法来规避这个问题。与其信任网页上的内容，搜索引擎会看其他网站，有没有链接到这个网站，如果只是写满”猫“的垃圾网站，没人会愿意指向它，如果有关于”猫“的有用内容，有网站会指向它，所以这些”反向链接“的数量，特别是有信誉的网站会特别多，反向链接的数量代表了网站的质量。 Google一开始时是1996年斯坦福大学一个叫BackRub的研究项目，两年后分离出来，演变成如今的谷歌。 1.4：我自己的个人电脑联通网络后可以被别的电脑通过ip+port访问到吗？我们的常识告诉我们，不行！ 为什么呢？前面说到了局域网，或者说我们管它叫内网，我们的个人电脑只是接在了局域网里。一个公司里的电脑，一个学校里的电脑，都可以组成一个局域网，而一个局域网内的计算机，都会有个内网IP，它们通过NAT（网络地址转换）实现内网的IP地址与公网的IP地址之间的相互转换，将大量的内网IP地址转换为一个或少量的公网IP地址，减少对公网IP地址的占用，NAT的最典型的应用是：在一个局域网内，只需要一台计算机连接上Internet，就可以利用NAT共享Internet连接，使局域网内其他计算机也可以上网。使用NAT协议，局域网内的计算机可以访问Internet上的计算机，但Internet上的计算机无法访问局域网内的计算机，这就是你连上局域网的电脑为什么不可以被别人访问的原因。 那么如何让自己电脑上的某个程序让别人可以访问到呢？需要配置部署一个公网IP地址，公网IP是指以公网连接Internet上的非保留地址。 公网、内网是两种Internet的接入方式，公网的计算机和Internet上的其他计算机是可以随意互相访问的。 ⚜️ 总结： 通过公网部署的计算机可以访问互联网里的任意计算机。 仅链接进局域网的计算机可以被公网计算机连接并访问，但是无法被别的计算机主动连接访问。 所以如果你想要建设一个个人站，首先要找到一个接入公网的计算机当做服务器，否则别人是无法访问你的，这种计算机在哪里买到呢？当然是阿里云啊~（不是广告，腾讯云也提供这种服务，这些服务按照职能不同还分了很多种类，相比阿里云这种提供全方位服务器的商户，七牛云和又拍云就只提供云存储和CDN服务） 如果你在阿里云购买了一台服务器，他会给你两个ip，一个是公网ip，一个是内网ip： 你如果也注册了域名（在中国需要给域名完成备案，相当于给你的域名搞个“身份证”），就可以将公网ip绑定你的域名让别人访问了~ 1.5：结束语第一部分的内容到这里就结束了，最后我想讲一个词，你最近可能经常听到，网络中立性（Net Neutrality），现在的你对数据包、路由、万维网应该有了个大体的认识，足够你理解这个争论的核心点，至少从技术角度上来说。 简单说网络中立性是应该平等对待所有数据包，不论这个数据包是我的邮件，或者是你正在看的视频，速度和优先级应该是一样的，但很多公司会乐意让他们的数据优先到达，拿Comcast举例，他们不仅是大型互联网服务提供商，而且拥有很多家电视频道，比如NBC、The Weather Channel等，可以在线观看，我不是特意要找Comcast麻烦，但要是没有网络中立性，Comcast可以让自己的内容优先到达，节流其他的线上视频（节流是指故意给更少带宽和更低的优先级）。 支持网络中立性的人说，没有中立性后，服务商可以推出提速的”高级套餐“，给剥削性商业模式埋下种子，互联网服务供应商称为信息的”守门人“，他们有着强烈的动机去碾压对手，另外，网飞和谷歌这样的大公司可以花钱买特权，而小公司，比如刚成立的创业公司将会处于劣势，阻止了创新。 另一方面，从技术原因看，也许你会希望不同的数据传输速度不同，你希望b站看视频优先级更高，而企业微信发的邮件慢几秒没关系，而反对网络中立性的人则认为，市场竞争会阻碍不良行为，如果供应商把客户喜欢的网站降速，客户会选择离开供应商，这场争辩还会持续很久，你应该学会去独立思考，主动获取一些信息之后再发表自己的看法，因为网络中立性十分复杂和广泛。 二、序列化2.1：序列化是做什么的？回到我们之前讲过的TCP/IP传递的数据包的基本结构，它长下面这样： 抛去带领我们数据路由的IP协议头和找到具体程序的TCP协议头，剩下的部分就是我们所要传输的具体数据流，我们假设现在浏览器发起一个请求，为了方便表述，这个请求只发一个数据包到我们的web server（端口号假设为80）程序上，它的过程是这样的： 前面说过，浏览器和服务器都是支持解析HTTP协议的（这里是不是更加理解为什么HTTP是应用层协议了呢？你看看上图里HTTP的解析，都是发生在实际应用里解析的），但是实际参与两个应用做业务处理的这些业务数据才是我们最终最终需要关心的数据，因为它们关系着我们的业务代码的走向，那么业务DATA也是个二进制字节流啊，那可怎么办？这意味着浏览器和服务端都无法解析出最终的业务数据应该是什么，举个我们常用的例子（为了方便描述问题，我们省去了应用层以下的流程）： 我们抽象了两种常用的HTTP请求方式，一种是获取信息的GET请求，一种是提交表单信息的POST请求，但是你会发现，GET在根据id查到season信息后，需要返回给浏览器，而浏览器也必须得识别后端程序发过来这个字节流的数据结构，才能进一步做业务处理，同样的，POST请求这个问题则表现得更加彻底，首先浏览器表单填写完成后，传的参数也是一个结构体，服务端处理完成后也会返回一个应答结构体供浏览器做自己的业务处理。 好的，这个过程我们梳理了下，回想下，浏览器里负责发请求以及解析后端返回数据的语言叫啥？嗯，就是JavaScript，而我们前面说过，web server又是由别的语言来实现的，比如tomcat，它是由java开发的，那现在问题简化多了： 上图的问题，被我们用json解决了，这就是json的意义，json是一个跨语言的序列化方式，js本身就支持json编解码，java也有自己的json编解码工具：fastjson、gson等等，这样，JS可以解析JAVA传过来的字节流，JAVA也可以解析JS传来的字节流。 根据上图，简单理解就是： 序列化就是以某种算法方式让自己的数据结构对象变成一个二进制字节流 反序列化就是以某种算法方式就是让一个以同样算法编码的字节流变成自己的一个数据结构对象 说人话就是，java可以让浏览器传来的字节流变成自己的一个类的对象，类就是java的数据结构，而js也可以通过ajax获取到的服务端字节流变成自己可识别的对象结构。 这种算法就是我们常说的序列化算法，目前有java自己的序列化算法、json、protobuf等。 发散下思维，如果图27里浏览器通信的也是JAVA代码写的程序而不是JS，会不会也有序列化问题？当然有啦，说到底为什么要序列化反序列化，还不是因为网络传输只能是二进制字节流，只要跨进程了，这个问题就固然存在，我们刚刚列举了一些序列化方式，json做到了跨语言，比如js支持，java也支持，go也支持，php也支持，所以json很厉害，而java也有自己的序列化方式，这种方式解码程序仅适用于java程序自己，它做不到让js也理解。 2.2：JAVA是如何做序列化的？通过上一节的理解，序列化就是让自己变成一串二进制字节流，反序列化就是让一串二进制字节流变成一个java对象，那我们现在用java的传统艺能来做下这个事情，还记得之前画的java io家族继承图吗？java用来序列化的内容也属于这一块的内容，它是由这俩类来完成的： 我们不能因此做一个网络传输，太费劲了，我们可以利用文件io来做这个事情（本质上没什么不同，读网络传进来的io字节流跟读文件里面的字节流效果是一致的，只不过一个是从网关接收数据，一个是从文件接收数据），我们定义一个类： 代码块21234567891011121314151617181920212223242526272829303132public class Season implements Serializable { //通过实现Serializable接口，来标记该对象是允许被序列化的 private int season_id; private String title; private transient String index; //被transient修饰的字段不参与序列化和反序列化 public int getSeason_id() { return season_id; } public void setSeason_id(int season_id) { this.season_id = season_id; } public String getTitle() { return title; } public void setTitle(String title) { this.title = title; } public String getIndex() { return index; } public void setIndex(String index) { this.index = index; }} 代码块3123456789101112131415161718192021222324252627282930313233public class Test { public static void main(String[] args) throws Exception { //new一个Season对象 Season season = new Season(); season.setSeason_id(1); season.setTitle(\"寒蝉鸣泣之时\"); season.setIndex(\"索引1\"); serializable(season); //把它序列化到一个文件里去 Season season2 = deSerializable(); //从文件读取被序列化好的season对象的字节流，然后反序列化成一个新的对象 System.out.println(\"season_id = \" + season2.getSeason_id() + \" title = \" + season2.getTitle() + \" index = \" + season2.getIndex()); System.out.println(season == season2); //这是俩不同的对象，结果为false } //序列化 public static void serializable(Season season) throws Exception { ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(new File(\"/Users/sunqinwen/Documents/season.txt\"))); //我们把这个对象写进对应的文件 oos.writeObject(season); System.out.println(\"对象序列化成功！\"); oos.close(); } //反序列化 public static Season deSerializable() throws Exception { ObjectInputStream ois = new ObjectInputStream(new FileInputStream(new File(\"/Users/sunqinwen/Documents/season.txt\"))); //我们把文件里的字节流读取进来进行反序列化 Season result = (Season) ois.readObject(); return result; } } 运行结果： 代码块4123对象序列化成功！season_id = 1 title = 寒蝉鸣泣之时 index = nullfalse 可以看到，被transient修饰的index字段并没有参与序列化和反序列化，现在让我们看下season.txt这个文件里的内容： 你没有必要读懂这段内容，你只需要知道，这块内容是java按照某种规则生成的内容，这段内容被以字节流的方式读进一个java程序里，再通过java反序列化类进行解码，它就可以得到一个你想要的对象。 2.3：FastJson与json序列化我们在2.1里讲过json是一个跨语言的序列化方式，它有着人可以读懂的字符串结构，将这些符合json结构的字符直接录入文件，然后让java读取出来，通过java的fastjson工具包，就可以将这个json字节流变成你java程序内对应的数据结构（类）的对象，fastjson是一个第三方库，使用前需要在你的gradle文件里引入它： 代码块51compile group: 'com.alibaba', name: 'fastjson', version: '1.2.67' //在你gradle文件的dependencies下加入这段代码，gradle就会帮你自动依赖进来它的jar包 然后我们将上面的例子来利用fastjson改编下： 代码块6123456789101112131415161718192021222324252627282930313233public class Test { public static void main(String[] args) throws Exception { //new一个Season对象 Season season = new Season(); season.setSeason_id(1); season.setTitle(\"寒蝉鸣泣之时\"); season.setIndex(\"索引1\"); serializable(season); //把它序列化到一个文件里去 Season season2 = deSerializable(); //从文件读取被序列化好的season对象的字节流，然后反序列化成一个新的对象 System.out.println(\"season_id = \" + season2.getSeason_id() + \" title = \" + season2.getTitle() + \" index = \" + season2.getIndex()); System.out.println(season == season2); //这是俩不同的对象，结果为false } //json序列化，我们利用fastjson将season对象序列化到文件里 public static void serializable(Season season) throws Exception { byte[] seasonByte = JSONObject.toJSONBytes(season); Path path = Paths.get(\"/Users\", \"sunqinwen\", \"Documents\", \"seasonJson.txt\"); if (!Files.exists(path)) { //若文件不存在 Files.createFile(path); //创建文件 } Files.write(path, seasonByte); //将序列化好的字节流存入文件 } //json反序列化，我们利用fastjson将文件里的字节流重新转换成Season对象 public static Season deSerializable() throws Exception { Path path = Paths.get(\"/Users\", \"sunqinwen\", \"Documents\", \"seasonJson.txt\"); byte[] seasonByte = Files.readAllBytes(path); //读取出来所有的字节 return JSONObject.parseObject(seasonByte, Season.class); //反序列化成对应类型的对象 } } 运行结果： 代码块712season_id = 1 title = 寒蝉鸣泣之时 index = nullfalse 让我们来看下seasonJson.txt： 我们终于可以看懂了，这就是json有魅力的地方，它实际的格式就是便于人类阅读的，实际上实现了json序列化的工具也是按照这种格式来解析这些json串的，json就不过多介绍了，实际上在测试当中，你已经见过太多复杂的结构体了。 但是，json串如何和我们的java对象映射起来呢？我们来写一个复杂一点的java类： 代码块8123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class Student { private int id; private String name; private int age; private int[] nos; private List&lt;Season&gt; seasons; private Map&lt;String, Season&gt; season_map; public int getId() { return id; } public void setId(int id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } public int[] getNos() { return nos; } public void setNos(int[] nos) { this.nos = nos; } public List&lt;Season&gt; getSeasons() { return seasons; } public void setSeasons(List&lt;Season&gt; seasons) { this.seasons = seasons; } public Map&lt;String, Season&gt; getSeason_map() { return season_map; } public void setSeason_map(Map&lt;String, Season&gt; season_map) { this.season_map = season_map; }} 然后我们来给它的一个对象赋值后再转成json串： 代码块9123456789101112131415161718192021222324252627282930313233343536public static void main(String[] args) { Student student = new Student(); student.setId(1); student.setName(\"蛤蛤\"); student.setAge(1000000); student.setNos(new int[]{1, 2, 3, 4, 5}); Map&lt;String, Season&gt; seasonMap = new HashMap&lt;&gt;(); Season season1 = new Season(); season1.setSeason_id(1); season1.setTitle(\"寒蝉鸣泣之时 第一季\"); Season season2 = new Season(); season2.setSeason_id(2); season2.setTitle(\"寒蝉鸣泣之时 第二季\"); seasonMap.put(\"第1季\", season1); seasonMap.put(\"第2季\", season2); student.setSeason_map(seasonMap); List&lt;Season&gt; seasons = new ArrayList&lt;&gt;(); Season season3 = new Season(); season3.setSeason_id(3); season3.setTitle(\"火影忍者 疾风传\"); Season season4 = new Season(); season4.setSeason_id(4); season4.setTitle(\"少女终末旅行\"); seasons.add(season3); seasons.add(season4); student.setSeasons(seasons); System.out.println(JSON.toJSONString(student)); //利用fastjson这个JSON类的静态方法toJSONString，可以把一个对象转成json串} 运行结果，我们格式化一下，是下面的字符串： 代码块101234567891011121314151617181920212223{ \"age\": 1000000, \"id\": 1, \"name\": \"蛤蛤\", \"nos\": [1, 2, 3, 4, 5], \"season_map\": { \"第1季\": { \"season_id\": 1, \"title\": \"寒蝉鸣泣之时 第一季\" }, \"第2季\": { \"season_id\": 2, \"title\": \"寒蝉鸣泣之时 第二季\" } }, \"seasons\": [{ \"season_id\": 3, \"title\": \"火影忍者 疾风传\" }, { \"season_id\": 4, \"title\": \"少女终末旅行\" }]} 现在，仔细观察这个json串的每一项，给我们的Student类里的属性一一对比，你就会发现它们之间的映射关系。 三、Tomcat终于到了最令人兴奋的时刻了，之前了解了那么多网络知识，是我们写的业务代码发挥它意义的时刻了~ 3.1：Tomcat简介用维基百科里的解释： Tomcat是由Apache软件基金会属下Jakarta项目开发的Servlet容器，按照Sun Microsystems提供的技术规范，实现了对Servlet和JavaServer Page（JSP）的支持，并提供了作为Web服务器的一些特有功能，如Tomcat管理和控制平台、安全局管理和Tomcat阀等。由于Tomcat本身也内含了HTTP服务器，因此也可以视作单独的Web服务器。 说人话就是：Tomcat是基于java语言实现的一个基于HTTP协议的web服务器程序，你写的业务代码可以放到这个服务程序下面，然后你的业务代码就可以被别人通过http://xxx.xxx.xxx/path的方式调用到，触发到了！而这节课所讲述的HTTP协议编解码、json序列化你完全不需要关心，tomcat已经支持了，你现在要做的是，编写自己的业务代码，把它们放进去，放到tomcat的身体里就好了！很开心吧？作为程序员的幸福感油然而生，正是因为前辈们不断的铺路，我们不用自己开发一个支持http协议的服务器程序，因为前辈们提供了Spring，我们不用自己深入理解IOC和AOP思想，因为前辈们提供了Mybatis，我们不用苦恼通过传统JDBC读出来的数据无法直接转成java对象。这里提到的Spring和Mybatis又是切实参与你业务代码编写的类库集，被我们称为”框架“，有了它们，我们写起来企业级业务代码会非常的开心，这些后面会通过一个程序带大家慢慢了解，正是因为前人不断的努力，我们其实已经很省力了，喜欢折腾的程序员还会重复造轮子，试图撼动这些传统框架的地位（个人观点：喜欢创新是好事，但要找对点，没有技术革新的造轮子唯一的意义就是帮助你更深理解前辈们的思想）。 3.2：Servlet你以为你写的业务代码就那么容易放进Tomcat的身体里吗？不存在的，上面说了，Tomcat是一个Servlet容器，什么意思？就是说你写的代码必须要符合Servlet规范，至少你的程序入口得符合Servlet，同样的，我们摘一段维基百科里对它的描述： Servlet（Server Applet），全称Java Servlet，未有中文译文。是用Java编写的服务器端程序。其主要功能在于交互式地浏览和修改数据，生成动态Web内容。狭义的Servlet是指Java语言实现的一个接口，广义的Servlet是指任何实现了这个Servlet接口的类，一般情况下，人们将Servlet理解为后者。 还记得我们前面讲过的类的继承多态以及接口吗？你可以这样理解，你的业务代码类得是Servlet的实现类，并且实现了它的一些方法后才可以被Tomcat识别并进入，这就是java多态的好处之一，你只要符合某种规范，就可以被别的符合同样规范的程序（本例指tomcat）所识别，你重写了里面的方法，别的程序会触发被你重写的方法，看！你的业务代码就这样被触发了！ 3.3：基于Servlet实现一个业务接口前面说了一大堆废话，太虚了，我们来新建一个gradle项目，编写一个返回一个对象json结构体的例子（其实沃特玛自己都忘了咋写了，回头学会用Spring Boot写一个同样的接口，你会唾弃现在这种干法的）。 3.3.1：下一个tomcat先tomcat下载地址：https://tomcat.apache.org/download-80.cgi 找到属于你自己平台的那个即可，tomcat是不需要特别安装的，直接解压就能用。 它的目录长这样： 圈出来的是最重要的几个包，bin存放的是tomcat的核心功能，conf下放的是tomcat本身的配置，webapps就是存放你业务代码的地方了~当然正常开发时，你是不需要手动将你编译好的业务代码放进去的，IDEA已经帮你做到了，你需要做的是给IDEA指定一个tomcat路径，让IDEA知道你的tomcat程序在哪里，这样你的web程序在启动时，IDEA就可以帮你把它编译好的代码放到webapps下面了。 3.3.2：添加依赖包我们在构建好的gradle项目里的build.gradle的dependencies里引入以下配置，引入servlet的jar包和fastjson的jar包： 代码块1112compileOnly (group: 'javax.servlet', name: 'javax.servlet-api', version: '4.0.1')compile (group: 'com.alibaba', name: 'fastjson', version: '1.2.67') 这样，你就可以使用servlet的包编写你的业务程序，之前讲过，通信时以字节做交互的话需要序列化，因为我们是一个输出信息的接口，所以我们选择使用json方式完成序列化，所以我们还需要引入一个json序列化工具fastjson用来做数据的序列化。 3.3.3：编写业务代码我们这个接口的目标是，将一个复杂的java对象，传输给浏览器，我们仍然使用之前那个复杂的Student做，只不过我们要利用分层思想来做： 还记得我们之前在做一个简单的增删改查时那个例子里的业务分层模式吗？这里也是一样的，不过我们没有做service层，因为不需要，我们只是要将一个对象输送出去，没有任何特别的业务逻辑操作。 我们来看下Dao层的代码： 代码块121234567891011121314151617181920212223242526272829303132333435363738394041public class StudentDao { //之前例子里给Student赋值的main方法里的逻辑，被抽象进了Dao层的一个方法里 public Student getStudentInfo() { Student student = new Student(); student.setId(1); student.setName(\"蛤蛤\"); student.setAge(1000000); student.setNos(new int[]{1, 2, 3, 4, 5}); Map&lt;String, Season&gt; seasonMap = new HashMap&lt;&gt;(); Season season1 = new Season(); season1.setSeason_id(1); season1.setTitle(\"寒蝉鸣泣之时 第一季\"); Season season2 = new Season(); season2.setSeason_id(2); season2.setTitle(\"寒蝉鸣泣之时 第二季\"); seasonMap.put(\"第1季\", season1); seasonMap.put(\"第2季\", season2); student.setSeason_map(seasonMap); List&lt;Season&gt; seasons = new ArrayList&lt;&gt;(); Season season3 = new Season(); season3.setSeason_id(3); season3.setTitle(\"火影忍者 疾风传\"); Season season4 = new Season(); season4.setSeason_id(4); season4.setTitle(\"少女终末旅行\"); seasons.add(season3); seasons.add(season4); student.setSeasons(seasons); return student; } } ok，截止目前，我们的程序已经写好了，但是，tomcat是如何知道这个Servlet类所在的地方呢？因此我们还需要给这个项目加一个叫web.xml的配置文件，它的位置在： 需要注意的是，WEB-INF是没办法省略的，否则tomcat无法读取到这个配置文件，下面我们来看看这个文件里都放了什么东西吧： 代码块141234567891011121314151617&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;web-app xmlns=\"http://xmlns.jcp.org/xml/ns/javaee\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_4_0.xsd\" version=\"4.0\"&gt; &lt;servlet&gt; &lt;!--指定一个Servlet块，并且告诉tomcat该Servlet的路径--&gt; &lt;servlet-name&gt;StudentInfoController&lt;/servlet-name&gt; &lt;servlet-class&gt;com.bilibili.controller.StudentInfoController&lt;/servlet-class&gt; &lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;StudentInfoController&lt;/servlet-name&gt; &lt;url-pattern&gt;/get/student/info&lt;/url-pattern&gt; &lt;!-- http的url映射--&gt; &lt;/servlet-mapping&gt; &lt;/web-app&gt; 这个文件是用来告诉tomcat，我自定义的Servlet类都在哪里，好让tomcat访问调用到，而下面的映射就更有意思了，你的浏览器在输入一个url时，除了域名，就是path了对吧？而这个映射的意思是说，当你在浏览器里输入“www.xx.com/get/student/info”时，这个url传到tomcat，tomcat可以根据path映射关系，找到这个叫StudentInfoController的Servlet，而这个Servlet对应哪个实现类呢？当然就是你自定义的子类StudentInfoController啦~ 至此，你的这个简单的，只提供输出Student对象的接口，就完成了，下面让我们点击下tomcat的运行按钮，就可以启动了，假设我们现在已经启动了，在你的浏览器，输入：127.0.0.1:8080/get/student/info试试吧，它的运行结果如下（我自己改了端口号8081，所以图里是8081 -_-||，其次，localhost = 127.0.0.1，它表示的是你本机的ip地址）： 你可以打开下浏览器的检查功能，可以具体看到HTTP协议头信息： 我们设置的Content-Type它生效了，它通过HTTP协议头来告诉浏览器：“我传给你的业务数据流是以json做的序列化，你也得用json序列化”。 你可能会好奇，这个过程中怎么没见main方法啊？因为这个过程中你的程序只是陪衬，真正运行的程序是包含了你的业务代码的tomcat，所以启动这个动作肯定是发生在了tomcat。 3.3.4：结束语你写的普通的业务代码终于可以堂堂正正的让浏览器通过HTTP请求访问到了！！现在知道什么是java web了吧？其实一开始大家分的很细，像什么JAVA SE、JAVA EE，JAVA EE就是java web，也叫java企业级开发，其实你不用计较这么多，java就是java，因为你借助一个web程序tomcat完成了你让别人通过HTTP请求来触发你的业务程序，所以你写的程序也被叫成了java web，仅此而已，你需要学会从更宏观的角度来看待这些细微的差别，而不是死抠一些专有名词。 最后还是要说一下，你没发现写一个上面的程序，很特么复杂吗？还得下载配置tomcat，还得配置你的IDEA，还得指定各类序列化方式等等，让它可以结合tomcat运转你的程序，事实上，很多事情复杂到一定程度，就一定会有工程师去搞个更方便的东西出来，这也是接下来要让你们接触到的一个东西：spring boot。 别害怕，spring boot并不是单独一门技术，它就是spring，spring的一切特性它都具备，但是它是spring的进化态，它里面包含了一个tomcat！spring boot写好的程序，就自动包含了上面一堆有关tomcat的配置，你再也不用担心tomcat的任何事情，写个main方法就好~","link":"/2020/03/27/LV4-5%EF%BC%9A%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E3%80%81%E5%8D%8F%E8%AE%AE%E3%80%81%E5%BA%8F%E5%88%97%E5%8C%96%E3%80%81%E7%A8%8B%E5%BA%8F%E4%B9%8B%E9%97%B4%E7%9A%84%E8%81%94%E7%B3%BB/"},{"title":"LV4-4：位图","text":"一、用法先来看看java里对位图数据结构的实现类：BitSet 用法如下： 代码块1123456789101112131415public static void main(String[] args) { BitSet bitSet = new BitSet(); bitSet.set(78); //只传一个参数代表将该位置上的数值置为1（true） bitSet.set(79, 82); //79、80、81位为true，即[79, 82) bitSet.set(83, true); //83位置为1（true） System.out.println(bitSet.get(78)); //true System.out.println(bitSet.get(79)); //true System.out.println(bitSet.get(80)); //true System.out.println(bitSet.get(81)); //true System.out.println(bitSet.get(82)); //82位为false System.out.println(bitSet.get(83)); //true bitSet.set(83, false); //再次将第83位上的值置为0（即false） System.out.println(bitSet.get(83)); //第83位上的值为0（false）} 打印结果如下： 代码块21234567truetruetruetruefalsetruefalse 很懵逼吧？位图到底是什么？这些”位“代表什么意思？记录true或false有什么用？继续往下看。 二、位图 其实位图就是个数组，所谓的”位“就是指数组里每个数值元素的二进制位，像不像我们对开关字段的处理方式？标记只有存在或者不存在（对应二进制的0和1），这样就可以做到用一个long型的数字就可以产生出64个标记信息，非常适合数据量庞大而判断状态少的应用场景，比如判断一个词语是否是屏蔽词，首先屏蔽词状态只有两种：命中or不命中，但是屏蔽词可能是个非常庞大的集合，如果一个个拿来比较，效率完全保证不了，那么就可以利用这个数据结构来解决这类问题，可以首先把所有的屏蔽词放到一个位图结构里，如果有相同的词语，只需要简单的两部运算就可以拿到是否命中结果，构建这个位图结构的过程如下（⚠️ 注：下方的hash算法纯属为了方便举例而yy产生）： 通过上图，屏蔽词位图结构就构建好了，如果有个词语需要判定是否命中屏蔽词，只需要让这个词语通过上面的哈希算法计算出哈希值，然后找到对应的数组下标，通过位运算算出其所在位置，将该位置的值取出，如果是0，则认为没有命中，1则认为命中。 抽象到代码块1的例子，其实就是将传入的index，与long型的64位做除法，计算出自己属于数组的哪个下标值，这样就找到了自己在数组中的位置，然后再将下标值跟64取模，计算出将该十进制的值的第几位标记为1或0.","link":"/2020/03/25/LV4-4%EF%BC%9A%E4%BD%8D%E5%9B%BE/"},{"title":"LV4-3：java里的Map","text":"什么是Map结构呢？其实就是键值对啦，跟缓存差不多，存入一个key-value，下次拿着key可以直接获取value的值： Map结构的基本使用123Map map = new XxxMap();map.put(\"b\", \"bilibili\"); //存入一个k-vString value = map.get(\"b\"); //这里就可以拿着b这个key值，直接取到value的值了 一、Map实现类关系图 二、Map提供了哪些主要方法？ 三、几种Map实现类的区别 类名 底层结构 是否线程安全 是否允许null key 说明 HashMap 1.7：数组+链表1.8：数组+链表（进化态：红黑树） 否 是 最基本常用的一个Map的实现，但它并非线程安全，使用时需要注意并发问题。 LinkedHashMap 数组+链表（进化态：红黑树） 否 是 继承了HashMap，基本功能仍然由HashMap掌管，只是它的key是有序的。 TreeMap 红黑树 否 否 它的key是有序的，且它同时实现了SortedMap接口。 HashTable 数组+链表 是 否 线程安全，性能较低，不建议使用的map。 ConcurrentHashMap 1.7：Segment【数组+链表】1.8：cas+数组+链表（进化态：红黑树） 是 否 1.7：线程安全，由于其内部分段锁设计，性能相对HashTable较高。1.8：基于cas保证线程安全，同时结构与1.8的HashMap一致 表1 ⚜️ 我们现在的程序基于java11进行，所以以1.8的优化为准，1.7做下了解即可，对内部数据结构感兴趣的话，可以参照源码或者一些博客进行深入理解，对于Map结构，至少要做到会用，它是java里经常用到的一种结构，主要用法参考图2的API。 四、实例代码块112345678910111213public static void main(String[] args) { Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); Map&lt;String, String&gt; map2 = new LinkedHashMap&lt;&gt;(); Map&lt;String, String&gt; map3 = new TreeMap&lt;&gt;(); Map&lt;String, String&gt; map4 = new Hashtable&lt;&gt;(); Map&lt;String, String&gt; map5 = new ConcurrentHashMap&lt;&gt;(); map.put(null, \"sss\"); map2.put(null, \"sss\"); map3.put(null, \"sss\"); //报错 map4.put(null, \"sss\"); //报错 map5.put(null, \"sss\"); //报错} 如上，证明下面三种Map实现类无法接收null作为key的参数。 代码块21234567891011public static void main(String[] args) { //泛型指定key和value都是String类型 Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); map.put(\"a\", \"AcFun\"); map.put(\"b\", \"哔哩哔哩\"); System.out.println(map.get(\"a\")); System.out.println(map.get(\"b\")); System.out.println(map.containsKey(\"b\")); System.out.println(map.containsKey(\"c\"));} 上面的代码输出如下： 代码块31234AcFun哔哩哔哩truefalse 代码块4123456789101112131415public static void main(String[] args) { Cat cat1 = new Cat(); cat1.setName(\"加菲\"); cat1.setAge(8); Cat cat2 = new Cat(); cat2.setName(\"加菲\"); cat2.setAge(8); Map&lt;Cat, String&gt; catMap = new HashMap&lt;&gt;(); catMap.put(cat1, \"cat1\"); catMap.put(cat2, \"cat2\"); System.out.println(catMap.size());} 我们之前说过，判断两只属性相同的猫相等的办法，就是重写其equals方法，那么代码块4里的size应该为1才对，因为我们说过，map里相同key会相互覆盖，既然重写了Cat类的equals方法，那么cat1和cat2属性相同就应该认为是相同的两个对象，那么最终第二个会覆盖掉第一个的值，最终size为1，不过可惜的是，这个程序结果是2，你可能会迷惑，在前面的集合类里这的确算是相同了，可惜Map的判定更加严格，需要Object的hashCode方法参与，所以我们除了要重写equals方法，还要重写Cat的hashCode方法： 代码块512345678910111213141516171819202122public class Cat extends Animal { @Override public void cry() { System.out.println(\"喵喵喵~\"); } public void catchMice() { //扩展方法：捕鼠 System.out.println(\"我会捉老鼠\"); } @Override public int hashCode() { return this.getAge() + this.getName().hashCode(); } @Override public boolean equals(Object obj) { Cat otherCat = (Cat) obj; //强转下类型（所以这里只能传进来一个Cat型的数据，否则报错） return otherCat.getAge() == this.getAge() &amp;&amp; otherCat.getName() == this.getName(); //如果一只猫的年龄和名字跟另一只猫相等，那么就认为它们相同 }} 现在再运行下代码块4，结果为1，hashCode方法又是什么？hashCode是对象产生后，对其地址的hash计算出来的一个哈希码，哈希码相同的对象不一定是同一个对象，但是哈希码不同的两个对象一定不是同一个对象（因为hash算法会冲突，不然散列表也不会存在解决哈希冲突这种问题了）。","link":"/2020/03/23/LV4-3%EF%BC%9Ajava%E9%87%8C%E7%9A%84Map/"},{"title":"LV4-2：java常用的集合类以及它们之间的区别","text":"集合类用来保存一堆对象或者基本数据，你可能会产生疑问，数组不也是做这个的吗？为什么还需要有集合类呢？数组和集合类的区别在于数组需要预先指定容积，比如声明： 数组声明1int[] a = new int[5]; 这里这个a只能装5个数据，超过5个就开始爆炸，集合类就解决了这个问题，你不需要事先知道元素的总个数，只需要往里面塞数据就好，在实际开发里，很少会预先知道元素的个数，因此集合类作为可以容纳任意个数个元素的工具类，在开发里就显得尤为重要。 一、集合实现类关系图1.1：集合家族的类关系图先来看下集合类的继承和实现关系： 顶上的接口叫做Collection，派生出俩接口，一个叫List，一个叫Set，我们先来了解下它们的方法都有哪些： 1.2：List的几个实现类及它们之间的区别 类名 底层结构 是否线程安全 说明 ArrayList 数组 否 ArrayList基于数组来实现集合的功能，其内部维护了一个可变长的对象数组，集合内所有对象存储于这个数组中，并实现该数组长度的动态伸缩 LinkedList 双向链表 否 LinkedList基于链表来实现集合的功能，其实现了静态类Node，集合中的每个对象都由一个Node保存，每个Node都拥有到自己的前一个和后一个Node的引用 Vector 数组 是 线程安全版的ArrayList，基于数组实现的集合，它可自定义扩容因子（但它的方法基本都是同步的，性能较低） CopyOnWriteArrayList 数组 是 也是线程安全版的ArrayList，它写加锁读不加锁，写的时候复制当前集合生成一个副本，然后给副本添加元素后修改引用变量指向，因此它较占内存，但是整体性能要比Vector高 表1 1.2.1：常用用法合集看了类继承结构，可以知道，表1里那么多类，都只是List的实现类，所以我们这里仅通过ArrayList来看下List的的用法： 代码块11234567891011121314151617181920212223242526272829303132333435363738public static void main(String[] args) { //初始化一个list对象，集合类是泛型的哦 List&lt;String&gt; names = new ArrayList&lt;&gt;(); //这句代码就初始化了一个ArrayList的对象，指定泛型只能存储String类型的数据 names.add(\"sun1\"); names.add(\"sun2\"); //通过add往里面加数据，只要你愿意，可以往里面加n多数据 names.add(\"sun3\"); List&lt;String&gt; names2 = new ArrayList&lt;&gt;(); //这里再来搞个新的List，并给它加俩值 names2.add(\"sun4\"); names2.add(\"sun5\"); names2.add(\"sun5\"); names2.add(\"sun5\"); //List是允许元素重复的 names.addAll(names2); //通过addAll将上面的List加到第一个List对象里 //删除某个元素 names.remove(\"sun2\"); names.remove(\"sun5\"); //删除重复了三次的元素中的其中一个 //for增强遍历 for (String name : names) { System.out.println(name); } System.out.println(\"-------------------------\"); //java8的lambda遍历 names.forEach(name -&gt; { System.out.println(name); }); System.out.println(\"-------------------------\"); System.out.println(names.contains(\"sun2\")); //利用contains来判断list里是否包含某元素 System.out.println(names.contains(\"sun5\")); System.out.println(\"-------------------------\"); List&lt;String&gt; subNames = names.subList(1, 3); //从下标1截取到下标3的元素，区间为：[1, 3) subNames.forEach(name -&gt; { System.out.println(name); });} 上面例子中涵盖了List系集合实现类的常用方法。 ⚠️ 注意：List是有序集合！ 1.2.2：contains和remove，equals我们上面讲了remove和contains的用法，这俩方法一看就是要遍历查找对应元素，那么既然是这种逻辑，一定会有判等逻辑：直接用==运算符或者通过对象的equals方法来进行判等。 先来看看==和equals，==我们在之前讲过，对于基本类型，它用来简单判断值是否相同，但对于引用变量，它则用来判断地址： 代码块212345678910111213141516171819202122232425public static void main(String[] args) { Cat cat1 = new Cat(); cat1.setName(\"加菲\"); cat1.setAge(8); Cat cat2 = new Cat(); cat2.setName(\"加菲\"); cat2.setAge(8); List&lt;Cat&gt; cats = new ArrayList&lt;&gt;(); cats.add(cat1); cats.add(cat2); Cat cat3 = new Cat(); cat3.setName(\"加菲\"); cat3.setAge(8); System.out.println(cats.size()); System.out.println(cats.contains(cat3)); cats.remove(cat3); System.out.println(cats.size());} 这个结果为： 代码块31232false2 大致意思你应该已经明白了，之前也说过，引用变量由于指向的是一块内存，内存里存放的就是类模板产生的类对象，一个对象里即便属性值完全一致，它也是俩对象。 那么如何让俩名字叫加菲，都是8岁的猫变成同一只呢？还记得Object这个大父类吗？一般情况下，判断俩对象是否相等，都是通过该方法进行的，默认该方法判断的是俩对象的地址，那么我们是否可以通过重写这个equals方法的方式，来重新定义俩对象是否相等这个概念呢？我们来重写下Cat类的equals方法： 代码块41234567891011121314151617public class Cat extends Animal { @Override public void cry() { System.out.println(\"喵喵喵~\"); } public void catchMice() { //扩展方法：捕鼠 System.out.println(\"我会捉老鼠\"); } @Override public boolean equals(Object obj) { Cat otherCat = (Cat) obj; //强转下类型（所以这里只能传进来一个Cat型的数据，否则报错） return otherCat.getAge() == this.getAge() &amp;&amp; otherCat.getName() == this.getName(); //如果一只猫的年龄和名字跟另一只猫相等，那么就认为它们相同 }} 重写后，再运行下代码块2，结果为： 代码块51232true1 可以看到，即便传入的对象是cat3，但由于重写了equals方法，俩对象是否相等不再是根据对象地址来判断了，而是根据内部的变量值是否一致，因为cat3跟集合对象里存放的其他俩对象一样，因此传入cat3，contains会返回true，按照cat3来进行remove也会成功remove掉集合里的一只加菲猫。 1.3：Set的几个实现类及它们之间的区别 类名 底层结构 是否线程安全 说明 CopyOnWriteArraySet CopyOnWriteArrayList 是 可以简单理解为：线程安全的有序Set。 HashSet HashMap 否 内部维护了一个HashMap结构，可先不做对该结构的了解，相比List，它不允许有重复元素（所谓重复与否，也是根据equals来的），且它是无顺序的。 LinkedHashSet HashMap 否 内部继承HashSet，但它是有序的，作为Set，它仍然不允许元素重复。性能较HashSet差。 TreeSet TreeMap 否 内部维护了一个TreeMap结构，可先不做对该结构的了解，它是有序的，作为Set，它仍然不允许元素重复。性能较HashSet差。 表2 例子： 代码块61234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public static void main(String[] args) { Set&lt;String&gt; names = new HashSet&lt;&gt;(); Set&lt;String&gt; names2 = new LinkedHashSet&lt;&gt;(); Set&lt;String&gt; names3 = new TreeSet&lt;&gt;(); Set&lt;String&gt; names4 = new CopyOnWriteArraySet&lt;&gt;(); names.add(\"sun1\"); names.add(\"sun2\"); names.add(\"sun3\"); names.add(\"sun2\"); names2.add(\"sun1\"); names2.add(\"sun2\"); names2.add(\"sun3\"); names2.add(\"sun2\"); names3.add(\"sun1\"); names3.add(\"sun2\"); names3.add(\"sun3\"); names3.add(\"sun2\"); names4.add(\"sun1\"); names4.add(\"sun2\"); names4.add(\"sun3\"); names4.add(\"sun2\"); for (String name : names){ System.out.println(name); } System.out.println(\"------------\"); for (String name : names2){ System.out.println(name); } System.out.println(\"------------\"); for (String name : names3){ System.out.println(name); } System.out.println(\"------------\"); for (String name : names4){ System.out.println(name); } System.out.println(\"------------\");} 输出结果如下： 代码块712345678910111213141516sun2sun3sun1------------sun1sun2sun3------------sun1sun2sun3------------sun1sun2sun3------------ 可以看到，Set的特点是不能有重复的两个元素，否则只保留一个，其次相比LinkedHashSet、TreeSet、CopyOnWriteArraySet的实现，HashSet不具备保存元素顺序的特性。 二、栈2.1：栈是怎样的一种结构？栈简单来说是一种先进后出的结构： 后被加进来的元素在取的时候优先被取出去。 2.2：java的栈结构如图1，栈的实现是基于Vector进行的，因此它是线程安全的，除此之外，我们看看Stack类拓展了哪些方法： push就是图3里往栈结构的栈顶加元素，pop就是用来弹出位于栈顶的元素： 代码块812345678910111213141516public static void main(String[] args) { List&lt;String&gt; stack = new Stack&lt;&gt;(); stack.add(\"sun1\"); stack.add(\"sun2\"); stack.add(\"sun3\"); for (String name : stack) { System.out.println(name); } Stack&lt;String&gt; names = (Stack)stack; System.out.println(\"------------\"); names.push(\"sun4\"); //push/add都可以用来给栈添加数据，新添加的数据总是位于栈顶 System.out.println(names.pop()); //弹出栈顶数据 System.out.println(names.size()); //输出当前数据量} 上述代码输出结果： 代码块9123456sun1sun2sun3------------sun43 可以看到，作为List的一个实现类，它仍然可以被List接收，用法跟普通集合一致，但是利用它作为Stack类拓展的方法时，它的栈属性得以体现出来。 这里说下peek，peek方法仅用来返回栈顶数据，并不具备pop那种“弹出”的效果。","link":"/2020/03/23/LV4-2%EF%BC%9Ajava%E5%B8%B8%E7%94%A8%E7%9A%84%E9%9B%86%E5%90%88%E7%B1%BB%E4%BB%A5%E5%8F%8A%E5%AE%83%E4%BB%AC%E4%B9%8B%E9%97%B4%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"title":"LV4-1：io的介绍&java的io api","text":"一、I/O家族下图为java自带的IO操作的整体家族树状图，整体分为字节流和字符流两大类，它们的区别在于： 字节流是以字节为单位来传输数据，它按照输入/输出方向不同分为了两个大父类：InputStream和OutputStream 字符流是以多个字节为单位来传输数据，它按照输入/输出方向不同分为了两个大父类：Reader和Writer ⚠️ 注：本文仅讲字节流常用的几种读写数据的方式以及好用的nio api，其余想要详细了解可以网上查阅具体的资料。 二、如何利用字节流读写数据？2.1：写我们现在写一个程序，让它新建一个文件，命名为a.txt，如果文件已经存在，则不再创建，然后通过程序往它的里面写入一句话：”Java是世界上坠吼滴语言！“： 代码块11234567891011121314151617181920public static void main(String[] args) throws IOException { File aFile; //声明文件类型变量 OutputStream os = null; //声明输出流类型变量 try { aFile = new File(\"/Users/sunqinwen/Documents/a.txt\"); if (!aFile.exists()) { //若文件不存在 if (!aFile.createNewFile()) { //利用createNewFile创建文件 System.out.println(\"创建文件失败！\"); return; } } os = new FileOutputStream(aFile); //和目标文件建立起数据流管道 os.write(\"Java是世界上坠吼滴语言！\".getBytes(StandardCharsets.UTF_8)); //使用UTF-8的编码方式，写入到a文件 System.out.println(\"执行完毕！\"); } finally { if (os != null) { os.close(); //管道用好了别忘了关闭 } }} 2.2：读上方运行结果就是a文件被创建了出来，并且顺利将那段文本写了进去，现在我们再来写一个程序，把这个文件里的信息读入到程序中，这时就要借助我们的输入流InputStream了： 代码块21234567891011121314151617181920212223242526public static void main(String[] args) throws IOException { File aFile; //声明文件类型变量 InputStream is = null; //声明输出流类型变量 try { aFile = new File(\"/Users/sunqinwen/Documents/a.txt\"); if (!aFile.exists()) { //若文件不存在 System.out.println(\"文件不存在！\"); return; } is = new FileInputStream(aFile); //和目标文件建立起数据流管道 byte[] msg = new byte[1024]; //以1KB为单位，将文件里的数据读进来 StringBuilder sb = new StringBuilder(); int n; while ((n = is.read(msg)) &gt; 0) { //当read返回-1，说明文件里的内容已被读完，那么循环终止，否则返回的就是本次读进来的字节数 //一旦进入这里，msg里的字节信息就已经被更新了，而且是按照1KB的大小读入的 sb.append(new String(msg, 0, n, StandardCharsets.UTF_8)); //每次把读进来的字节累加到结果字符串里 } System.out.println(sb); //直接把结果输出 } catch (IOException e) { //IO操作需要捕获处理IO异常 e.printStackTrace(); } finally { if (is != null) { is.close(); //管道用好了别忘了关闭 } }} 通过上面的方式，利用FileInputStream，可以将文件里的信息读出来。 对于文件读写、创建，理论上掌握一种方式就可以，但是你会发现上面的写法真的很啰嗦，java有没有更好的读写文件的方法呢？来看下一节。 三、利用JAVA NIO API创建&amp;读写文件不知道你发现了没，Mac系统的文件路径是正斜杠划分，而Windows系统却是反斜杠，这就意味着你上面的代码对于文件路径这一块在双系统下是不兼容的，而且上面的代码很繁琐，于是Java提供了更好用的NIO API，让我们来改造下上面的代码。 3.1：写将”Java是世界上坠吼滴语言！“写入同路径的b.txt文件： 代码块312345678public static void main(String[] args) throws IOException { Path path = Paths.get(\"/Users\", \"sunqinwen\", \"Documents\", \"b.txt\"); if (!Files.exists(path)) { //若文件不存在 Files.createFile(path); //创建文件 } Files.write(path, \"Java是世界上坠吼滴语言！\".getBytes(StandardCharsets.UTF_8)); System.out.println(\"执行完毕！\");} 这里用Path来组装路径，利用Path组装后的路径会智能转成符合当前操作系统的路径模式。然后利用Files完成文件的创建和写入。 3.2：读现在将这句话从文件里读出来，相比2.2，真的简化了好多： 代码块41234public static void main(String[] args) throws IOException { Path path = Paths.get(\"/Users\", \"sunqinwen\", \"Documents\", \"b.txt\"); System.out.println(Files.readString(path, StandardCharsets.UTF_8));} 同样利用Path组装路径，然后利用Files读取数据。 Files读取数据时，我们可以详细探索下，它是很有用的一个工具类：","link":"/2020/03/20/LV4-1%EF%BC%9Aio%E7%9A%84%E4%BB%8B%E7%BB%8D&java%E7%9A%84io%20api/"},{"title":"LV3-4：反射[转载]","text":"Java 反射机制在程序运行时，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性。这种 动态的获取信息 以及 动态调用对象的方法 的功能称为 java 的反射机制。 方法大全：https://zhuanlan.zhihu.com/p/27331449 用法请参考：https://juejin.im/post/598ea9116fb9a03c335a99a4 如何获取到方法上的注解信息？ 代码块112Method method = teacherClass.getDeclaredMethod(\"method\"); //拿到Class具体的名叫“method”的Method对象NeedPrint needPrint = method.getAnnotation(NeedPrint.class); //尝试获取该方法上的NeedPrint注解","link":"/2020/03/17/LV3-4%EF%BC%9A%E5%8F%8D%E5%B0%84/"},{"title":"LV3-3：Object&泛型","text":"泛型，即“参数化类型”。一提到参数，最熟悉的就是定义方法时有形参（方法定义里括号里的参数声明），然后调用此方法时传递实参（实际传给方法的参数）。那么参数化类型怎么理解呢？顾名思义，就是将类型参数化，类似于方法中的变量参数，此时类型也定义成参数形式（可以称之为类型形参），然后在使用/调用时传入具体的类型（类型实参）。泛型的本质是为了参数化类型（在不创建新的类型的情况下，通过泛型指定的不同类型来控制形参具体限制的类型）。也就是说在泛型使用过程中，操作的数据类型被指定为一个参数，这种参数类型可以用在类、接口和方法中，分别被称为泛型类、泛型接口、泛型方法。 一、一个例子1.1：使用Object实现假设现在让我们开发一个用于在应用中传递对象的容器。但对象类型并不总是相同。因此，需要开发一个能够存储各种类型对象的容器。下面来试着开发一下： 前面的章节我们提到过，Object是所有类的父类，那么意味着可以用Object类型的参数变量来接收任何类型的对象： 代码块1123456789101112public class ObjectContainer { private Object obj; public void setObj(Object obj) { //只要你喜欢，你可以放任何类的对象进来 this.obj = obj; } public Object getObj(){ //通过此方法获取到当前容器内的对象 return obj; }} 我们来测试下看看： 代码块21234567public static void main(String[] args){ ObjectContainer container = new ObjectContainer(); //先把容器对象创建出来 container.setObj(new Cat()); //存 Cat cat = (Cat) container.getObj(); //取，之前讲过，小类型接大类型需要强转 } 这段代码是没有任何问题的，现在让我们改下代码： 代码块31234567public static void main(String[] args){ ObjectContainer container = new ObjectContainer(); //先把容器对象创建出来 container.setObj(new Cat()); //存 Dog dog = (Dog) container.getObj(); //取，这里强转成Dog型 } 这段代码在编译阶段不会有任何报错，但是在运行时，就会报类型转换错误。事实上，我们设计的这个对象容器就面临着这种问题，它确实可以接收并储藏任何类型的对象，但是对于使用方来说，他并不知道你这里面存的到底是什么类型，只知道它是个Object，使用方在强转时也不会有任何提示说存在类型转换错误的风险，这样稀里糊涂上了线，就会在运行期加大报错的风险。 1.2：利用泛型改造我们现在利用泛型改造下代码： 代码块4123456789101112public class ObjectContainer&lt;T&gt; { //尖括号内的字母就是类型声明 private T obj; //类里已经声明了的类型，在这里可以直接替代Object public void setObj(T obj) { //现在只能接收被声明的类型 this.obj = obj; } public T getObj() { //通过此方法获取到当前容器内的对象，返回的变成了被声明的类型 return obj; }} 上面的泛型声明放到了类上面，这个类被称为泛型类，现在来测试代码就可以在实例化容器对象的同时声明容器内存储的类型了： 可以看到，泛型类在实例化的时候，可以指定声明的具体类型，被指定后，意味着该容器存放的对象类型必须是Cat或者其子类型的对象，因为有了这个特性，下面getObj方法返回的是Cat类型现在也是已知的，因此强转成Dog会在编译期就报错，IDE的语法检查也过不了。 泛型的好处就在于这里，它可以触发更加严谨的语法检查，让程序员编写的类似容器的代码在允许接收任何类型对象的同时不容易发生类型转换错误。 二、泛型的分类2.1：泛型类即声明在类上的泛型，上面的例子就是，我们在类名后面加上泛型声明，那么这个类就是一个泛型类，既然泛型是一种类似方法里形参的一种声明性的东西，那么它就应该支持声明多个： 代码块51234567891011121314151617181920212223242526272829303132public class ObjectContainer&lt;X, Y, Z&gt; { //该泛型类声明了三个类型（这个类似方法的形参，理论上可以无限声明） private X x; //X类型的对象 private Y y; //Y类型的对象 private Z z; //Z类型的对象 public void setX(X x) { //现在该方法只能接收X的类型的对象 this.x = x; } public X getX() { //通过此方法只能获取X类型的对象 return x; } public void setY(Y y) { //现在该方法只能接收Y的类型的对象 this.y = y; } public Y getY() { //通过此方法只能获取Y类型的对象 return y; } public void setZ(Z z) { //现在该方法只能接收Z的类型的对象 this.z = z; } public Z getZ() { //通过此方法只能获取Z类型的对象 return z; }} 这是一个声明了三种类型的泛型类。 你也可以控制泛型的取值范围，比如你希望所有的X类型必须是Animal的子类，你就可以把声明改成这样： 代码块61public class ObjectContainer&lt;X extends Animal, Y, Z&gt; //这样你传入X对应的类型就必须是Animal或者它的子类们（值得注意的是，不可以通过implements关键词限定接口） 2.2：泛型方法泛型除了可以声明在类的开头，也可以用来放到方法的声明当中去： 代码块71234public &lt;K, V&gt; V getValue(K k) { //声明该方法有俩泛型变量，K和V V value = (V) mc.getValue(k); return value;} 此外，需要注意，方法上声明的泛型，即便命名和类里声明的一致，它们所代表的的含义也完全不一样： 跟普通的声明一样，声明位置不同，作用域也不同。方法里声明的泛型跟类里一样，也可以通过extends关键词来约束接收对象的范围。 让我们来加深一下方法泛型的理解，重新定义一下上面的类： 代码块81234567891011121314151617public class ObjectContainer2&lt;K, V&gt; { //这里声明的K，V是类全局的，跟方法里的不一样 private K k; private V v; private Mc mc = new Mc(); //辅助类，先别关注 public &lt;K, V&gt; V getValue(K k) { //这里声明的KV是方法自己的 return getValue2(k); } public &lt;K, V&gt; V getValue2(K k) { //这里声明的KV是方法自己的 V value = (V) mc.getValue(k); return value; } } 然后写个测试类： 代码块912345678public static void main(String[] args) { ObjectContainer2&lt;Cat, Dog&gt; catDogObjectContainer2 = new ObjectContainer2&lt;&gt;(); String key = \"myKey\"; Cat cat = catDogObjectContainer2.getValue(key); } 这个过程会发生什么？来看下分析图： 2.3：泛型接口即在接口里加上泛型声明，比如我们现在定义一个泛型接口，如下： 代码块10123456789public interface DataOp&lt;ID, DATA&gt; { //声明一个泛型类型，ID表示的是数据ID的类型，DATA表示的是数据类型 boolean save(DATA data); //存储数据 boolean del(ID id); //根据id删除数据 DATA selectById(ID id); //根据id查询数据 } 然后我们就可以利用不同的实现类来定义泛型具体的类型： 代码块111234567891011121314151617public class CatDataOp implements DataOp&lt;Integer, Cat&gt; { //指定这个实现类是针对id为Integer类型且数据为Cat类型的数据操作实现 @Override public boolean save(Cat cat) { // 实现方法这里必须是指定类型 //业务代码 return false; //处理结果 } @Override public boolean del(Integer integer) { return false; } @Override public Cat selectById(Integer integer) { return null; }} 当然你也可以选择不指定泛型，继续让其实现类也是个泛型类： 代码块1212345678910111213141516public class DataOpImpl&lt;ID, DATA&gt; implements DataOp&lt;ID, DATA&gt; { @Override public boolean save(DATA data) { return false; } @Override public boolean del(ID id) { return false; } @Override public DATA selectById(ID id) { return null; }} 然后写一个子类继承该类时再指定具体类型也可以： 代码块131234567public class DogDataOp extends DataOpImpl&lt;Integer, Dog&gt; { @Override public boolean save(Dog dog) { //重写父类方法时，由于该子类指定了具体类型，所以这里参数就变成具体的类型了 return super.save(dog); }} 2.4：通配符泛型跟前面的有关系，但不太大，我们来通过一个例子来说下什么是通配符泛型。 首先定义一个”窝“类，每个窝盛放一个动物： 代码块141234567891011121314//窝，可以容纳一种动物，利用泛型规范自己所接收的对象public class Den&lt;T extends Animal&gt; { private T animal; public void setAnimal(T animal) { this.animal = animal; } public T getAnimal() { return animal; } } 然后我们定义一个动物园类，动物园可以容纳多个窝： 代码块151234567891011121314151617181920public class Zoo { private Den[] dens = new Den[10]; private int i = 0; public void setDen(Den&lt;Animal&gt; animal) { //接收Den对象，泛型就用动物们的父类Animal赋值好了~ this.dens[i++] = animal; } public void forEachAllDensAnimalCry() { //打印出来动物园内每个窝里动物的叫声 for (Den den : dens) { if (den != null) { Animal animal = den.getAnimal(); animal.cry(); } } } } 我们设想的是，新建好的存放着某种动物的窝，通过setDen建到动物园里，我们期待着setDen可以接收Den&lt;Cat&gt;和Den&lt;Dog&gt;等泛型类对象，但是： 可以看到，Den&lt;Cat&gt;和Den&lt;Dog&gt;无法被Den&lt;Animal&gt;所接收，这并不是java的多态出了问题，而是这种泛型类里的泛型类型，根本就没有任何类的特性，我们前面说过，它们仅仅是类似于方法声明里的形参一样的参数，在编译器眼里，Den&lt;Cat&gt;、Den&lt;Dog&gt;、Den&lt;Animal&gt;是三个不同的类定义，它们三个一个里面放的是Cat对象，一个是Dog对象，一个是Animal对象，这又怎么可以称之为一种类定义呢？ 我们再来改造下setDen方法： 代码块16123public void setDen(Den&lt;?&gt; animal) { this.dens[i++] = animal;} 泛型类作为参数接收时，定义&lt;?&gt;通配符可以表示，你可以传入任意Den泛型对象，这个时候再次按照图4里的传参方式，Den&lt;Cat&gt;和Den&lt;Dog&gt;就都可以被接收了。 但是，我们说了，&lt;?&gt;意味着你可以传任意符合Den泛型的对象，如果我想要搞一个只接收猫族特权的方法，该怎么定义呢？如下： 代码块17123public void setCatDen(Den&lt;? extends Cat&gt; cat) { //通配符仍然可以控制接收泛型类参数的范围，但相比Den原类，范围只能缩小 this.dens[i++] = cat;} 看到了吗？就算是&lt;?&gt;通配符，你仍然可以定制它的接收范围，当然，这个范围肯定不可以比Den本身大，例如例子里的Den&lt;? extends Cat&gt;，它确实只能接收Cat及其子类的猫窝，但是它也符合Den定义的Den&lt;T extends Animal&gt;这个规则。","link":"/2020/03/17/LV3-3%EF%BC%9AObject&%E6%B3%9B%E5%9E%8B/"},{"title":"LV3-2：内部类","text":"你知道吗？java里允许一个类的声明放在另外一个类的域里面（再次套娃），如果一个类被声明在了另外一个类里面，那么它就是一个内部类，内部类按照自己的修饰符和出现的区域不同，又分成了以下种类： 如图，我们把包含内部类的普通类叫做外部类。内部类又按照静态非静态做了区分，非静态内部类又细分了三类。下面，让我们来走进内部类。 ⚠️ 注意：本节内容默认你已经对类有很深入的理解，因此本节的例子类命名不再像前几节那么“讲究”，为了方便说名问题，我们把内部类以InnerA/B/C的形式命名，外部类以OuterA/B/C的方式命名。 一、静态内部类1.1：静态内部类的定义&amp;特性静态内部类是内部类的一个大分类，它和非静态内部类有着很大的差别。静态内部类的定义方式如下： 代码块112345678public class OuterA { //在OtherA这个类的域内，定义另外一个使用static修饰的类，这个类就被称作OtherA的静态内部类 public static class InnerA { } } 静态内部类有哪些特性呢？ 事实上静态内部类没有任何与其他类不同的特性，你只需要把它当成平时的类即可，只不过它被定义在另外一个类的域里而已，它的实例化方式需要这样做： 代码块21234public static void main(String[] args) { OuterA outerA = new OuterA(); //外部类的实例化跟普通类没有任何区别 OuterA.InnerA innerA = new OuterA.InnerA(); //内部类的实例化方式} 此外，普通类的定义里，只能把类定义成public类或者干脆不写权限修饰符，且public修饰的类被称为公开类，要求每个java文件里最多只能存在一个公开类，且类名必须要和java文件名一致，但是在静态内部类的世界，是允许加任意权限修饰符的，且限制跟之前讲的那张表中一致： 代码块312345678public class OuterA { //静态内部类的访问权限修饰符可以是public、protected、default、private里的任意一个 private static class InnerA { } } 但是改成protected或default后，可以引用并访问。还有一点，外部类是可以无视静态内部类里资源的访问权限修饰符的，也就是说，外部类甚至可以访问静态内部类的private级别的内容。 静态内部类没什么好说的，你可以理解只是一个类被定义在了另外一个类里头，此外它跟普通类的用法没有任何区别。 1.2：静态内部类的意义如果你认真看了1.1，你会发现，既然这东西跟普通类没什么区别，为什么它还要存在呢？是的，确实定义一个普通类避免了静态内部类那么奇怪的实例化语法，更加利于理解，静态内部类存在就只是java支持这种写法而已，一般情况下，当类B仅会被类A使用，不存在别的地方使用它的情况，这时就建议将类B定义为类A的静态内部类使用，这时访问权限建议设置为private ←这是较官方的说法，其实java支持这种规则，在实际开发中往往有更多种多样的写法，比如某些情况下封装成一个负责收集外部类各种属性的内部类： 代码块41234567891011121314151617181920212223242526272829303132333435363738394041public class Student { private int id; private int age; private String name; private Student(int id, int age, String name) { this.id = id; this.age = age; this.name = name; } public static Builder newBuilder() { return new Builder(); //定义一个可以返回内部类对象的外部静态方法 } public static class Builder { private int id; private int age; private String name; public Builder id(int id) { this.id = id; return this; } public Builder age(int age) { this.age = age; return this; } public Builder name(String name) { this.name = name; return this; } public Student build() { return new Student(id, age, name); //静态内部类可以调用外部类的private方法 } }} 上面的Builder内部类负责给它的外部类收集属性值，最终通过build构建一个外部类实例，然后Student的构造器被我们搞成private了，因此Student不可以通过构造器的方式进行实例化了，但是却可以通过Builder这个内部类实例化： 代码块5123public static void main(String[] args) { Student student = Student.newBuilder().id(1).age(12).name(\"sssss\").build(); //←链式调用赋值} 如果一个对象属性过多，相比那种定义一堆的set、get方法的方式给属性赋值，不如使用这种链式调用的方式来的简洁。 注意，举这个例子的目的不是让你一定要用静态内部类搞这种事情，而是要告诉你，java支持了这种定义方式，你可以利用这种定义方式，来做你自己认为正确的类拆分。 二、非静态内部类2.1：成员内部类先来看看成员内部类怎么定义： 代码块612345678public class OuterB { //在OtherB这个类的域内，定义另外一个非静态的类，这个类就被称作OtherA的成员内部类 public class InnerB { } } 成员内部类跟静态内部类就完全不一样了，来看看它是怎么完成实例化的： 代码块71234public static void main(String[] args) { OuterB outerB = new OuterB(); //同样的，外部类的实例化跟普通类没有任何区别 OuterB.InnerB innerB = outerB.new InnerB(); //成员内部类必须依靠外部类的引用变量完成初始化} ok，现在来介绍下成员内部类的特性： 成员内部类不允许拥有非final的静态属性，不允许拥有任意形式的静态方法。 成员内部类必须通过外部类的引用变量通过new关键词去实例化（必须是外部类本类的引用变量才行，外部类的父类引用变量实例化内部类是不允许的） 成员内部类可以访问任意外部类的资源（无视访问权限修饰符的访问） 成员内部类的定义跟静态内部类一样拥有权限修饰符，我们一般建议设置成private的，由于非静态内部类的特性，它可以被外部类new出来之后操作外部类里面任意资源，因此它常被用来隔离一些外部类的复杂操作，例如一个外部类过于庞大，功能点很多，你就可以按照不同的功能点设置不同的成员内部类进行逻辑归类。 2.2：局部内部类局部内部类就是定义在一个方法或者一个作用域里面的类，定义如下： 代码块8123456789101112131415161718192021222324252627282930313233public class OuterC { static { //静态块里定义的局部内部类 class InnerC1 { } InnerC1 innerC1 = new InnerC1(); //只能在静态块内使用 } public void method() { final int a = 1; final int b = 14; class InnerC2 { //方法体里定义的局部内部类 private int a = 2; //自己内部也可以定义与同域内命名相同的变量 public void m() { a = 12; System.out.println(a); //默认使用的a是它自己的 System.out.println(this.a); //跟上面一致，上面只是隐式的this.a，我们发现，如果自己的成员变量跟外面同域内声明的变量命名一致，则再也无法指定外部域里那个变量了 System.out.println(b); //局部内部类可以使用其外面的同域内的数据，但是要求数据必须声明成final修饰的常量 } } InnerC2 innerC2 = new InnerC2(); //上面是局部内部类的声明，完成后在下面即可实例化后使用 innerC2.m(); } public static void method2() { class InnerC3 { //静态方法体里定义的局部内部类 } } } 来总结下局部内部类的特性： 无法访问任何外部类的非静态的资源 不允许定义访问权限修饰符 作为定义在某个作用域内的局部内部类，它可以访问同域的变量，但是同域变量必须加上final关键词 允许在任意局部域内定义，定义后即可在下方直接使用 目前针对局部内部类做下了解即可，真正开发中极少用到。 2.3：匿名内部类匿名内部类即无名称的内部类，我们在前面学过抽象类和接口，它们是无法被实例化的，但是我们现在这样做： step1：定义一个接口，让它拥有两个方法，a和b 代码块91234public interface A { void a(); void b();} step2：定义一个抽象类，让它拥有抽象方法c 代码块10123public abstract class B { abstract void c();} step3：定义一个普通的类，让其拥有一个接受A和B类型参数的方法，然后触发abc方法 代码块111234567public class C { public void testInner(A a, B b) { a.a(); a.b(); b.c(); }} ok，我们前面讲过，接口和抽象类都是无法被直接实例化的，所以我们在调用testInner这个方法的时候一定是传的A的实现类和B的子类的对象，但我现在告诉你，我没有实现A的类，也没有B的子类，该传什么呢？好的，我们开始引入匿名内部类： 代码块1212345678910111213141516171819public static void main(String[] args){ C c = new C(); c.testInner(new A() { //这里new了A，下方是对其的实现代码，这个代码块就是一个匿名内部类，这个类实现了A接口，属于A接口的实现类，这段代码作为匿名内部类被new了出来，赋给了testInner的第一个参数 @Override public void a() { System.out.println(\"实现了A接口的匿名内部类a方法逻辑实现\"); } @Override public void b() { System.out.println(\"实现了A接口的匿名内部类b方法逻辑实现\"); } }, new B() { //同样的，这里new了B，下方是对其抽象方法的重写，这个代码块也是一个匿名内部类，这个类继承了B，属于B的子类，这段代码作为匿名内部类被new了出来，赋给了testInner的第二个参数 @Override void c() { System.out.println(\"实现了B抽象类的匿名内部类c方法逻辑实现\"); } });} 输出如下： 代码块13123实现了A接口的匿名内部类a方法逻辑实现实现了A接口的匿名内部类b方法逻辑实现实现了B抽象类的匿名内部类c方法逻辑实现 可以看到，匿名内部类里的实现逻辑已被成功触发。 这是在干什么呢？为什么好好的实现类和子类你不写，非要用匿名内部类写成这样？其实这是一种偷懒的写法，有些类你不想专门给它写实现和子类的话，别的地方又需要用到它们的对象，那就可以利用匿名内部类来“偷懒”","link":"/2020/03/12/LV3-2%EF%BC%9A%E5%86%85%E9%83%A8%E7%B1%BB/"},{"title":"LV3-1：作用在类、变量、方法上的其它修饰符","text":"一、static关键词 简单解释：static关键词放到访问权限修饰词的后面，用来修饰类的属性、方法，被static修饰了的属性和方法被称为静态属性和静态方法，静态属性和静态方法可以在不实例化对象的情况下直接通过“类名.属性/方法”的方式触发。 现在开始我们的详细介绍： 我们在之前学类的时候会发现，所有的方法和属性必须要在类做实例化产生自己的对象之后，通过对象的引用变量来触发调用，简单来说，非static的内容，都是属于类对象的，没有产生具体对象的类里的这些非static内容没有任何意义，那static呢？被static修饰的内容属于类本身的内容，静态方法和静态属性是可以直接用类调用的，你可以理解静态属性和方法就是类定义本身的属性和方法，不依赖产生的对象而让自己变得有意义，只要类存在，且被加载进jvm，它就是有意义的，它就是可以通过类本身去触发使用的，让我们来举个例子： 让我们把前面Animal的类改造下，我们知道，Animal是为了描述动物的一个超大父类，里面定义了一些作为动物应该有的类属性，比如name、age，这些属性你会发现一个特点，那就是它们必须要依赖具体某一个动物个体才有意义，换句话说，每个动物的age、name各不相同，这个具体的个体就是我们利用Animal产生的对象，name、age离开了对象个体没有意义，现在我们再来发散下思维，如果需要有一个字段来描述Animal这个类，比如Animal是动物类，假如我们未来还会有植物类，我们现在需要一个字段用来区分它们，这时我们新增一个字段叫“生物分类”的字段，Animal里这个字段的值恒等于“动物”，这个字段我们发现是专门用来形容类的，你让这个字段变成一个普通的类属性，那它每次new一个Animal对象，这个属性就跟着存到对应的对象一次，是不是觉得很浪费资源？这时就可以把这个属性定义成一个静态变量： 代码块11234567891011121314151617181920212223242526public class Animal { //这里定义一个静态变量，用来标记该类属于那种生物分类 public static String taxonomy = \"动物类\"; private String name = \"xx\"; public int age; public void cry() { System.out.println(\"叫声为：未知\"); } public void eat() { System.out.println(this.name + \"正在吃东西\"); } public void setName(String name) { this.name = name; } public void setAge(int age) { this.age = age; } } 我们怎么访问这个字段呢？我们前面已经说过，通过“类名.属性/方法”即可触发： 代码块2123public static void main(String[] args) { System.out.println(Animal.taxonomy);} 看，这样就可以访问到静态属性了，不光属性是这样，方法调用也是一样的，现在我们加一个工具方法，用来判断一个动物的年龄是否合法： 代码块312345678910111213141516public class Animal { //这里定义一个静态变量，用来标记该类属于那种生物分类 public static String taxonomy = \"动物类\"; private String name = \"xx\"; public int age; ...省略... public static boolean isAgeTooLong(int age) { //这个静态方法用来判断一个年龄对于一个动物来讲是否合理 return age &gt; 10000; } } 方法的调用也是类似： 代码块41234public static void main(String[] args) { System.out.println(Animal.taxonomy); //访问静态属性 System.out.println(Animal.isAgeTooLong(1)); //调用静态方法} 那静态的变量/方法跟普通的变量/方法有什么区别呢？我们来看下流程图： 按照这个特性，非静态方法仍然可以调用静态方法： 在Animal内部调用甚至可以省略Animal，想想这是为什么？因为通过图1可以知道，静态内容是属于类模板本身的，而非静态内容则是类对其对象所做的约束定义，通过这些非静态的内容规定了它产生的对象所包含的内容，所以对于静态内容来说，只要有访问权限（是的，静态内容也受访问权限修饰符的影响，规则跟之前相符），不管在哪里，都可以通过类名调用。 static主要用来定义一些工具方法或者属性，这些方法和属性仅为了处理一些非对象级别的操作，比如例子里判断年龄对于动物是否合法的方法，你可以认为它就是一个工具方法，因为它跟具体的动物对象没有关系，而是对整个动物圈都有效，那你就可以把它搞成一个静态工具类。 ⚠️ static是不可以直接修饰类的，只能修饰内部类，被static修饰的内部类被称作静态内部类，有关内部类的内容在后面的文档，这里先不做了解。 二、final关键词在Java中，final关键字可以用来修饰类、方法和变量（包括成员变量和局部变量）。下面就从这三个方面来了解一下final关键字的基本用法。 2.1：final修饰类当用final修饰一个类时，表明这个类不能被继承。也就是说，如果一个类你永远不会让他被继承，就可以用final进行修饰。final类中的成员变量可以根据需要设为final，但是要注意final类中的所有成员方法都会被隐式地指定为final方法。 例如你不希望Animal产生任何的子类，那么可以这样定义Animal： 使用final修饰了Animal之后，Cat这里就会报错： 因为被final修饰的class，不允许被继承。 ❓ 思考：abstract关键词可以和final一起使用吗？ 2.2：final修饰方法被final修饰的方法，不允许被覆盖重写，比如我们把Animal里的cry方法改成final的： 这时Cat类里就会报错： 可以看到，IDE会提示被final修饰的方法无法被重写。 ⚠️ 在继承关系中，父类除了final方法不允许被子类重写，private的方法同样不允许，private方法会被隐式的指定为final方法。 2.3：final修饰变量final可以修饰成员变量、局部变量，被final修饰了的变量，不可以被再次赋值，例如： 被final修饰的成员变量和局部变量的初始值赋值也是有区别的： 可以看到，当成员变量设置为final时如果不赋初始值，则会提示报错，而局部变量则不会，局部变量只需要在第一次使用前赋初始值即可，赋完之后再次赋值才会提示重复赋值的错误。 结论就是final关键词的目标就是保证变量内容仅可以被赋值一次，被final修饰的变量更像是一个常量，因为它并不会再次发生改变了。 final在实际开发过程中通常和static一起使用，用来保存一个类里的常量： 试想一下为什么，首先这是个静态数据，还是个public的，这就意味着任意类任意地方都有可能把它的值给改掉，这就不符合常量这一标准，所以我们需要使用final把它锁起来。 我们再来发散下思维，final控制数据不被再次赋值，那么引用变量会不会改变？ 我们之前讲过，引用变量内部保存了一个对象的地址信息，而被final修饰的变量不可以被再次赋值，我们举个例子： 所以final仅仅保证一个变量不会再次被赋值，但不保证引用变量引用的对象内容不发生改变。","link":"/2020/03/12/LV3-1%EF%BC%9A%E4%BD%9C%E7%94%A8%E5%9C%A8%E7%B1%BB%E3%80%81%E5%8F%98%E9%87%8F%E3%80%81%E6%96%B9%E6%B3%95%E4%B8%8A%E7%9A%84%E5%85%B6%E5%AE%83%E4%BF%AE%E9%A5%B0%E7%AC%A6/"},{"title":"LV2-7：【案例】学生信息管理系统","text":"学完了类，我们利用类，结合现有的软件设计的分层思想，来做一个基本的学生信息管理系统，因为我们还没有接触数据库，所以我们利用数组来做基本的存储，该系统拥有的功能如下： 欢迎页面，介绍功能点、指令 指令1，新增学生信息，输入完最后一个学生属性后，输出“ok，已完成录入”的信息，然后回到欢迎页 指令2，修改学生信息，跟1一样，输入学生id确定需要修改的学生，首先输出其信息，然后输入一个个的新属性值直到完最后一个学生属性后，输出“ok，已完成修改”的信息，然后回到欢迎页 指令3，删除学生信息，输入学生id，删除对应的学生信息，删除后，输出“ok，已删除”，然后回到欢迎页 指令4，根据id查询学生信息，输入学生id，找到其信息，然后输出其信息，然后回到欢迎页 指令5，输出所有学生的信息，最好以一个表格的形式展示，然后回到欢迎页 指令0，终止程序 首先我们来看下项目的基本结构： 一、Model层首先抽象好一个Person类来做Student的父类： 代码块11234567891011121314151617181920212223242526272829303132333435363738394041424344454647package student.system.model;//这里定义一个父类，定义人最基本的属性，为什么要把人独立出来做父类？如果以后还有老师类，就不用再写一遍下面的属性了啊，直接继承即可public class Person { //名字 private String name; //年龄 private int age; //性别：0女性，1男性 private int gender; //住址 private String address; //下面就是用来设置属性值和获取属性值的方法 public String getName() { return name; } public void setName(String name) { this.name = name; } public int getAge() { return age; } public void setAge(int age) { this.age = age; } public int getGender() { return gender; } public void setGender(int gender) { this.gender = gender; } public String getAddress() { return address; } public void setAddress(String address) { this.address = address; }} 这里面存放学生作为「人」的基本属性。 定义一个班级类，用来表示学生所在的班级，最终它要和学生类关联起来： 代码块2123456789101112131415161718192021222324252627package student.system.model;//班级信息类public class Grade { //班级编号 private int id; //班级名称 private String name; //下面就是用来设置属性值和获取属性值的方法 public int getId() { return id; } public void setId(int id) { this.id = id; } public String getName() { return name; } public void setName(String name) { this.name = name; }} 最后定义出来学生类： 代码块3123456789101112131415161718192021222324252627282930313233343536package student.system.model;//学生类，最终存放学生信息的类，学生是人，所以他们具备所有人的属性，除此之外，他们有自己的一些拓展属性public class Student extends Person { //学号 private int id; //平均成绩 private float avg_score; //所属班级 private Grade grade; //下面就是用来设置属性值和获取属性值的方法 public int getId() { return id; } public void setId(int id) { this.id = id; } public float getAvg_score() { return avg_score; } public void setAvg_score(float avg_score) { this.avg_score = avg_score; } public Grade getGrade() { return grade; } public void setGrade(Grade grade) { this.grade = grade; }} 可以看到，学号、成绩这些都是学生特有的，像什么性别年龄这些，都是作为「人」共有的，为了达到复用，我们把它们抽了出来放到Person类，日后如果需要拓展一个老师类，那么老师也符合这些信息，直接继承复用即可。 二、Dao层这一层用来模拟数据库存储，主要用来实现增删改查最基本的元操作： 代码块4123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778package student.system.dao;import student.system.model.Student;//这个类位于我们定义的dao层，dao层一般负责和数据库交互进行最基本的增删改查操作//由于我们没有数据库，就在dao层这个类里定义一个数组用来保存student信息吧public class StudentDao { //在实际的项目里，你可以理解这个数组相当于某个数据库的student表 private Student[] students = new Student[10]; //假如数据库的容量仅为10，超过这个量，就会报错 private int studentsNum = 0; //数组下标 //增：用来添加student信息进students里 public boolean add(Student student) { //接收一个Student类型的参数，这个student就是需要add方法做存储处理的 if (isOverflow()) { System.out.println(\"数据库已满，添加失败！\"); return false; } else { students[studentsNum] = student; //保存这个student studentsNum++; //为了便于下一个数据的存储且不覆盖当前位置的数据，这里下标自增1 return true; } } //删：按照学生的id进行删除该学生信息 public boolean del(int id) { //ps：数组做删除是件非常麻烦的事情-_-||，这里偷懒了，只需要知道该方法可以完成一个数据的删除即可 Student[] newArray = new Student[10]; int j = 0; for (int i = 0; i &lt; studentsNum; i++) { if (students[i].getId() != id) { newArray[j] = students[i]; j++; } } students = newArray; studentsNum--; //删除掉一个数据，下标自减1 return true; } //改：根据输入的新的学生信息，按照id替换掉原先对应的学生信息 public boolean update(Student student) { boolean result = false; for (int i = 0; i &lt; studentsNum; i++) { if (students[i] != null &amp;&amp; students[i].getId() == student.getId()) { students[i] = student; result = true; break; //终止循环 } } return result; } //查：按照学生id，查询学生信息 public Student getStudentById(int id) { for (int i = 0; i &lt; studentsNum; i++) { if (students[i] != null &amp;&amp; students[i].getId() == id) { return students[i]; //找到了直接返回出去 } } return null; //找不到就返回空对象 } //列表：返回当前所有的学生信息 public Student[] getAllStudents() { Student[] nowStudents = new Student[studentsNum]; //这个操作会把students里下标为0~studentsNum的元素copy给nowStudents System.arraycopy(students, 0, nowStudents, 0, studentsNum); return nowStudents; } //用于内部判断当前数据库是不是已经存满数据了 private boolean isOverflow() { return studentsNum == 10; }} 三、Service层这一层主要跟Dao层交互，通常会利用Dao层里的元操作来完成更加复杂的业务性质的操作： 代码块512345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152package student.system.service;import student.system.dao.StudentDao;import student.system.model.Student;//service层的类，它触发Dao层的基本操作的同时，还会用来处理更加复杂的业务逻辑public class StudentService { private StudentDao studentDao = new StudentDao(); //因为service层需要做基本的增删改查操作，因此它持有一个StudentDao类型的属性 //这个方法用来触发Dao层的\"增\"方法 public boolean add(Student student) { if (student == null) { System.out.println(\"请输入合理的student！\"); return false; } //你会发现不就是简单触发一下Dao层的add方法吗？为啥需要单独拆一个Service层来套娃呢？因为我上面解释过了，service是用来处理复杂业务逻辑的 //真正的service层可能会引很多Dao层进行混合操作，本例之所以这么简单是因为这个系统本身就很简单啊。。理解这个点就好~ return studentDao.add(student); } //这个方法用来触发Dao层的\"删\"方法 public boolean del(int id) { if (id &lt;= 0) { System.out.println(\"请输入合理的id！\"); return false; } return studentDao.del(id); } //这个方法用来触发Dao层的\"改\"方法 public boolean update(Student student) { if (student == null) { //这块代码跟add里的重复，理论上可以封装成一个独立的方法，但这里我们不这么干了。。所以说有些规则只是理想化下会选择这么做，但实际开发中可能并没有这么严格 System.out.println(\"请输入合理的student！\"); return false; } return studentDao.update(student); } //这个方法用来触发Dao层的\"查\"方法 public Student getStudentById(int id) { if (id &lt;= 0) { return null; } return studentDao.getStudentById(id); } //这个方法用来触发Dao层的\"列表\"方法 public Student[] getAllStudents() { return studentDao.getAllStudents(); }} 四、View层这一层负责和用户交互，将用户输入的指令传达给Service，并将Service的处理结果通知给用户，这一层在企业级软件设计里被称为视图层（虽然MVC在很多场景下已经“过时”，但是我们仍然可以从中学到企业级开发时所需要的分层思想）： 代码块6123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174package student.system.view;import student.system.model.Grade;import student.system.model.Student;import student.system.service.StudentService;import java.util.Scanner;//视图层，用来展示给用户看的public class StudentSystemView { //因为视图层需要请求基本的增删改查操作，因此它持有一个StudentService类型的属性 private StudentService studentService = new StudentService(); public static void main(String[] args) { StudentSystemView view = new StudentSystemView(); while (true) { int opNum = view.home(); if (opNum == 0) { //终止程序 break; } else if (opNum == 1) { //需要新增学生信息 view.addStudent(); } else if (opNum == 2) { view.updateStudent(); } else if (opNum == 3) { view.delStudent(); } else if (opNum == 4) { view.getStudentById(); } else if (opNum == 5) { view.getAllStudents(); } else { System.out.println(\"请输入有效的指令！\"); } } } //首页 public int home() { System.out.println(\"+----------------------------------\"); System.out.println(\"| S大学-学生信息管理系统\"); System.out.println(\"+----------------------------------\"); System.out.println(\"| 指令集：\"); System.out.println(\"| 1.新增\"); System.out.println(\"| 2.修改\"); System.out.println(\"| 3.删除\"); System.out.println(\"| 4.查找\"); System.out.println(\"| 5.学生列表\"); System.out.println(\"| 0.终止程序\"); System.out.println(\"+----------------------------------\"); System.out.println(\"请输入您的操作：\"); Scanner s = new Scanner(System.in); return s.nextInt(); } //添加学生信息 public void addStudent() { System.out.println(\"您现在正在添加一个学生的信息，请按照指令输入学生的各项信息：\"); Student student = scannerStudent(); boolean result = studentService.add(student); //最后我们通过studentService将这个学生信息加到\"数据库\"里去 //按照service层的处理结果，输出不同的文案 if (result) { System.out.println(\"数据添加成功！\"); } else { System.out.println(\"数据添加失败！\"); } } //删除学生信息 public void delStudent() { System.out.println(\"您现在正在删除一个学生的信息，请输入需要删除学生的id：\"); Scanner s = new Scanner(System.in); int id = s.nextInt(); boolean result = studentService.del(id); //最后我们通过studentService将这个学生信息加到\"数据库\"里去 //按照service层的处理结果，输出不同的文案 if (result) { System.out.println(\"数据删除成功！\"); } else { System.out.println(\"数据删除失败！\"); } } //修改学生信息 public void updateStudent() { System.out.println(\"您现在正在修改一个学生的信息，请按照指令输入学生的各项信息：\"); Student student = scannerStudent(); boolean result = studentService.update(student); //最后我们通过studentService将对应的学生信息修改掉 //按照service层的处理结果，输出不同的文案 if (result) { System.out.println(\"数据修改成功！\"); } else { System.out.println(\"数据修改失败！\"); } } //按照id查找学生信息 public void getStudentById() { System.out.println(\"请输入需要查找学生的id：\"); Scanner s = new Scanner(System.in); int id = s.nextInt(); Student result = studentService.getStudentById(id); //最后我们通过studentService查找到对应的学生信息 //按照service层的处理结果，输出不同的文案 if (result != null) { printStudentInfo(result); } else { System.out.println(\"未查到对应数据！\"); } } //输出当前\"数据库\"内所有学生的信息 public void getAllStudents() { Student[] result = studentService.getAllStudents(); //最后我们通过studentService将所有的学生信息拿到 //按照service层的处理结果，输出不同的文案 if (result != null) { for (Student student : result) { printStudentInfo(student); } } else { System.out.println(\"未查到对应数据！\"); } } //用来让用户输入一个学生的信息，由于修改信息那里以及新增信息那里都需要这个操作，而且这个操作代码又很长，因此为了复用这块的代码，我们把它封装出来，以供别人重复使用 private Student scannerStudent() { System.out.println(\"请输入学生的id：\"); Scanner s = new Scanner(System.in); int id = s.nextInt(); System.out.println(\"请输入学生的名字：\"); String name = s.next(); System.out.println(\"请输入学生的年龄：\"); int age = s.nextInt(); System.out.println(\"请输入学生的性别（0女1男）：\"); int gender = s.nextInt(); System.out.println(\"请输入学生的住址：\"); String address = s.next(); System.out.println(\"请输入学生的平均分：\"); float avgScore = s.nextFloat(); System.out.println(\"请输入学生所属班级的id：\"); int gradeId = s.nextInt(); System.out.println(\"请输入学生所属班级的名称：\"); String gradeName = s.next(); //开始根据输入的信息组装学生对象 Student student = new Student(); student.setId(id); student.setName(name); student.setAge(age); student.setGender(gender); student.setAddress(address); student.setAvg_score(avgScore); //构建一个班级类 Grade grade = new Grade(); grade.setId(gradeId); grade.setName(gradeName); student.setGrade(grade); //给student对象里持有的grade属性赋值 return student; } //按照某种格式打印出来一个student信息的方法，在getStudentById和getAllStudents中均有使用 private void printStudentInfo(Student student) { System.out.println(\"id = \" + student.getId() + \"\\t name = \" + student.getName() + \"\\t age = \" + student.getAge() + \"\\t address = \" + student.getAddress() + \"\\t gender = \" + (student.getGender() == 1 ? \"男\" : \"女\") + \"\\t avg_score = \" + student.getAvg_score() + \"\\t address = \" + student.getAddress() + \"\\t grade_id = \" + student.getGrade().getId() + \"\\t grade_name = \" + student.getGrade().getName()); }} 六、运行结果直接运行view层代码的main方法，开始录入学生并完成一次查找： 录入学生信息&查找学生信息1234567891011121314151617181920212223242526272829303132333435363738394041424344454647+----------------------------------| S大学-学生信息管理系统+----------------------------------| 指令集：| 1.新增| 2.修改| 3.删除| 4.查找| 5.学生列表| 0.终止程序+----------------------------------请输入您的操作：1您现在正在添加一个学生的信息，请按照指令输入学生的各项信息：请输入学生的id：1请输入学生的名字：s请输入学生的年龄：26请输入学生的性别（0女1男）：1请输入学生的住址：shanghai请输入学生的平均分：100请输入学生所属班级的id：1请输入学生所属班级的名称：2.1数据添加成功！+----------------------------------| S大学-学生信息管理系统+----------------------------------| 指令集：| 1.新增| 2.修改| 3.删除| 4.查找| 5.学生列表| 0.终止程序+----------------------------------请输入您的操作：4请输入需要查找学生的id：1id = 1 name = s age = 26 address = shanghai gender = 男 avg_score = 100.0 address = shanghai grade_id = 1 grade_name = 2.1 你可以自己模仿着写一个类似这种指令输入方式的“xxx管理系统”，然后自己可以运行下，感受下。","link":"/2020/03/12/LV2-7%EF%BC%9A%E3%80%90%E6%A1%88%E4%BE%8B%E3%80%91%E5%AD%A6%E7%94%9F%E4%BF%A1%E6%81%AF%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/"},{"title":"LV2-6：java原生类&第三方包依赖","text":"截止到目前，我们已经了解了类定义、类的特性、封装等概念，就目前而言，我们已经可以自定义一些自己的类了，你甚至可以从继承多态、抽象封装的角度上，来设计你的类。 但是java作为一个高级语言，并且社区之庞大，是其它语言所不能比的，我们除了可以使用自己定义的类，也可以使用现成的类，比如String，String就是java自带的一个类，它的内部属性包含一个byte数组。同样的，像后面讲的集合类，位图等数据结构，都是java自带的工具类，我们都是可以直接使用的。 抛去这些java自带的类，我们还想要其他更加高级的功能，就需要引第三方包了，引入之后，我们就可以使用他们定义的类了。 比如，你现在需要用到guava这个第三方包里的类，那你就把它下载下来，导入到你的java项目里即可。 https://mvnrepository.com 上面这个网站用来搜索和下载第三方jar包。 然后怎么导入呢？ 我们以Idea为例： 然后在弹出的对话框里这样选： 然后选择你需要导入的jar文件导入后点右下角的OK键即可。 guava的jar包里放的就是guava相关的所有代码（其实是所有代码编译后的字节码，然后压缩成的jar文件），你项目里引入了，便可以使用它们了。","link":"/2020/03/12/LV2-6%EF%BC%9Ajava%E5%8E%9F%E7%94%9F%E7%B1%BB&%E7%AC%AC%E4%B8%89%E6%96%B9%E5%8C%85%E4%BE%9D%E8%B5%96/"},{"title":"LV2-5：接口","text":"接口（英文：Interface），在JAVA编程语言中是一个抽象类型，是抽象方法的集合，接口通常以interface来声明。一个类通过继承接口的方式，从而来继承接口的抽象方法。接口并不是类，编写接口的方式和类很相似，但是它们属于不同的概念。类描述对象的属性和方法。接口则包含类要实现的方法。除非实现接口的类是抽象类，否则该类要定义接口中的所有方法。接口无法被实例化，但是可以被实现。一个实现接口的类，必须实现接口内所描述的所有方法，否则就有必要声明为抽象类。 简单来总结下：接口类似一个只有抽象方法的抽象类，就像抽象类一样，它无法被实例化，必须要有实现类它才会有意义。 一、接口的定义&amp;用法1.1：例子&amp;用法用最简单最好理解的方式来说，所谓接口就是一个只有抽象方法的”抽象类“（这里只是类比，接口不是类），接口跟抽象类、抽象类跟普通类之间的区别和意义，我们放到第二节来讲，这里看个案例： 代码块1123456789101112131415161718192021222324252627282930313233343536373839404142434445464748//通过interface关键词定义接口public interface TrafficTools { //交通工具接口，用来抽象符合交通工具所具备的一系列行为标准 String move(); //所有的交通工具都具备移动这一标准 } //下面是实现了该接口的两种具体的交通工具//------------------------------------------------------------------------------------------------ //使用implements关键词实现接口public class HighSpeedRailway implements TrafficTools { //高铁是一种交通工具，符合交通工具所定义的标准 private String speed = \"306km/h\"; //高铁属性你就随意抽象，这里抽象了一个叫速度的属性出来 @Override public String move() { return \"靠行走在\" + railway() + \"上的火车，以\" + speed + \"的速度\"; } public String railway() { //高铁是走在高铁铁轨上的 return \"高速轨道铁路\"; } public void setSpeed(String speed) { this.speed = speed; }} public class Plane implements TrafficTools { //飞机是一种交通工具，符合交通工具所定义的标准 private String company = \"华航\"; //飞机属性你就随意抽象，比如飞机型号、名称、所属公司，按需定义 @Override public String move() { //实现move方法 return \"靠\" + company + \"的飞机，以\" + fly() + \"的方式\"; } public String fly() { //具体的移动方式，飞机当然是靠飞了 return \"飞行\"; } public void setCompany(String company) { this.company = company; }} 接口不允许有属性，可以简单理解接口是定义一系列动作（即方法）的标准，任意符合这个标准的类都可以实现这个接口，比如例子中的飞机和高铁类，都可以实现TrafficTools接口，因为它们都是交通工具，都有move这一基本标准。现在定义好了交通工具，让我们再定义旅游类，它是一个抽象类，抽象出来了旅行所具备的属性和方法： 代码块2123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public abstract class Tour { private String start; //出发点 private String end; //终点 private String name; //主角们的名字 private TrafficTools tools; //旅行需要一种交通工具 public abstract String kind(); //旅行方式，这个需要子类去实现，旅游种类：度假、蜜月、探亲等 public abstract String play(); //玩了什么？同样旅行类无法实现，只能靠子类去实现 public String getTour() { //输出对这次旅行的描述 return kind() + tools.move() + \"，从\" + start + \"出发，抵达\" + end + play(); } //属性的设置方法 public void setStart(String start) { this.start = start; } public void setEnd(String end) { this.end = end; } public void setName(String name) { this.name = name; } public String getName() { return name; } public void setTools(TrafficTools tools) { this.tools = tools; }} //下面是两个实现了旅行类的子类，分别代表着不同的旅行类型//----------------------------------------------------------------------------------------------------------------public class AloneTour extends Tour { //个人旅行是旅行的一种 @Override public String kind() { return super.getName() + \"的个人旅行\"; } @Override public String play() { return \"完成了自己的个人旅行，顺便修了个仙儿~\"; }} public class HoneyMoonTour extends Tour { //蜜月旅行是旅行的一种 @Override public String kind() { return super.getName() + \"的蜜月旅行\"; } @Override public String play() { return \"玩了专为情侣准备的蜜月项目\"; }} 让我们来具体测试下： 代码块312345678910111213141516171819public static void main(String[] args) { Tour myAloneTour = new AloneTour(); //打算开始一次个人旅行 myAloneTour.setStart(\"东京都\"); //始发地 myAloneTour.setEnd(\"雏见泽\"); //目的地 myAloneTour.setName(\"前原圭一\"); //需要进行这次旅行的主角名字 //选择一种交通工具 TrafficTools tools = new HighSpeedRailway(); //选择高铁（可以看到，接口类的引用变量也可以接收自己实现类的对象实例） myAloneTour.setTools(tools); System.out.println(myAloneTour.getTour()); //输出这次旅行的内容 Tour hmTour = new HoneyMoonTour(); //打算开始一场蜜月旅行 hmTour.setStart(\"木叶忍者村\"); //始发地 hmTour.setEnd(\"涡の国\"); //目的地 hmTour.setName(\"漩涡鸣人&amp;日向雏田\"); //需要进行这次旅行的主角名字 //选择一种交通工具 TrafficTools tools2 = new Plane(); //选择飞机 hmTour.setTools(tools2); System.out.println(hmTour.getTour()); //输出这次旅行的内容} 上方代码输出如下： 代码块412前原圭一的个人旅行靠行走在高速轨道铁路上的火车，以306km/h的速度，从东京都出发，抵达雏见泽完成了自己的个人旅行，顺便修了个仙儿~漩涡鸣人&amp;日向雏田的蜜月旅行靠华航的飞机，以飞行的方式，从木叶忍者村出发，抵达涡の国玩了专为情侣准备的蜜月项目 看完本例，你可以了解接口的基本用法。 非常重要的一点：接口支持多继承，一个类可以同时实现多个接口 举个非常简单的例子： 代码块5123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990public interface A { void a(); } public interface B { void b(); } public interface C extends A, B { //接口的多继承，这时如果一个类要想实现C，则需要同时实现ABC里的方法 void c(); } public interface D { void d(); } public class E implements C, D { //一个类可以同时实现多个接口，接口里所有的方法均需要实现 @Override public void c() { } @Override public void a() { } @Override public void b() { } @Override public void d() { }} public abstract class F implements C, D { //同样的，抽象类也可以同时实现多个接口，但由于抽象类的特性，它可以不用实现接口里的方法，全员叫给它的子类去做，当然，也可以选择实现，如下方c方法 abstract void f(); void f2() { } @Override public void c() { }} public class G extends F { //继承了上方抽象类的子类，需要同时实现父类的抽象方法f以及父类继承了的接口且没有进行实现的方法 @Override void f() { } @Override public void a() { } @Override public void b() { } @Override public void d() { }} 接口、抽象类、类之间的关系不管依赖多么复杂，当方法发生调用时，均遵循上一章节讲的”就近调用“规则。 1.2：接口与类相似点 一个接口可以有多个方法。 接口文件保存在.java结尾的文件中，文件名使用接口名。 接口的字节码文件保存在.class结尾的文件中。 接口相应的字节码文件必须在与包名称相匹配的目录结构中。 1.3：接口与类的区别 接口不能用于实例化对象。 接口没有构造方法。 接口中所有的方法必须是抽象方法。 接口不能包含成员变量，除了static和final变量。 接口不是被类继承了，而是要被类实现。 接口支持多继承。 二、类、抽象类、接口之间的区别在java的世界里，一切皆对象，而对象又是根据类的模板生成的符合类定义的一个个体，所以类是对一类事物的具体抽象。 抽象类无法被实例化，它是为普通类服务的，简单来说，普通类用来抽象真实存在的事物，给它们分类（猫、狗、数据库对象等），而抽象类是位于普通类上层的用来给类分类的，比如猫狗，都可以拆分成猫类，狗类，但是猫和狗具有相同意义的属性和方法，例如年龄、吃饭睡觉等，这些就可以再次被分类，成为猫和狗的父类，比如动物，你会发现不光猫狗具备这些被抽象出来的属性和方法，日后如果定义一个人类，同样也是符合的，抽象类就是为类分类而生的，它必须要有子类继承才有意义，抽象类可以自己定义方法的实现，也可以把一些方法全部交给子类去实现，如果一个抽象类里并不存在被抽象出来的属性，只有一些行为规范，而且这些行为规范抽象类自己实现没有意义，只能交给子类去实现，这个时候就可以将抽象类里的方法全部定义成抽象方法（现在很像接口了对吧），而接口就是这种位于上层里仅定义行为规范的一种结构。","link":"/2020/03/12/LV2-5%EF%BC%9A%E6%8E%A5%E5%8F%A3/"},{"title":"LV2-4：类的特性、关系","text":"一、继承和多态1.1：一个例子参考之前的Cat类，如果我现在让你设计一个Dog类，它仍然具备眼睛、性别等属性，也会跑、吃等动作，这个时候你一定会发现，单独再为Dog搞一套跟Cat差不多的类定义吗？就没有更聪明的办法吗？ 此时java里允许的类继承就可以起到作用了，想想我们的目标，我们的目标是让这些重复的属性不再重复定义一遍，那么像眼睛、毛色这种属性，或者跑、吃饭这类的操作就可以再拆出一个类：动物类 现在我们来定义下动物类： 代码块1123456789101112131415161718192021222324public class Animal { private String name = \"xx\"; //动物学名，动物们都有学名，这里默认为xx，因为动物类是用来描述动物的，动物有学名，但你并不知道是那种动物，name只能未知 private int age; //动物们的年龄 public void cry() { //控制叫声的方法 System.out.println(\"叫声为：未知\"); } public void eat() { //负责吃饭的方法 System.out.println(name + \"正在吃东西\"); } //用来给name赋值的方法 public void setName(String name) { this.name = name; } //用来给age赋值的方法 public void setAge(int age) { this.age = age; } } 然后再定义一下Cat和Dog类： 代码块2123public class Cat extends Animal { //让Cat类继承Animal类 } 代码块3123public class Dog extends Animal { //让Dog类继承Animal类 } Cat和Dog都是一个”空“类，但是分别利用extends关键词继承了Animal类，现在让我们进行如下测试： 代码块412345678910111213141516171819202122232425public static void main(String[] args) { //造一只9岁的猫 Cat cat = new Cat(); cat.setAge(9); cat.setName(\"猫\"); //让这只猫叫一声，再吃一口东西 cat.cry(); cat.eat(); //造一只10岁的狗 Dog dog = new Dog(); dog.setAge(10); dog.setName(\"狗\"); //让这只狗叫一声，再吃一口东西 dog.cry(); dog.eat(); //造一只10岁的狗，但不设置学名 Dog dog2 = new Dog(); dog2.setAge(10); //让这只狗叫一声，再吃一口东西 dog2.cry(); dog2.eat();} 上述程序输出结果如下： 代码块5123456叫声为：未知猫正在吃东西叫声为：未知狗正在吃东西叫声为：未知xx正在吃东西 可以看到，Cat和Dog虽然啥都没定义，但仍然可以使用Animal类里的方法，我们此时管Cat和Dog叫做Animal的子类，相反的Animal叫做Cat和Dog的父类。 可以通过这个例子发现，子类通过继承，可以得到父类的一些功能，需要确认的一个点是：在初始化完成一个子类初始化的同时其父类信息也会自动加载进子类，然后子类可以共享父类里的方法和属性。 这里说一下this关键词，它表示的是本类所产生的具体对象对其内部内容做访问时用的一个标识，你甚至可以省略不写。 1.2：父类方法覆盖通过上面的输出，我们发现，猫狗的cry方法输出的是Animal默认的内容，这时我们需要猫和狗能有自己的叫声，那么这时就可以选择重写父类的方法： Cat类重写父类的cry方法： 代码块61234567public class Cat extends Animal { @Override //加上@Override标识为方法重写 public void cry() { System.out.println(\"喵喵喵~\"); //这里重新定义方法的实现 }} Dog类重写父类cry方法： 代码块712345678public class Dog extends Animal { @Override public void cry() { System.out.println(\"汪汪汪~\"); } } 现在再运行下测试代码输出如下： 代码块81234喵喵喵~猫正在吃东西汪汪汪~狗正在吃东西 通过这个例子，我们可以知道，如果子类不想要父类里的实现，那么可以通过重写的方式重新设计对应的方法，例子中通过重写cry方法，让Cat和Dog都拥有了自己的cry方法。 1.3：子类方法扩展经过上面一步，我们知道猫和狗都是动物，所以它们离不开动物类里定义的属性和方法，但是猫和狗仍存在一些不太一样的地方，比如猫具有捕鼠的能力，但不具备看家的能力，而狗具有看家能力，却没有捕鼠的能力，这种相对比较独立的能力就可以用来扩展： Cat类新增捕鼠方法 代码块9123456789101112public class Cat extends Animal { @Override //加上@Override标识为方法重写 public void cry() { System.out.println(\"喵喵喵~\"); //这里重新定义方法的实现 } public void catchMice() { //扩展方法：捕鼠 System.out.println(\"我会捉老鼠\"); } } Dog类新增看家方法： 代码块10123456789101112public class Dog extends Animal { @Override public void cry() { System.out.println(\"汪汪汪~\"); } public void houseKeeping() { //扩展方法：看家 System.out.println(\"我会看家\"); }} 通过本例我们知道，子类是可以自由扩展的，通过子类的引用变量依然可以完成调用： 代码块1112345678public static void main(String[] args) { Cat cat = new Cat(); cat.catchMice(); Dog dog = new Dog(); dog.houseKeeping();} 输出： 代码块1212我会捉老鼠我会看家 1.4：多态现在来介绍下多态，多态是一种类和类之间的一种引用关系，因为有了继承，才有了多态这种特性，我们现在来改造下代码块4： 代码块131234567891011121314151617public static void main(String[] args) { Animal cat = new Cat(); //声明变量时，改成父类类型 cat.setAge(9); cat.setName(\"猫\"); cat.cry(); cat.eat(); Animal dog = new Dog(); //声明变量时，改成父类类型 dog.setAge(10); dog.setName(\"狗\"); dog.cry(); dog.eat(); } 运行如下： 代码块141234喵喵喵~猫正在吃东西汪汪汪~狗正在吃东西 仍然可以正常运行，并且被重写的方法cry仍然执行的是子类里的那个。 这就是多态，如果不太好理解，我们可以通过下图来加深一下记忆： 这种使用父类修饰的引用变量，可以指向其任意子类对象的行为，我们称为类的多态，可以发现父类接收的子类对象无法访问扩展内容，但仍然可以访问父类所具备的内容，哪怕被重写的方法cry，但这时由于父类指向的还是子类对象，因此所触发的内容均属于子类，这里先不探讨多态的好处，你只需要加深对代码块13和图1的理解类的这种特性即可。 ❓ 疑问点1：为什么通过Animal的引用变量指向的Cat对象，无法调用其catchMice方法？为什么cry方法可以调用触发的却是Cat里的cry方法？ 1.5：如何直接访问父类的内容？1.5.1：直接new父类也是一个普通的类嘛，要想不受子类的任何影响，直接new不就完事儿了。但一般父类都不太可能直接new，在多子类的情况下，又该怎样直接访问自己的资源呢？来接着往下看1.5.2吧。 1.5.2：super关键词通过上面的介绍，我们了解了类的继承和多态，子类在继承了父类的功能和属性之后，可以自定义方法实现对父类的扩展，针对实现不合子类要求的方法，子类也可以通过方法重写来覆盖原父类方法，然后就是多态，多态简单来说就是通过一个父类引用变量直接指向一个子类对象，还是结合图1理解。 那么我们现在再来做一个实验，现在我想让Cat和Dog在吃饭的时候自带叫声，现在我们来继续改造下这些类： 首先把Animal里的eat方法改造下，让它的逻辑执行之前，先调用下cry方法： 代码块151234567891011121314151617181920212223public class Animal { private String name = \"xx\"; private int age; public void cry() { System.out.println(\"叫声为：未知\"); } public void eat() { this.cry(); //吃饭之前先叫两嗓子 System.out.println(name + \"正在吃东西\"); } public void setName(String name) { this.name = name; } public void setAge(int age) { this.age = age; } } 别的代码不动，单独测试下eat方法： 代码块161234567public static void main(String[] args) { Animal cat = new Cat(); cat.eat(); Animal dog = new Dog(); dog.eat();} 最终输出： 代码块171234喵喵喵~xx正在吃东西汪汪汪~xx正在吃东西 这里说一下，即便代码块16直接用Cat和Dog接收对应的类对象，结果都是一致的。 🦩 结论：new出来具体的子类之后，父类里即便使用this调用它内部的被子类重写的方法，实际触发的仍然是子类的方法。 ❓ 疑问点2：为什么呢？我明明是在Animal对象里调用的this.cry，为什么执行的却是子类的？ ok，那我们怎么直接访问父类里的方法呢？借助super关键词即可，同this关键词一样用来描述类对象内部引用的，与this不同的是，子类利用super可以调用父类资源，注意，这里说了，想用super，就得在某子类内，我们现在如果想调用Animal本身的cry方法，那么就需要在Cat或Dog里，通过super触发： 首先把Animal里面的eat方法还原： 代码块181234567891011public class Animal { ...省略... public void eat() { //负责吃饭的方法 System.out.println(name + \"正在吃东西\"); } ...省略...} 然后改造下Cat类，让它重写eat方法，我们目标是让这个eat方法先调用一遍父类原始的cry方法，再调用一次父类原始的eat方法： 代码块1912345678910111213public class Cat extends Animal { ...省略... @Override public void eat() { //这里重写eat，同时使用super直接调用父类的cry和eat方法 super.cry(); super.eat(); } ...省略... } 现在再次调用eat方法，输出结果如下： 代码块2012叫声为：未知xx正在吃东西 你会发现，现在的调用就是父类里面原生的方法。 现在，让我们把代码块19里的cry的super关键词去掉（之前说过，去掉相当于使用this关键词调用）： 代码块2112345678910111213public class Cat extends Animal { ...省略... @Override public void eat() { cry(); //这里把super去掉 super.eat(); //等看完这一块的内容之后，结合Cat类的代码，分析一下这里eat的super关键词去掉会发生什么？ } ...省略... } 然后调用eat的结果如下： 代码块2212喵喵喵~xx正在吃东西 可见，Cat里使用this调用cry方法，当然是调用的它自己的咯。 ❓ 疑问点3：为什么父类的被子类重写过的方法可怜到只能通过子类的super关键词才能触发？为什么父类里也有的方法要优先执行子类里重写后的？不管在父类还是子类，用this关键词调用的资源都是优先以子类为准吗？找不到的资源才去考虑找父类吗？ 1.6：继承链首先请记住现在的Animal类： 代码块2312345678910111213141516171819202122232425public class Animal { private String name = \"xx\"; //动物学名，动物们都有学名，这里默认为xx，因为动物类是用来描述动物的，动物有学名，但你并不知道是那种动物，name只能未知 private int age; //动物们的年龄 public void cry() { //控制叫声的方法 System.out.println(\"叫声为：未知\"); } public void eat() { //负责吃饭的方法 System.out.println(name + \"正在吃东西\"); } //用来给name赋值的方法 public void setName(String name) { this.name = name; } //用来给age赋值的方法 public void setAge(int age) { this.age = age; } } Cat类清理一下之前的测试代码，记住现在的Cat类： 代码块24123456789101112public class Cat extends Animal { @Override //加上@Override标识为方法重写 public void cry() { System.out.println(\"喵喵喵~\"); //这里重新定义方法的实现 } public void catchMice() { //扩展方法：捕鼠 System.out.println(\"我会捉老鼠\"); } } java语法里，只允许一个类继承一个类，也就是说，extends关键词只能在一个类定义里出现一次，根据这个规则，父类也是一个类，类允许继承一个别的类（套娃警告），描述起来是越来越乱，不如再改造下前面的例子来说明问题；现在我们发现Cat也只是定义了猫的基本概念，包括被它重写后的cry，确实，猫都是喵喵喵的叫，猫都会捕鼠，但是不同品种的猫，也存在差异，例如相比其它猫，橘猫的食量惊人，它还可以扮猪，如果这时定义一个橘猫类，首先它也是猫，那就让它继承Cat类，那么eat这个方法就可以单独拎出来再被重写一次，以显示橘猫特有的食量，其次应该单独加一个橘猫特有技能：扮猪，看代码： 代码块25123456789101112public class OrangeCat extends Cat { @Override public void eat() { //重写eat方法 System.out.println(\"我寻思你不能因为我吃的多就专门为我量身定做一个eat方法吧？。。。。。。艾玛真香~\"); } //假装自己是头猪 public void playAPig() { System.out.println(\"我说我是🐷，你能怎么办？（🤪）\"); }} ok，至少通过橘猫类，我们知道了，java虽然不允许一个类继承多个类，但可以间接继承多个类，现在的继承链为： 那么一样的，我们前面了解了java的继承和多态，这种继承链也是符合上述所有的继承相关的特性，比如我们来利用OrangeCat做个试验： 代码块26123456789101112131415public static void main(String[] args) { //继承链仍符合多态规则，看下方的代码，我用Animal和Cat都可以接收OrangeCat对象，因为它们俩都是OrangeCat的父类 Animal orangeCat1 = new OrangeCat(); Cat orangeCat2 = new OrangeCat(); OrangeCat orangeCat3 = new OrangeCat(); orangeCat1.cry(); orangeCat1.eat(); orangeCat2.cry(); orangeCat2.eat(); orangeCat3.cry(); orangeCat3.eat();} 结果如下： 代码块27123456喵喵喵~我寻思你不能因为我吃的多就专门为我量身定做一个eat方法吧？。。。。。。艾玛真香~喵喵喵~我寻思你不能因为我吃的多就专门为我量身定做一个eat方法吧？。。。。。。艾玛真香~喵喵喵~我寻思你不能因为我吃的多就专门为我量身定做一个eat方法吧？。。。。。。艾玛真香~ 可以看到，不管用谁去接收OrangeCat对象，最终输出结果都是正确的，首先OrangeCat没有重写cry方法，因此触发的是父类Cat里的cry方法，又因为自己重写了eat对象，所以eat调用时就直接触发了自己的eat方法。 同样的，playAPig方法属于OrangeCat自定义的扩展方法，它只能由OrangeCat类型的引用变量触发，因此代码块26里，只有orangeCat3可以调用这个方法，由于OrangeCat继承了Cat，所以它也会捕鼠（catchMice）了。 我们再来看看OrangeCat具备了哪些能力： 代码块28123456789public static void main(String[] args) { OrangeCat orangeCat3 = new OrangeCat(); orangeCat3.setAge(9); //其直系父类Cat继承了Animal，因此它也可以调用Animal的setAge方法 orangeCat3.setName(\"橘猫\"); //同上 orangeCat3.cry(); //其直系父类Cat有cry方法（虽然是重写Animal的cry来的），因此这里触发的是Cat里那个cry方法 orangeCat3.eat(); //因为自己有eat方法，因此这里触发的是自己的eat方法 orangeCat3.catchMice(); //其直系父类Cat有catchMice方法，因此这里触发的是Cat里那个catchMice方法 orangeCat3.playAPig(); //这是OrangeCat特有的方法，自然可以调用} 看到了吗，经过一层层的继承，OrangeCat拥有的技能要比俩父类加起来还多，这是因为继承的特性，就是父类所有的东西都是可以给子类的（当然能不能访问就是另一回事了，下面会说访问权限相关的内容），而子类又可以重写父类的方法，如果不重写，则按照就近原则触发对应的方法，比如例子里虽然OrangeCat没有重写cry方法，但它调用cry的输出结果却是Cat类里的那个，因此，在重写方法调用上，一般遵循就近原则，此外子类也可以扩展自己的内容，比如Cat相比Animal，可以捕鼠，再比如OrangeCat相比Cat和Animal，可以扮猪。 1.7：巩固继承&amp;多态前面讲了那么多，现在我们通过画图的方式，系统的解释一遍继承和多态。 1.7.1：继承关系下，重复方法的就近调用规则先看下我们在前面留下的3个问题： ❓ 疑问点1：为什么通过Animal的引用变量指向的Cat对象，无法调用其catchMice方法？为什么cry方法可以调用触发的却是Cat里的cry方法？ ❓ 疑问点2：为什么呢？我明明是在Animal对象里调用的this.cry，为什么执行的却是子类的？ ❓ 疑问点3：为什么父类的被子类重写过的方法可怜到只能通过子类的super关键词才能触发？为什么父类里也有的方法要优先执行子类里重写后的？不管在父类还是子类，用this关键词调用的资源都是优先以子类为准吗？找不到的资源才去考虑找父类吗？ 先不用急着看问题，我们先来分析下一个类方法或属性在调用时，都经历了些什么。 在LV2-1的时候，我们就知道了，对象实际上就是一个整体，被保存在了内存里，然后通过一个引用变量来指向它，然后就可以利用这个引用变量来操纵它了，现在我们来看下，正常没有继承任何类的类，是如何访问方法的： 🦜 这里说明一下，下面的流程图只是为了便于让大家理解并记住java继承关系下的访问规则，真实的java对象在内存里并不长这样（虽然确实包含属性值），方法的调用在java底层也很复杂，这些都要对jvm有深入了解才行。 如果一个类没有任何父类，那么它的方法调用就很纯粹，没有任何悬念，现在让我们的看看Cat类： 让我们再来看看更复杂的橘猫： 我们仍然可以找到一些规则，来说明在继承里方法的调用和最终触发哪个方法的问题。 好的，截止目前，集合图4和图5，我们解决了疑问点2和疑问点3，通过图4，我们知道了this关键词取决于当前的对象究竟属于哪个类，仍然符合就近调用规则，例如Animal里面使用this.cry调用cry，如果当前对象是Animal自己，那么毫无疑问，最终触发的会是它自己的cry方法，但如果当前对象是Cat，那么this调用cry时就遵循就近规则，优先触发自己所属类（也就是Cat类）里的cry方法。 图4告诉我们，super关键字在子类里可用来直接调用其父类里的方法： 这个可以解答疑问点3. 现在让我们解答下疑问点1，为什么父类引用变量可以接收子类对象？ 还记得讲基本类型时，存在大类型和小类型吗？大类型可以自动接收小类型，而小类型想要变成大类型就得强转，类同样拥有类似的规则，比如你可以把父类理解成比较大的类，子类是比较小的类，那么父类当然可以自动接收子类咯，只不过类有不一样的地方，一个子类被父类接收以后，访问域就受父类限制了： 既然规则跟基本类型差不多，当然也有类型强转啦： 代码块2912Animal cat = new Cat();Cat cat2 = (Cat)cat; 看吧，连写法都跟基本类型的强转一样，括号里加上要转的目标类型即可。 但是需要注意的是，别瞎转，比如你知道这个Animal的确是一个Cat，然后像代码块29里那样转成Cat类型即可，但如果对象并不是Cat，那样强转语法里是允许的，但是运行时会报类型转换错误，比如下面这样： 代码块3012Animal dog = new Dog();Cat cat = (Cat)dog; 上面的代码在语法里不会有问题，但实际上运行时便会报错，即类型转换错误。 到目前为止，有关问题1、2、3都说完了，继承和多态这一块确实很复杂，最好能结合本节流程图记下来这个规则，以后做分析时就不会乱了。 1.7.2：属性在继承关系下的访问规则上面只是说了方法的调用规则，却没有说属性，我们来做个试验： 现在让Cat类里也有一个叫age的属性： 代码块3112345678910111213141516public class Cat extends Animal { public int age = 12; //新增一个跟父类一样的属性，初始值为12 @Override public void cry() { System.out.println(this.age); //通过this关键词访问age System.out.println(super.age); //通过super关键词访问父类里的age System.out.println(\"喵喵喵~\"); } public void catchMice() { //扩展方法：捕鼠 System.out.println(\"我会捉老鼠\"); } } 然后让父类里的age访问权限变成public（详细看下方1.8，这里改成public的目的是为了让Cat可以直接访问这个属性），然后让其初始值为15，并且在其eat方法里使用this关键词访问age： 代码块321234567891011121314public class Animal { ...省略... public int age = 15; //动物们的年龄，初始值为15 public void eat() { System.out.println(this.age); //输出age System.out.println(this.name + \"正在吃东西\"); } ...省略...} 现在让我们来测试下： 代码块3312345public static void main(String[] args){ Animal cat = new Cat(); cat.cry(); cat.eat();} 输出如下： 代码块34123451215喵喵喵~15xx正在吃东西 🎑 结论：由运行结果我们可以知道，与方法的重写不同，类的属性本身该属于哪个类还是属于哪个类，记住这个规则。 好了，上面的这个试验有画蛇添足的嫌疑，因为age父类里就有，为什么Cat里还要加上一个同名的age呢？是的，真实设计类的时候没人会这样设计的，我们需要明确一个点，方法是行为，方法在继承关系里允许各种重写，但是属性作为一种没有任何行为方式的东西，你重复定义它的意义大吗？显然是不大的，而且极易跟父类混淆，所以我们说子类可以重写父类的方法，继承关系里，方法调用符合以对象所属类为参照就近调用的规则，子类还可以自己拓展父类里没有的方法，子类的对象还可以被父类引用变量接收，只是这样会影响子类对象的访问域（图7），子类还可以拓展父类里没有的属性，但不建议子类覆盖父类的属性，这种覆盖是没有意义的，通过上面的例子我们也发现了，同名属性在类继承链里是相互隔离的。 1.8：访问权限通过1.7，你应该知道了某种类与父类访问规则，但是我们在LV2-2的访问权限符的介绍里，介绍了访问权限修饰符以及被它们修饰了的资源的访问权限，也就是说理想状态下1.7的流程图都是成立的，但是你有没有发现，这些方法的权限修饰符都是public的？是的，public表示资源公开，比如Animal类里的age和name两个属性都是private类型的，子类可以通过super直接访问这俩属性吗？显然是不行的，必须得通过public的setName和setAge俩方法完成对name和age的赋值，这就是访问权限符的意义了，至于怎么区分能不能访问呢？我们再把那张表里有关修饰符访问等级的表贴来： 只需要记住，如果俩类为继承关系，则protected关键词无视包目录，均可访问，其余规则全部跟普通类一样，由于之前讲过权限修饰符，因此这里不再做详细介绍。 1.9：继承和多态的好处通过前面对继承和多态的了解，我们大体上对这俩特性有了一个认识，首先继承是为了解决什么问题的？ 回归到我们的例子上，Animal里定义了符合动物特性一些行为和属性，Cat是一种动物，符合Animal定义的行为和属性，它们俩建立起父子关系后，Cat就可以直接使用父类里定义的属性和方法了，这样像eat这种方法，Cat和Dog都不需要自己定义了，这样会让我们的代码看起来很简洁，而且同样的代码逻辑块，不需要重复定义多次，有需要可以重写，子类还可以自定义自己的行为方法。 多态呢？它有什么好处？ 多态就是指使用一个父类引用变量，可以接收子类对象并允许触发它内部的方法，参见图1，那么这样的一种特性的好处究竟是什么呢？我们本节先不会涉及，之后讲JDBC时可以提一嘴，现在你要做的是，牢记java的这种机制。 1.10：抽象类了解完了继承和多态，我们再来了解一种特殊的类：抽象类 什么是抽象类？让我们来改造下Animal方法，不知道你有没有发现，由Animal派生出来的子类Cat和Dog都重写了cry方法，说明大部分时候，Animal里的cry方法都不会主动生效，因为意义不大，这时我们认为cry方法在绝大多数情况下都是需要被重写的，像这种方法，我们就可以把它定义成一个抽象方法，包含抽象方法的类被称作抽象类，普通的类是不能有抽象方法的，因此想要cry变成抽象方法，首先要把Animal变成抽象类： 代码块351234567891011121314151617181920212223//利用abstract修饰的class，被称为抽象类public abstract class Animal { private String name = \"xx\"; //动物学名，动物们都有学名，这里默认为xx，因为动物类是用来描述动物的，动物有学名，但你并不知道是那种动物，name只能未知 public int age = 15; //动物们的年龄 //被abstract修饰的方法被称为抽象方法 public abstract void cry(); //动物会叫，但每个动物的叫声都完全不一样，有的甚至不会叫，那\"叫\"就应该是一个标准，作为动物的标准，动物本身只要定义好这个标准就好了，其余交给具体的动物（子类）去实现 public void eat() { //负责进食的方法 System.out.println(this.name + \"正在进食\"); } public void setName(String name) { this.name = name; } public void setAge(int age) { this.age = age; } } 我们可以看到，抽象方法没有任何实现，所以称它为一个标准，所有直系子类都必须实现这个标准，这时Cat类继承了Animal类，如果不实现cry方法，语法上就会报错： 现在我们只能让Cat类实现这个抽象方法： 这就相当于强制让子类去重写cry方法，事实上确实需要每个动物都实现自己的cry方法，但是像eat这种方法就没有必要这样做，因为大部分动物都需要进食且都是用嘴进食的，大同小异，如果真的存在不一样的吃东西的方法，那么就跟之前一样重写就好了，但大部分动物都不需要重写eat方法，父类Animal里的已经够用了，这是在告诉你什么时候需要搞抽象方法，什么时候不需要。 现在你知道了，有的方法需要被抽象，那就定义抽象方法，因为存在抽象方法，所以类必须被声明为抽象类（abstract关键词）。 抽象类的特点： 抽象类不能被实例化（也就是说不可以利用new关键词实例化一个抽象类） 由于1的特性，我们认为，抽象类是为继承而生的，如果你定义了一个抽象类，而它没有任何子类，那么这个抽象类就没有任何意义 抽象类不一定有抽象方法，但有抽象方法的类一定是抽象类 抽象类可以继承抽象类，当继承了抽象类时，子类抽象类也可以不实现其父类的抽象方法，全部交给非抽象的子类去做 抽象类也可以继承普通类，也可以选择重写普通类的方法，更神奇的是，它还可以让通过重写，让普通类的方法变成一个抽象方法 抽象类虽然不可以被实例化，但是它仍然存在构造器（这是句废话，抽象类是天生的父类，父类构造器在子类被实例化的时候会自动调用一次，所以作为天生的父类，它必有构造器） 🌵 思考： 现在Animal定义如下： Cat类是它的子类，定义如下： 结合前面对继承和多态以及本部分对抽象类的理解，请问如下测试代码输出结果是什么？ 1.11：万类の父-Object不知道你有没有发现，任何对象都自动有这些方法： 这是因为类被定义出来，就隐藏继承了一个父类，名叫Object，这是java自带的类，不需要显式的extends出来。 二、封装封装充满我们代码，还是以前面的例子为准，我们认为对一类操作使用一个方法定义来圈起来，就是封装，比如eat、cry等方法，例子里面很简单，只打印了一句话，但实际开发中，一个方法可能会完成很复杂的操作，这里拿之前一个例子来说明问题： 代码块36123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public class Cat { public int gender; protected int age; int color; private String name; public Cat(int gender, int age, int color, String name) { this.gender = gender; this.age = age; this.color = color; this.name = name; } public void cry(Cat cat) { if (this.tooOld()) { //通过tooOld来判断当前这只猫是否年龄太大了 System.out.println(\"叫不动了...\"); } else { System.out.println(\"喵喵喵~\"); } } public void eat() { System.out.println(\"要恰饭的嘛\"); } protected void emm() { if (this.tooOld()) { //通过tooOld来判断当前这只猫是否年龄太大了 System.out.println(\"骚不动了...\"); } else { System.out.println(\"猪肉卷和千层面不存在二选一，我全都要！\"); } } void run() { if (tooOld()) { //通过tooOld来判断当前这只猫是否年龄太大了 System.out.println(\"跑不动了...\"); } else if (age &gt; 10) { System.out.println(\"奔跑速度：10km/h\"); } else { System.out.println(\"奔跑速度：15km/h\"); } } ...省略... //被拆出来的逻辑块，因为这段逻辑会出现在多个方法里，因此单拆出来封装成一个方法，这种方法建议设置成private权限的，因为仅限本类内调用 private boolean tooOld() { return this.age &gt; 15; //经过本层封装，只要需要这段逻辑的地方，直接调用下这个方法即可，本例太简单，因为只做了age大小判断，实际开发里可以把更复杂的且重复度过高的代码块像这样拆出来，封装成一个方法提供服务 }} 通过本例，我们知道了封装的目的是减少代码的重复度，提高复用率，这点是不是跟继承也很像？继承的意义之一也是为了解决代码重复度的。 封装仅仅只是为了解决重复度吗？并不是，通过之前的Animal的例子，我们发现像age、name这种属性的赋值操作也被封装成了一个个方法（setName和setAge），而Animal里的name和age全是private的，这是为什么呢？为什么不让age和name变成protected或public这种权限呢？让别的类直接赋值不是更好？为啥还要多此一举搞俩专门赋值的方法？ 这就体现了封装的另外一层意义：隔离 现在我们试着将Animal里的age变成public的，来看看下面： 代码块3712Animal animal = new Animal();animal.age = 10000; //由于Animal的age变成public公开了，因此在外界可以随意对其赋值 但是Animal这时不干了，你见过能活10000岁的生物吗？？因此Animal把自己的age封了起来，首先把访问权限变成private，然后提供一个setAge方法用来给age赋值，setAge就是Animal对外封装的一个专门给age赋值的方法，这个时候Animal就由之前的被动状态变成主动状态了，如果你不想让别人设置的age太过分，就可以在setAge里做限制，反正是自己的方法嘛，还不是想怎么设计怎么设计，让我们来改一下Animal里setAge的逻辑： 代码块381234567public void setAge(int age) { if (age &gt; 10000) { //当传入的age超过1w时，输出这句话反问对方 System.out.println(\"这种生物你给我找来个康康？\"); } else { //否则的话就给自己的age赋值 this.age = age; }} 看看，这样age的赋值就完全被隔离进本类内了，外界进行赋值时就不敢那么放肆了~ 封装的意义： 封装的意义在于保护或者防止代码（数据）被我们无意中破坏。 保护成员属性，不让类以外的程序直接访问和修改； 隐藏方法细节 这也是为啥我们在定义一个类的时候，属性经常被设置成private访问级别的，然后通过定义赋值方法来让外界给自己的属性赋值。 关于对象封装的原则: 内聚：内聚是指一个模块内部各个部分之间的关联程度 耦合：耦合指各个模块之前的关联程度 封装原则：隐藏对象的属性和实现细节，仅对外公开访问方法，并且控制访问级别 在面向对象方法中，用类来实现上面的要求。用类实现封装，用封装来实现高内聚，低耦合。 ↑上面的话先读一下，做多了东西自然就懂了，我们做开发时所作的封装、多态等，都是为了这一个目标：高内聚、低耦合来进行的。","link":"/2020/03/10/LV2-4%EF%BC%9A%E7%B1%BB%E7%9A%84%E7%89%B9%E6%80%A7%E3%80%81%E5%85%B3%E7%B3%BB/"},{"title":"LV2-3：变量的作用域","text":"通过对前面很多代码的了解，我们发现变量的声明通常发生在一个大括号内，被声明在某个大括号内的变量，它的作用域就对应的大括号内，不够直观？我们来通过一张图来说明一下： 如图所示，一个类定义代码，请仔细按照从上到下的顺序仔细阅读每一段说明。 每一个”{}”看做一个域，变量在哪个域做的声明，则在哪个域生效，上图已经按照颜色标明不同的域了。除此之外，变量的声明是在同域内不允许重名的，上述的loopTest的方法域里的age跟类域里的age重名，但是程序并不会报错，因为他俩作用域不一样。","link":"/2020/03/05/LV2-3%EF%BC%9A%E5%8F%98%E9%87%8F%E7%9A%84%E4%BD%9C%E7%94%A8%E5%9F%9F/"},{"title":"LV2-2：包的定义、类的基本组成、访问权限修饰符","text":"一、包包说白了就是一个个的文件夹，在java中，所有的类按照其特点，以包的形式进行做区分，包的目录层级命名规则一般是公司的网址倒过来，例如： 如上图所示，.java文件内就是存放的java代码，例如Cat.java文件里面放的就是上一节里那只猫的Cat类的代码，由于现在Cat类有了目录，所以它就需要在头部加上一句话： 代码块112345package com.bilibili.pet; //这句话位于代码首部，用于标识当前类的java文件在哪个目录下public class Cat { //代码省略..} 因为Cat有了相对目录，这里就需要加上package来标记自己所在的目录层级，否则报错。 再回到图1中，我们看下分包的目的是什么，从图1可以看到，基础包为com.bilibili（这里提一下，java代码里的包目录引用都是以“.”做分隔的，而不是斜杠），然后基础包下细分了俩包，一个是pet（宠物），一个是human（人类），这俩包下分别有两个java文件，pet下放的是Cat和Dog，而human下放的是Boy和Girl，所以其实包就是用来给类分类的（套娃警告），这里可能会晕，因为之前说过，类是用来描述一类事物的单元，那么既然这样，为啥还需要用目录再分割一层做分类呢？这其实是一个分类大小方向的划分，例如，粗略划分人类包下的类可以包含男生、女生、学生、老师，但是像猫狗明显和人类不是一回事，这时候就需要用一种更高层级的东西来做隔离，包就这样产生了，当然这个是没有绝对标准的，只要你喜欢，可以按照你的方式做分包，比如，你仍然可以认为，pet和human两个包可以合并为biology（生物），因为不管你是人类还是宠物，都是生物，所以对于包的划分，是个“哲学”问题，不过先不用担心，等写的多了，自然就会划分了，这一部分你只需要知道包是个什么东西，以及在存在包目录的情况下类要加package声明头即可。 二、如何使用代码定义一个类？2.1：类的定义和java文件上一节已经知道Cat类的代码了，只是没有细说一个类该如何定义，这次我们来讲下类是如何定义的，以及它和java文件是什么关系。 首先，新建一个叫Cat.java的文件，然后在这个文件里写入以下代码： 代码块2123public class Cat { } 保存，这样一个Cat类就就建好了，诶？？就这？ 是的，我们来拆解和分析一下这段代码： public是一种权限修饰符，可以控制被修饰内容的访问权限，在介绍完类的所有概念之前，不会涉及它的概念，这里只需要记住，一个java文件允许定义多个class，但只允许有一个public类，而且这个public的类名字一定要和这个java文件的命名一致，比如例子里的public的Cat类就定义在Cat.java里，我们还可以在一个java文件里加多个class，但是public的只能有一个。 代码块2里只是定义了一个空类，通过上一节内容，我们可以知道一个类所包含的内容大概有两大类，一个是属性，一个是方法（也叫函数，我们后面统称方法），所有的类都符合这个结构，类可以不定义任何属性，也可以不定义任何方法，这个没有硬性要求。 2.2：类的属性和方法上一节介绍过，这一节细分析下。 通过上一节的Cat我们会发现类的属性其实也是各种变量组成的，这种变量在类定义里叫做类的属性（之前也说过，类本身是一种信息模板，而类产生的对象才是我们需要访问的实际数据），而在类产生的对象里叫做对象的成员变量，成员变量在对象内部是可以在任意地方随意访问的，比如Cat类里所有方法都是可以直接访问到对象内的成员变量的。成员变量的作用域就是整个类对象，如果别的地方需要用到它们，则需要通过其对应的引用变量进行访问。 我们现在再把Cat类改造下： 代码块31234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677//猫类的定义public class Cat { //性别：0雌，1雄 private int gender; //年龄 private int age; //毛色：1黑，2白 private int color; //昵称 private String name; //构造方法，用于构造一个类对象 public Cat(int gender, int age, int color, String name) { //通过传入的参数，初始化自己对象内的每一个成员变量 this.gender = gender; this.age = age; this.color = color; this.name = name; } public void cry() { if (this.tooOld()) { //通过本类的私有方法来判断当前这只猫是否年龄太大了 System.out.println(\"叫不动了...\"); } else { System.out.println(\"喵喵喵~\"); } } public void eat() { System.out.println(\"要恰饭的嘛\"); } public void emm() { if (this.tooOld()) { System.out.println(\"骚不动了...\"); } else { System.out.println(\"猪肉卷和千层面不存在二选一，我全都要！\"); } } public void run() { if (tooOld()) { System.out.println(\"跑不动了...\"); } else if (age &gt; 10) { System.out.println(\"奔跑速度：10km/h\"); } else { System.out.println(\"奔跑速度：15km/h\"); } } //这个方法用来介绍自己 public String intro() { String gender = this.gender == 0 ? \"母\" : \"公\"; String color = this.color == 1 ? \"黑色\" : \"白色\"; //将这些属性值拼成一句间接返回出去 String result = \"我是一只名叫\" + name + \"有着\" + color + \"皮毛的\" + age + \"岁\" + gender + \"猫\"; return result; } //给猫加年龄的方法 public void plusAge(int plusCount) { this.age += plusCount; } //返回猫当前的年龄 public int getCurrentAge() { return this.age; } //private修饰的私有方法，私有方法仅允许本类内访问 private boolean tooOld() { return this.age &gt; 15; //如果年龄超过15岁，则返回true }} 我们改造了之前的旧方法，让猫的行为根据自己的年龄的不同再次发生改变，此外新增了intro方法用来返回出去自己的简介，plusAge用来增加猫的年龄，getCurrentAge用来返回猫当前的年龄，tooOld使用了private修饰，跟public一样属于访问权限，加了这个访问权限意味着在别的地方通过对象无法调用该方法，只能由对象内部触发调用，后面会着重介绍访问权限修饰符。此外，方法又分为两种，一种是有返回值的，比如getCurrentAge，一种是没有返回值的，比如plusAge，它们的区分方式在于访问权限修饰符public后面是void还是其它类型，如果是void则说明此方法没有返回值，否则就是返回值对应的类型（如int、long，返回结果为引用变量的话，则为对应的类，例如String、Cat等）。 现在拿着这个Cat类来做个试验，我们之前有说过，访问对象里面的方法或者属性时，需要通过引用变量来进行访问，引用变量又是通过初始化类对象来的，初始化类对象之前说过，通过new后面加上构造方法来进行构造一个类对象： 代码块41234567891011121314151617181920212223242526272829303132public static void main(String[] args) { //造出两只猫，现在我们有了两个引用变量，可以利用它们访问对象实体了 Cat cat1 = new Cat(0, 1, 0, \"咲川\"); Cat cat2 = new Cat(1, 5, 1, \"龟田志斌\"); String cat1Into = cat1.intro(); //这个方法会返回对应猫咪的简介，这里用一个变量接收 String cat2Into = cat2.intro(); System.out.println(cat1Into); //打印第一只猫的简介 System.out.println(cat2Into); //打印第二只猫的简介 System.out.println(\"------------------------------------\"); //为方便观察，这里用分割线隔开 cat1.emm(); //由于这里cat1对应的猫对象里的age为1，因此这里仍然可以打印迷惑语录 cat2.emm(); //由于这里cat2对应的猫对象里的age为5，因此这里仍然可以打印迷惑语录 System.out.println(\"------------------------------------\"); cat1.plusAge(15); //让川桑老15岁，由于plusAge的逻辑为原age加上传入的年龄增量，所以现在的咲川应为16岁 cat2.plusAge(6); //让龟田老6岁，由于plusAge的逻辑为原age加上传入的年龄增量，现在的龟田志斌应为11岁 //通过getCurrentAge把二位的年龄拿到 int cat1Age = cat1.getCurrentAge(); int cat2Age = cat2.getCurrentAge(); System.out.println(\"咲川当前年龄为：\" + cat1Age + \" \" + \"龟田志斌当前年龄为：\" + cat2Age); //这里再次打印出二位当前的年龄值 System.out.println(\"------------------------------------\"); //再触发一下它们的迷惑行为 cat1.emm(); //由于咲川现在为16岁，符合tooOld为true，因此这里打印骚不动了... cat2.emm(); //由于龟田志斌现在为11岁，符合tooOld为false，因此这里仍然打印迷惑语录} 上方代码块打印： 代码块512345678910我是一只名叫咲川有着白色皮毛的1岁母猫我是一只名叫龟田志斌有着黑色皮毛的5岁公猫------------------------------------猪肉卷和千层面不存在二选一，我全都要！猪肉卷和千层面不存在二选一，我全都要！------------------------------------咲川当前年龄为：16 龟田志斌当前年龄为：11------------------------------------骚不动了...猪肉卷和千层面不存在二选一，我全都要！ 通过这个例子我们会发现： 类在产生对象以后，可以利用引用变量来操纵它，具体引用变量可以调用对象的方法，甚至可以直接访问它的属性（能不能访问具体看访问权限修饰符，public或private等，后面讲） 类在产生对象后，每个对象都持有自己的属性和方法，互不干扰 对象内的方法可以访问对象内的任意成员变量以及其他方法（无视访问权限修饰符） 对象内某些方法可以改变成员变量的值，比如plusAge，它会重新给自己的成员变量赋值，而有些方法则不会，例如intro，它仅仅是拼接自己内部的成员变量 2.3：构造器构造器是类里的一个特殊的方法，注意，它也是一个方法，构造器特殊的原因在于它有且仅被触发一次，就是在类构造类对象的时候，一般你定义了一个类如果不写构造器，那就会默认一个构造器，例如代码块2里，虽然是个空类，但它有一个隐藏的构造器，即： 代码块6123456public class Cat { public Cat() { //不定义构造器，则默认有这样一个构造器，由于它并没有什么实际意义（因为{}内没有任何实质性的逻辑代码），所以java语法里允许不写 }} 而在代码块4里的构造器没有省略，因为它是有实际意义的，它的意义在于给成员变量赋值： 代码块712345678//构造方法，用于构造一个类对象public Cat(int gender, int age, int color, String name) { //通过传入的参数，初始化自己对象内的每一个成员变量 this.gender = gender; this.age = age; this.color = color; this.name = name;} 通过上面的代码我们可以发现构造器的特点，名称和类名一致，没有标记返回结果的修饰符（void关键词或其他返回类型），且在构造一个对象时必须要通过调用构造方法完成。 三、访问权限修饰符3.1：访问权限符介绍在前面所介绍的类定义、类属性定义、类方法的定义的最前端，都会有类似public、private这种修饰符，它们的意义和作用是什么呢？ 首先我们来介绍一下它们，它们是用来控制访问权限的修饰符，是编译器做访问语法检查时所做的判断依据，一般放到代码首部，可以用来修饰类、类的属性变量、类的方法，用来约定它们的访问权限。 关于表里父子类相关内容需要了解过继承后再去看 ps：这个访问权限，仅仅是根据类定义的上下文来判定的。 单看表和描述是很难发现它们的区别的，现在我们来举个具体的例子，我们现在再来改造下Cat类（修改了一些内容的访问权限，顺带精简了下代码）： 代码块812345678910111213141516171819202122232425262728293031323334353637383940414243public class Cat { public int gender; //gender字段改为public访问权限 protected int age; //age字段改为protected访问权限 int color; //color字段改成默认（default关键字可以省略不写）的访问权限 private String name; //name仍然是private访问权限 public Cat(int gender, int age, int color, String name) { this.gender = gender; this.age = age; this.color = color; this.name = name; } public void eat() { //eat方法仍为public访问权限 System.out.println(\"要恰饭的嘛\"); } protected void emm() { //emm方法改为protected访问权限 if (this.tooOld()) { System.out.println(\"骚不动了...\"); } else { System.out.println(\"猪肉卷和千层面不存在二选一，我全都要！\"); } } void run() { //run方法改为默认访问权限 if (tooOld()) { System.out.println(\"跑不动了...\"); } else if (age &gt; 10) { System.out.println(\"奔跑速度：10km/h\"); } else { System.out.println(\"奔跑速度：15km/h\"); } } private boolean tooOld() { //tooOld方法仍为private访问权限 return this.age &gt; 15; //如果年龄超过15岁，则返回true }} 3.2：举例说明：同一个类ok，现在让我们基于改造好的Cat类，从第1列的内容试起： 我们在写代码的时候也发现了，同一个类内部的所有属性、方法无视其访问权限，直接获取&amp;访问即可，现在再来试试这样： 由上图可以看到，同一个类定义内，如果存在另外一个相同类产生的对象，仍然可以通过其引用变量无视访问权限进行访问，因为它们属于同一个类，分析如下： 首先cry是Cat类的方法，接收的参数是Cat类产生的引用变量，因此在cry内可以无视访问权限，直接通过cat这个变量访问到所有的内容。 3.3：举例说明：同一个包我们现在再在同一个包创建一个Test类，在其main方法里做实验： 这是符合预期的，Test和Cat不属于同一个类定义，但它们在同一个包内，因此除了private的内容，均可访问。 3.4：举例说明：同包/不同包的父子类后面讲继承的时候会讲。 3.5：举例说明：不同包的类跟3.3相反，我们现在新建一个跟Cat不同包的Test类，然后用其main方法做实验： 这是符合预期的，Test和Cat不属于同一个类定义，且它们不在同一个包内，因此除了public的内容，均不可访问。","link":"/2020/03/05/LV2-2%EF%BC%9A%E5%8C%85%E7%9A%84%E5%AE%9A%E4%B9%89%E3%80%81%E7%B1%BB%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%BB%84%E6%88%90%E3%80%81%E8%AE%BF%E9%97%AE%E6%9D%83%E9%99%90%E4%BF%AE%E9%A5%B0%E7%AC%A6/"},{"title":"LV2-1：类&引用变量的简单介绍","text":"一、类的简单介绍、什么是引用变量前面讲述完基本变量类型，现在说下引用变量类型，这里先简单介绍下java里非常重要的一个概念：类 1.1：什么是类？java是一个面向对象语言，那么什么是对象？类又和对象有什么关系呢？ 之前了解了所有的基础类型数据，我们可以用它们来搞个事情，现在，让我们用这些基本类型造只猫。 那么，我们现在需要提炼出来猫所具备的一些属性，比如，每只猫都有自己的生理性别，那么性别就可以是一个属性，每只猫可能还会有一个昵称，每只猫还会有它的年龄，当然，随便什么属性都可以，自己喜欢就好，都可以加进你的设计里，前提是这些属性是客观存在的事实。除了一些属性，猫还会有一些行为，比如吃饭、奔跑、叫、无法归类的迷惑行为等，把属性加上这些行为，一个猫类就被我们抽象出来了： 如上图所示，抽象后的数据结构，被我们称作一个类，类在java中无处不在，类内按照特性分为了两类，上方红色部分，我们称它为类的属性，属性主要用来描述一些客观存在的事实，比如猫确实有年龄，猫确实有生理性别等，而猫会叫，会跑等这些含主观意识构成的特性，我们管它叫类的方法，类方法可以用来描述这个类里所具备的具体功能，比如例子里的猫，具备吃饭的功能，具备跑的功能等，而很多时候，这些功能特性还会依赖自己的属性来决定功能的运行方式，比如猫在奔跑的时候可能会依赖自己的年龄，年龄大的猫奔跑速度可能赶不上年龄小的猫，这个时候奔跑这个功能便会受猫的年龄属性的影响。 这里简单了解下方法在java中的定义方式：类里的方法又叫函数（function），它的基本定义结构如下： 上面是一个简单的求两个数之和的方法（函数），它的逻辑是，计算输入的两个参数的和，并将其返回出去。 1.2：什么是对象？通过对1.1的理解，我们知道了类的基本概念，那么类就跟它的名字一样，属于对一类事物的描述，比如猫，猫是一个种群，因为猫都具备图1中的特性，所以猫这个种群可以用Cat类来描述，但是既然是种群，肯定存在个体差异，比如，你家的猫是母猫，他家的猫是公猫，你家的是白猫，他家的是黑猫，这种个体的差异没有越过类的描述，不管是公还是母，本质上都是一种生理性别（gender），那么如何解决这种细微的个体差异性呢？ 这时对象就出来了，我们来看下下面这个图： 如上图所示，利用Cat这个类的信息，对其内部属性赋值，就可以造出各种各样“符合类规则”的猫了，产生的这只猫，就叫做类的对象，每个类都可以产生许多许多个对象，这些对象符合类的基本定义，但是却各不相同。 1.3：利用java实现一个类、认识并理解引用变量 本篇文章旨在简单介绍java类是什么，重点介绍引用变量，主要结合图片理解，可以对类可以有个基本的认识以及更深层次的理解引用变量。 通过前面所有内容的了解，我们利用java代码来实现下Cat： 代码块112345678910111213141516171819202122232425262728293031323334353637383940//猫类的定义public class Cat { //性别：0雌，1雄 private int gender; //年龄 private int age; //毛色：1黑，2白 private int color; //昵称 private char name; //构造器：关键方法，用来制造对象，传入的这些属性值得以赋值到对象内部 public Cat(int gender, int age, int color, char name) { this.gender = gender; this.age = age; this.color = color; this.name = name; } //叫 public void cry() { //猫叫声的实现 System.out.println(\"喵喵喵~\"); } //吃 public void eat() { System.out.println(\"要恰饭的嘛\"); } //无法归类的迷惑行为 public void emm() { System.out.println(\"猪肉卷和千层面不存在二选一，我全都要！\"); } public void run() { if (age &gt; 10) { //假如奔跑速度跟年龄有关，年龄超过10岁，则速度变慢 System.out.println(\"奔跑速度：10km/h\"); } else { System.out.println(\"奔跑速度：15km/h\"); } }} 经过上述代码，我们已经完成了1.1中，有关猫类的抽象。 接下来，我们来完成1.2中的造对象过程，现在新造一个Test类用于测试（注：java所有的程序，都是以一个main方法开始的，这里通过Test类里的main方法来开始测试）： 代码块21234567891011121314public class Test { public static void main(String[] args) { //下面都是基本类型 int gender = 1; int age = 5; int color = 1; char name = 'A'; //这里将上方赋好值的数据，送入构造器，然后构造器构造出来对应的对象 Cat cat = new Cat(gender, age, color, name); cat.run(); //利用这个对象，调用它自身的run方法 }} 例子中通过new构造器来构造一个具体的对象，命名为cat，这个cat是一个变量，这种非基本类型的变量，叫做引用变量，引用变量相比基本类型，较为复杂，它内部其实存储的是一段地址，这段地址指向了具体的对象体，例子里就是通过构造器构造出来的对象本身，而cat.run()就是在利用这个引用变量cat访问具体的对象的run方法，通过代码块1，可以知道在age&lt;=10的时候，输出应该是15km/h，事实上运行一下这段代码确实是这个结果： 1奔跑速度：15km/h 目前做简单了解就好，后面会仔细讲，这个过程可以用下方流程图简单说明一下： 需要注意的是，与基本变量不同，引用变量不存在默认值，如果一个引用变量不进行赋值操作，那么它的“默认值”是null，即引用变量并没有指向任何对象实体，如果之前有听说过java鼎鼎大名的空指针异常（或称npe异常），那么空指针异常在多数情况下都是因为使用了一个值为null的引用变量去访问某个对象本体导致的。 通过上述的例子会发现，猫的名字是char类型，但是char类型仅能保存一个字符，如果我的猫名字叫“加菲”，对于Cat类，就是一件很棘手的事情了，因为“加菲”是由两个汉字字符组成的词语，那么这时候有没有一种类型可以用来接收它呢？ 字符串类型String就是用来做这个的，LV1-2里说过，它并非基本类型，它的声明方式如下： 代码块3123String a; //声明名称为a的String类型变量（由于其非基本类型，因此a属于引用变量，不赋值的情况下默认值为null）String b = \"加菲\"; //声明名称为b的String类型变量（初始值为：加菲）String c = new String(\"加菲\"); //不建议的写法，声明名称为c的String类型变量（初始值为：加菲） 这样，将Cat类改造下，将其内部的name改成String类型： 代码块41234567891011121314public class Cat { private int gender; private int age; private int color; private String name; //改成String类型 public Cat(int gender, int age, int color, String name) { //接收name参数时使用String类型接收 this.gender = gender; this.age = age; this.color = color; this.name = name; } ...方法省略，参考代码块1...} 此时的main方法改成如下： 代码块51234567int gender = 1;int age = 5;int color = 1;String name = \"加菲\"; //声明一个叫name的字符串，初始化值为”加菲“ Cat cat = new Cat(gender, age, color, name);cat.run(); 由于name也变成了一个引用变量，所以图4就变成了下方的流程： String为java自带的原生类，而Cat是我们自己定义的类，本质上都是类，未来编写java程序时，会用到很多类似String这样的java现成的类，自己实现一些业务逻辑时，需要创造自己的类，自己的类里也可以使用别的类，类似上述例子中，Cat类就是用了String类，只要是类产生的对象，对应它的变量一定是一个引用变量，引用变量永远是一段访问实际对象时的内存地址，如果一个对象在内存里失去了变量引用，则认为该对象废弃，这时JVM就会将其回收，这就是java的垃圾回收机制，这里不需要知道太多，反正后面都会讲，先了解下即可。","link":"/2020/03/02/LV2-1%EF%BC%9A%E7%B1%BB&%E5%BC%95%E7%94%A8%E5%8F%98%E9%87%8F%E7%9A%84%E7%AE%80%E5%8D%95%E4%BB%8B%E7%BB%8D/"},{"title":"LV1-4：条件语句&循环语句","text":"通过前面的介绍，我们已经了解了变量和运算符，通过运算符可以将多个变量进行不同的运算，这已经可以做出一个计算器了，可是一段具备各种逻辑的程序，仅靠简单的运算是无法完成的，于是，我们便有了各种条件语句和循环语句，用来组合运算逻辑，从而实现更为复杂的业务逻辑。 .heimu { color: #000; background-color: #000; } .heimu:hover { color: #fff; } 一、条件语句1.1：if、else if、else先来看一个例子： 代码块112345678910111213141516171819202122public static void main(String[] args) { int a = 1; if (a == 1) { //这里放a等于1时的逻辑代码 } else if (a == 2) { //这里放a等于2时的逻辑代码 } else if (a == 3) { //这里放a等于3的逻辑代码 } else { //如果a不满足上面任意一个条件，则走这里的逻辑 }} 这就是if条件语句的基本用法，它一定是以if开头的，如果除了if头的条件不满足，还需要别的判断条件，则使用else if，如果没有任何满足的条件时需要做进一步处理，则使用else，条件语句括号内的语句，一定是一个关系运算式，结果为true或false，结果为true，则走对应的代码块，它的运行模式如下： 由图知：if语句从上到下开始判断，一旦碰到条件满足的，就执行满足条件的代码块，其余代码块不再运行，思考，下方哪个代码块会执行？ 代码块21234567891011121314public static void main(String[] args) { int a = 3; if (a == 1) { //代码块1 } else if (a == 2 || a == 3) { //代码块2 } else if (a == 3) { //代码块3 } else { //代码块4 }} 首先，第二个条件语句（a == 2 || a == 3）和（a == 3）都是true，那么究竟是执行代码块2还是执行代码块3呢？答案是代码块2，因为上面已经说过了，判断时只要遇到一个true，就直接执行对应的代码块了，其余的代码块都不会生效。 ok，看完上面的案例，再来说下if语句本身，if语句的else或者else if是可以不存在的，比如我现在仅做一个最基本的判断： 代码块31234567int a = 3; if (a == 1) { //代码块1} //其它代码 这段代码的意思是说，只要a的值为1，就会执行代码块1，其它不需要做任何判断，同样的，if可以跟一个else使用： 代码块4123456789int a = 3; if (a == 1) { //代码块1} else { //代码块2} //其它代码 这段代码的意思是说，如果a的值为1，则运行代码块1，如果a的值不等于1，则运行代码块2. if也可以单独跟一个或多个else if一块使用： 代码块512345678910int a = 3; if (a == 1) { //代码块1} else if(a == 2) { //代码块2} //其它代码 这段代码的意思是说，如果a的值为1，则运行代码块1，如果a的值为2，则运行代码块2。 1.2：switch、case语法如下： 代码块6123456789101112131415161718public static void main(String[] args) { int a = 2; switch (a){ case 1: //代码块1 break; case 2: //代码块2 break; case 3: //代码块3 break; default: //代码块4 }} 语法规范：switch括号内的类型只能是基本类型、枚举类、String，一个switch里可以包含多个case，default可以写也可以不写。 它是用来做什么的呢？ 其实它有些像if语句，用来匹配符合规则的代码块，以代码块6为例，它的执行流程如下： 可以看到，switch也是自上到下根据条件判断出需要执行的代码块，那么这个break又是什么意思呢？根据图2可以发现它是用来终止程序用的，如果少了break会发生什么？如下图，现在我们把程序里的break去掉： 可以看到，如果在case2的代码里没有做程序终止，则从case2开始往下的每一项都会触发执行。 那么，default这一项是干嘛的？这个其实跟if里的else类似，就是switch内的值在case内没有任何匹配项的时候，默认触发的代码块，此外switch是允许不定义此项的。 ok，了解了switch的执行流程，那么请思考一下下方这个结果应该输出多少？ 代码块71234567891011121314int a = 2; switch (a){ case 1: System.out.println(1); break; case 2: System.out.println(2); case 3: System.out.println(3); break; default: System.out.println(\"default\");} 答案：输出2和3 那么下面这个输出什么呢？ 代码块8123456789101112131415int a = 233; //注意这里a的值变了 switch (a){ case 1: System.out.println(1); break; case 2: System.out.println(2); break; case 3: System.out.println(3); break; default: System.out.println(\"default\");} 答案：default 二、循环语句2.1：for循环语句语法如下： 代码块91234567public static void main(String[] args) { for (int i = 0; i &lt; 10; i++) { System.out.println(i + \"哔哩哔哩干杯\"); } } 上面这个程序会输出10个字符串，即：0哔哩哔哩干杯 ~ 9哔哩哔哩干杯，for循环是如何运行的？它是以什么为依据运行10遍的？下面我们来拆解下它的语法： 如图4，根据对括号内的了解，我们来梳理下代码块9里的执行过程： 请仔细阅读图5中的流程，这是推动for循环执行的流程。 学完了if语句和for循环，我们来写一个程序，程序要求：输出0~9里的奇数和偶数，最后将它们按照空格分开打印出来： 代码块101234567891011121314151617public static void main(String[] args) { String odd = \"\"; //声明存放奇数数据的字符串 String even = \"\"; //声明存放偶数数据的字符串 for (int i = 0; i &lt; 10; i++) { //循环运行，每次i+1，一共循环10次，i的赋值范围为0~9 if (i % 2 == 0) { even += i + \" \"; //如果被2整除，则说明是偶数，加空格后拼在even后面 } else { odd += i + \" \"; //如果不能被2整除，则说明是奇数，加空格后拼在odd后面 } } System.out.println(\"0~9中的奇数为：\" + odd); System.out.println(\"0~9中的偶数为：\" + even); } 输出结果为： 代码块11120~9中的奇数为：1 3 5 7 90~9中的偶数为：0 2 4 6 8 2.2：while、do whilewhile循环的语法如下： 代码块1212345678910public static void main(String[] args) { int i=0; while (i&lt;10){ System.out.println(i + \"哔哩哔哩干杯\"); i++; } } 这个跟代码块9输出一样，也是：0哔哩哔哩干杯 ~ 9哔哩哔哩干杯，先来说下while这个单词本身的意思，它的意思是“当….的时候”，所以while(i&lt;10)这句话就很好理解了，就是当i的值小于10的时候，运行while循环体里的代码，相比for循环，while理解起来相对简单，它的运行流程图不再画。 do while跟while类似，将代码块12里的代码利用do while改造一下： 代码块1312345678910public static void main(String[] args) { int i = 0; do { System.out.println(i + \"哔哩哔哩干杯\"); i++; } while (i &lt; 10); } 输出结果跟while一致，那你可能要问了，while和do while有啥区别？ 来看下它们的执行流程图： 这就是它们的区别，while上去就会判断条件是否满足，从而决定是否走下去，而do while往往上去就会触发一次循环体，然后才做判断决定是否再次触发循环体。 按照代码块13的逻辑来说，while和do while确实没差别，因为i一开始确实小于10，所以第一次的时候无论如何都会触发一次循环体里的逻辑，是否先判断条件已经无所谓了，我们来举一个不一样的例子： 代码块1412345678910public static void main(String[] args) { int i = 10; do { i--; System.out.println(i + \"哔哩哔哩干杯\"); } while (i &gt; 0 &amp;&amp; i &lt; 10); } 这段代码输出为：9哔哩哔哩干杯 ~ 0哔哩哔哩干杯 我们现在用while改造一下： 代码块1512345678910public static void main(String[] args) { int i = 10; while (i &gt; 0 &amp;&amp; i &lt; 10) { i--; System.out.println(i + \"哔哩哔哩干杯\"); } } 这段代码啥也不会输出，因为while会先判断循环条件是否成立，再决定走不走循环体，而需要i&lt;10成立的i–操作却在循环体内，因此代码块14成功执行，而代码块15未成功。 三、数组&amp;for增强3.1：数组是什么？我们前面讲基本类型变量时说过，设置一个变量的步骤是：①声明、②赋值、③使用，但是当时的变量都是我们一个个声明的，有没有一种方式可以一次性声明n个变量呢？数组就是这样一种结构，它允许一次性声明n个变量，并且支持逐个赋值，数组跟基本类型一样，也需要声明，声明方式为： 代码块1612int[] nums = new int[10]; //这种声明方式只是简单的定义了一个容量为10的int型数组，并未给内部的元素赋值int[] nums2 = {1, 222, 334, 566, 256}; //这种声明方式就是连带着初始值给每个元素都赋上了，这里是指从0~5的下标的元素值分别为：1, 222, 334, 566, 256 上述的代码的第一段就是声明了一个同时声明了10个int型变量的数组变量（套娃中…），也是常用的数组声明方式，那么之前也说过了，java除了基本变量之外，其余全都是引用变量，因此，数组类型也是一种引用变量，引用变量的原理性质的东西到讲类的时候会细说。 数组到底是什么？代码块16那样的做法会发生什么？ 先说下数组的结构，数组就是包含了多个元素的集合，例如代码中的nums，本身就一次性声明了10个int型元素，那么我们知道int型如果不给赋值的话，它的默认值是0，如何给数组里的元素赋值呢？这里就要引出一个叫做下标的东西，下标的含义就是指元素本身在数组中的位置，下标从0开始计算，nums在完成声明后的结构如下： 那么现在我如果要给下标5的数赋值为256，该如何操作呢？看代码： 代码块171nums[5] = 256; 利用数组变量，将下标值放进去，即认为操作的就是对应元素本身，因此利用等号赋值即可，赋完值的nums如下： 下标为5的int型数据元素已经被成功赋值了。 那么如何批量赋值呢？这样一个个的赋值太麻烦了，如果现在有一个size为10000的数组，这样一个个赋值会死人的~还记得本节所学的内容吗？循环啊，就不能利用循环赋值吗？比如for循环，完全可以把循环体里的i变量当成下标访问数组里的元素哇，现在我们利用循环操作，来给nums这个数组的每个元素都赋上256： 代码块18123456789public static void main(String[] args) { int[] nums = new int[10]; for (int i = 0; i &lt; 10; i++) { nums[i] = 256; //利用i当做nums的下标来访问它里面的元素 } } 这样nums里面的每一个元素的值就都是256了~ 再思考一个问题，这里因为我们事先知道了nums的元素个数是10个，下标是0~9，所以for循环里的i初始化为0，每次自增1，且自增至9终止循环，正好把nums里的每个元素赋值一遍，那如果我们事先不知道数组大小该怎么办？这时可以利用数组变量自带的方法length来获取数组的大小： 代码块19123for (int i = 0; i &lt; nums.length; i++) { //nums.length返回的就是数组大小，本例中就是10 nums[i] = 256;} 我们现在知道了根据数组的下标可以访问数组内具体的元素，通过循环体可以按照循环自增的i值快速访问整个数组（这个过程叫做“遍历”），那么现在我想要输出整个数组里每一个元素的值就变得很简单了，依然是for循环： 代码块20123for (int i = 0; i &lt; nums.length; i++) { System.out.println(nums[i]); //仍然是i当下标，将对应的数值输出即可} 3.2：for增强遍历数组其实除了上述的方式之外，还有一种增强方式，写法如下： 代码块21123for (int num : nums) { System.out.println(num);} 可以看到，for增强连下标都省去了，直接使用数组的基本类型接收都行，它是用来简化数组遍历的一种方式。 3.3：二维数组&amp;for嵌套上面介绍了数组，数组就是简单一批数据，那么二维数组又是啥？ 相比一维数组，二维数组跟它的名字一样，具有横竖两个维度的元素，它的结构如下： 可见，相比一维数组，二维数组定位到一个元素需要两个下标，那么给二维数组里的每一个元素赋值就不能用一个for循环来进行了，这时需要借助我们的for嵌套： 代码块221234567891011121314151617181920212223public static void main(String[] args) { int[][] a = new int[5][5]; //获取横向的长度 int lenX = a.length; //获取纵向的长度 int lenY = a[0].length; for (int i = 0; i &lt; lenX; i++) { for (int j = 0; j &lt; lenY; j++) { a[i][j] = 1; } } //遍历与打印 for (int i = 0; i &lt; lenX; i++) { for (int j = 0; j &lt; lenY; j++) { System.out.print(a[i][j]); } //每遍历完一遍横轴的元素，则换一次行 System.out.println(); }} ❓ 思考：请仔细阅读上面的代码，根据自己学过的for循环，想象一下for嵌套的执行顺序。","link":"/2020/03/02/LV1-4%EF%BC%9A%E6%9D%A1%E4%BB%B6%E8%AF%AD%E5%8F%A5&%E5%BE%AA%E7%8E%AF%E8%AF%AD%E5%8F%A5/"},{"title":"LV1-3：java中的运算符","text":".heimu { color: #000; background-color: #000; } .heimu:hover { color: #fff; } 一、运算符1.1：算术运算符算术运算符主要包含： 运算符 说明 示例 备注 + 加法 a + b 计算变量间的和 - 减法 a - b 计算变量间的差 * 乘法 a * b 计算变量间的积 / 除法 a / b 计算变量间的商 % 取余 a % b 计算变量间的余 ++ 自增1 a++ 或 ++a 等效于：a = a + 1; 或者 a += 1;(+=在下面赋值运算符会介绍)++放变量前面和后面，是有区别的，具体请参考第二部分的例3 – 自减1 a-- 或 --a 等效于：a = a - 1; 或者 a-=1;(-=在下面赋值运算符会介绍)跟++一样，也有前后之分。 表1 1.2：位运算符位运算符包含： 运算符 说明 示例 特性 &amp; 与运算 a &amp; b 1&amp;1=1、1&amp;0=0、0&amp;1=0、0&amp;0=0 | 或运算 a | b `1 ~ 取反 ~a ~1=0、~0=1 ^ 异或运算 a ^ b 1^1=0、1^0=1、0^1=1、0^0=0 &gt;&gt;&gt; 无符号右移 a &gt;&gt;&gt; b 参考注1 &gt;&gt; 右移运算 a &gt;&gt; b 参考注1 &lt;&lt; 左移运算 a &lt;&lt; b 参考注1 表2 ⚜️ 注1： 关于位运算的一切，请参考：JAVA有关位运算的全套梳理 1.3：赋值运算符赋值运算符主要包含： 运算符 说明 示例 备注 = 赋值 a = 1 让等号左边的值等于右边的值，这在之前的示例代码里已经体现过很多次了，简单一个变量声明的初始化值都需要用到赋值运算符：int a = 1; += 做加法操作后并且赋值给自身 a += 2 等效于：a = a + 2这种等式一般是从右往左执行，所以该表达式的意思是：先让a自身加上2，然后再把这个结果重新赋值给a。 -= 做减法操作后并且赋值给自身 a -= 2 同上，只不过换成减法 *= 做乘法操作后并且赋值给自身 a *= 2 同上，只不过换成乘法 /= 做除法操作后并且赋值给自身 a s/= 2 同上，只不过换成除法 %= 做取余运算并且赋值给自身 a %= 2 同上，只不过换成取余运算 &lt;&lt;= 做左移运算并且赋值给自身 a &lt;&lt;= 2 等同于：a = a &lt;&lt; 2 &gt;&gt;= 做右移运算并且赋值给自身 a &gt;&gt;= 2 等同于：a = a &gt;&gt; 2 &gt;&gt;&gt;= 做无符号左移运算并且赋值给自身 a &gt;&gt;&gt;= 2 等同于：a = a &gt;&gt;&gt;2 &amp;= 做与运算并且赋值给自身 a &amp;= 2 等同于：a = a &amp; 2 |= 做或运算并且赋值给自身 `a = 2` ^= 做异或运算并且赋值给自身 a ^= 2 等同于：a = a ^ 2 表3 1.4：关系运算符关系运算符用来运算两个值的大小、是否相等等关系，经过关系运算符运算后的两个值的结果一定是boolean类型的，结果只有真或假。 关系运算符主要包含： 运算符 说明 示例 备注 &gt; 大于 a &gt; b 参考注2 &gt;= 大于等于 a &gt;= b 同上，判断关系为大于等于 &lt;= 小于等于 a &lt;= b 同上，判断关系为小于等于 &lt; 小于 a &lt; b 同上，只不过判断关系为小于 == 等于 a == b 参考注3 != 不等于 a != b 参考注4 表4 ⚜️ 注2： 用于判断两个值大小，需要注意的是，比较大小的关系运算符无法用来比较大部分的引用变量，因此不存在cat1 &gt; cat2这种代码。 代码块1123char a = '我';int b = 25105;boolean d = a &gt; b; 上面结果为false ⚜️ 注3： 用来判断两个变量是否相等： 代码块21234567//声明a、b、c，并为它们赋初始值int a = 1;int b = 1;int c = 2;//运算符==返回的是布尔型数据，因为其判定结果只有真或假两种状态boolean d = a == b;boolean e = a == c; 如上代码，d的结果为true，e的结果为false 简单的基本类型，判等方式是很简单的，值一样，就相等，哪怕类型不一致： 代码块3123char a = '我';int b = 25105;boolean d = a == b; d的结果值依然是true，因为汉字“我”对应的Unicode码值就是25105。 通过上面两个例子，可以发现基本类型的判等很单纯，就是看数值是否相等而已。 ⚠️下面的内容建议看完LV3-1后再来阅读 既然==是用来为变量判等的，那它是否也能用来判断复杂的引用变量是否相等呢？ 现在让我们回忆一下LV3-1里的那只猫，通过那只猫的例子，我们知道了引用变量其实是一个保存了某个内存地址的变量，它并不是一个实际的对象，它只是保存了实际对象的地址，方便通过它在内存中访问到那块对象数据，我们还知道，当一个类利用其构造方法被new出来的时候，它就产生了一个对象，对象就会存在一块内存区域内，此时对象自然就有了对应的内存地址，而将该地址赋值给对应的引用变量，这样就可以通过此引用变量访问具体的对象内容了，如果还是不太熟悉这个过程，请再次翻看LV1-2中的图7。 现在我们仍然用那个猫类来说明问题： 代码块4123456789101112public static void main(String[] args) { int gender = 1; int age = 5; int color = 1; String name = \"加菲\"; Cat cat1 = new Cat(gender, age, color, name); Cat cat2 = new Cat(gender, age, color, name); boolean result = cat1 == cat2; System.out.println(result);} 由上面的代码可知，我们造出来了两只猫，它们的各项属性全部一致，那么现在请思考一下，引用变量cat1和引用变量cat2是否相等呢？上面的结果应该是true还是false？ 输出结果为false。 为什么呢？它们明明什么都一样！！回忆一下我们在上一节有关引用变量的解释，引用变量保存的是内存地址，而我们这个例子中，造出了两只猫，尽管它们啥都一致，但事实上，这种一致你可以理解成“巧合”，碰巧有两只名字都叫加菲的5岁黑色公猫， 所以实际上它们是不同的两个对象，它们在地球上所处的位置不可能穿透对方的身体重合存在，抽象到计算机层面，它们需要存储在不同的存储单元内，存储单元不同，自然存储地址不同，那么cat1和cat2又怎么会相等呢。 那么现在再思考下，如何让cat1和cat2相等？ 现在让我们改造下上面的代码： 代码块5123456789101112public static void main(String[] args) { int gender = 1; int age = 5; int color = 1; String name = \"加菲\"; Cat cat1 = new Cat(gender, age, color, name); Cat cat2 = cat1; //这里把cat1直接赋值给cat2 boolean result = cat1 == cat2; System.out.println(result);} 这时结果就为true，这里应该都理解为什么了，因为cat1本身是引用变量，保存的是new出来那个对象的内存地址，现在直接把它的值赋值给cat2，因此cat2里保存的地址跟cat1中是一样的，所以它们一定是相等的。 ⚜️ 注4： 跟==性质一样，只不过它用来表示不等于，运算方式跟==相反。 所以： 代码块6123char a = '我';int b = 25105;boolean d = a != b; 这个例子把==换成!=后，输出结果就为false了，引用变量就不举例了。 1.5：逻辑运算符讲完了关系运算符，现在再来认识下逻辑运算符，逻辑运算符主要用来运算两个或多个布尔值间的真假，因此逻辑运算符两边连接的变量必须是boolean类型的数据。 运算符 说明 示例 备注 &amp; 与运算 a &amp; b 参考注5 | 或运算 `a b` ^ 异或 a ^ b 同上，只是逻辑同“异或” ! 非 !a 参考注6 &amp;&amp; 短路与 a &amp;&amp; b 参考注7 || 短路或 `a 表5 ⚜️ 注5： 没错，又是这个符号，我们在1.2里的位运算符里已经碰到过了，只不过1.2里它连接两个非boolean类型的变量，用来表示两个变量的二进制数的与运算， 这里它被用来连接两个boolean类型的变量时，由于boolean类型的数据仅有真和假两种状态，因此它仅用来做简单的与门转换： 代码块7123456boolean a = true;boolean b = false; boolean c = a &amp; b; //a和b相与 System.out.println(c); 结果为false，因为true &amp; false = false 结合1.4里的关系运算符，可以知道任意关系运算符连接的式子，其结果一定是一个boolean值，那么我们可以写更为复杂的相与代码： 代码块8123456789int a = 1;int b = 1; int c = 5;int d = 6; boolean r = (a == b) &amp; (c != d); System.out.println(r); 结果为true，因为使用关系运算符==连接的a、b结果为true，而用!=连接的c、d的结果也为true，那么最终符号&amp;连接的boolean值就被简化成了：true &amp; true 结果自然是true，利用这种特性我们后续在java程序里可以做很多逻辑判定，比如我们之前举得Cat类里的run方法： 代码块91234567public void run() { if (age &gt; 10) { //这里就是一个逻辑判定，利用关系运算符&gt;连接两个数，当age大于10的时候条件成立为true，输出10km/h，否则输出下面的15km/h System.out.println(\"奔跑速度：10km/h\"); } else { System.out.println(\"奔跑速度：15km/h\"); }} ⚜️ 注6： 类似位运算里的按位取反~，即让自己的结果值反过来，例如： 代码块10123456int a = 1;int b = 1; boolean c = !(a == b); System.out.println(c); a == b这个关系语句的结果肯定是true，但是它在外层加了!，意味着结果要跟实际值反过来，因此c的结果为false。 ⚜️ 注7： 相比&amp;，短路与更加“智能”，如果第一个条件不成立，则直接返回结果，这么说不容易理解，下面通过一个例子来说明： 代码块111234567891011int a = 1;int b = 1; int c = 5;int d = 6; boolean r1 = (a != b) &amp; (c != d);boolean r2 = (a != b) &amp;&amp; (c != d); System.out.println(r1);System.out.println(r2); 上面r1和r2的执行结果是一致的，都是false，但执行的方式却不一样： 上述就是短路与和正常与参与运算时的区别，但是如果第一个条件成立(true)，仍然需要进行后续条件的判断，这时与&amp;无异，但短路与总是尽可能的减少关系运算的次数。 一般建议使用短路与做与逻辑运算。 1.6：三元运算符通过对上面关系运算符的了解后，现在来了解一种特殊的运算符：三元运算符（xx ? yy : zz）。 三元运算符会结合赋值运算符和关系运算符合计使用，旨在挑选符合自己预期的值（其实跟后面要讲的条件语句if/else有些像）。 现在来简单了解下其结构： 上图展示了一个三元表达式以及其组成部分。 最终s的值为11； 我们还可以嵌套三元表达式： 代码块1212345678//声明变量&amp;赋值int a = 1;int b = 2;int c = 5;int d = 6; int s = (a &gt; b) ? 10 : ((c &lt; d) ? 66 : 77); 首先判断a&gt;b是否成立，由a和b的值可知是不成立的，所以会跳过10这个结果，选取冒号后面的值，而冒号后面仍然是一个三元表达式，这个三元表达式的值为66，因此s最终等于66。理论上java允许你无限套娃，但为保证代码可读性，不建议嵌套三元表达式 二、运算符间的优先级2.1：运算顺序猜想我们在之前的示例代码里经常会看到类似下面这样的代码： 代码块13123int a = 1; //声明变量a，并通过赋值运算符为其赋值int b = 2; //声明变量b，并通过赋值运算符为其赋值int c = a + b; //声明变量c，然后通过赋值运算符为其赋值，值为a和b通过算术运算符”+“运算后的结果值 那么类似int c = a + b;这种语句，它的执行顺序为什么不是直接给c通过”=“运算符赋上a的值，然后再计算a和b的”+“运算呢？即： 如图，如果我们把int c = a + b;这种语句按照上图的拆分方式进行运算，那么这段代码的最终运行结果将是c等于a的值，其余无事发生，然而经过前面那么多示例，我们知道这是不对的，c的值应该等于a+b的运行结果才对，那么这就意味着加法运算发生在赋值运算之前，即： 真正的执行过程应该是图4中的过程，这样的结果似乎在告诉我们，运算符之间存在着某种等级，等级高的先执行，等级低的往后稍稍，这就是运算符之间的执行优先级，通过我们对int c = a + b;这种语句的了解，我们至少已经知道了”+“应该发生在”=“之前，即算术运算符应发生在赋值运算符之前执行。 2.2：运算符的优先级列表那么我们现在就来列一下所有运算符的执行优先级： 优先级（从高至低排序） 运算符 结合性 1 ( ) [ ] . 左→右 2 ! ~ ++ – 右→左 3 * / % 左→右 4 + - 左→右 5 &lt;&lt; &gt;&gt; &gt;&gt;&gt; 左→右 6 &lt; &lt;= &gt; &gt;= instanceof 左→右 7 == != 左→右 8 &amp; 左→右 9 ^ 左→右 10 | 左→右 11 &amp;&amp; 左→右 12 || 左→右 13 ? : 右→左 14 = += -= *= /= &amp;= |= ^= ~= &lt;&lt;= &gt;&gt;= &gt;&gt;&gt;= 右→左 表6 2.3：实例结合上面的表，我们通过几个例子来固化一下我们的记忆： 例1：括号的运算等级是最高的代码块14123int a = 1;int b = (a + 1) * 2; //这里由于括号运算符的优先级高于一切，所以括号内的内容a+1先被执行int c = (a + b) + (22 + 5); //如果有多个括号，则依据\"结合性\"可知，应该由左至右执行，因此这里的执行顺序为先执行a+b，再执行22+5 这个符合数学里的运算规则，例如： 乘除法的优先级高于加减法，所以类似“int c = a + b * 2”，首先计算的是b2的值，然后再拿着b2的结果跟a相加，但是如果改成“int c = (a + b) *2”的话，就变成先执行括号内的加法运算，然后拿着和去和2相乘。 例2：括号、非运算、与运算、或运算结合普通非运算+括号： 代码块151234//非int a = 1;int b = 2;boolean s = !(a &lt; b); // 按照优先级，括号内的先运算，a&lt;b结果为true，然后再运算仅次于括号的运算符!，最后赋值给s，s的值为false 嵌套非运算： 代码块16123456int a = 1;int b = 2; boolean s = !!(a &lt; b);// ↑首先计算优先级高的括号内的值，为true，然后同级非运算的结合性按照从右到左，因此右侧的\"!\"首先参与运算，// 结果为false，然后拿着结果再用最外侧的\"!\"参与运算，结果为true 括号、嵌套非运算、与、或运算结合： 代码块17123456int a = 1;int b = 2;boolean c = false;boolean d = true;boolean e = true;boolean s = !!(a &lt; b) &amp; c &amp; d | !!!e; 这个看起来很复杂，我们借助流程图来梳理下： 例3：i++、++i问题由表6可知++运算符优先级仅次于括号。 类似这种++的方式，有两种写法：a++和++a ++位于变量的前面和后面最终变量的结果都是一样的： 代码块18123456789//声明变量a和b，它们的值都是1int a = 1;int b = 1; a++; //a自增1，++放其后面++b; //b自增1，++放其前面 System.out.println(a);System.out.println(b); 最终输出结果都是2，那么++放前面和放后面有何区别呢？ 代码块1912345678910111213//声明变量a和b，它们的值都是1int a = 1;int b = 1; int c = a++; //a自增1，++放其后面，将a的值赋值给cint d = ++b; //b自增1，++放其前面，将b的值赋值给d System.out.println(a);System.out.println(b); System.out.println(c);System.out.println(d); 上面输出结果，a和b自然还是等于2，但是c和d却不相同： c最终等于1，d最终等于2. 首先我们知道++操作相当于变量自增，a++相当于a = a+1，所以类似这种语句： int c = a++; 相当于有两个赋值运算符： 先记住这个流程，下面来看下++放到变量后面和前面的区别： 所以 int c = a ++;这段代码是先执行的赋值运算符，因此c的值等于还没完成自增前的a。相反的，如果++放到自增变量前面，则意味着它的执行优先级高于赋值运算符，所以 int c = ++ a;这段代码是先执行++操作，然后再将++后的值赋值给c。 ❓ 思考：请问下方的a、b、c的最终值分别等于多少？ 代码块2012345int a = 1;int b = a++ + 1;int c = ++b + 1;System.out.println(b);System.out.println(c); 答案：a=2，b=3，c=4 三、运算符下的自动转型我们通过LV-2-四可以知道基本类型是会区分按照取值范围来区分大小的，我们再次把它们的大小等级贴出来： 我们还知道了小类型不可以接收大类型的数据，如必须要接收，需要做强转，而如果使用一个大类型接一个小类型的值，则这个小类型的值会自动转换成对应的大类型，这种类型的转换叫类型自动转换，贴一下当时的例子。 例1：代码块2112int α = 1; //声明int型的变量α，值为1long β = α; //声明long型的变量β，使其值等于α的值，由于long型表示范围比int型大，因此α的值不需要做类型强转，但其实是α的值类型已经自动转成long了 现在结合本节的运算符知识再来看下这个类型自动转换是如何发生的： 首先声明的α是一个int型，然后使用大类型的β接收，此时α的值（也就是1）已经自动转成了long型，这就是运算符导致了类型的转换，在本例中，=就是这个运算符（赋值运算符），因为α的值被赋给了β，因此α值的类型自动提升为long。 结论：运算符可以导致某些数据变量值的类型转换。 例2：代码块22123int a = 1;float b = 1;int s = a + b; //这是不允许的，经过加法运算，a的值已经被自动提升为float型，因此a+b的值是一个float型，较小的int是不能接收较大的float型的 上述注释内的内容用下图来描述： 其实最终都要回归到强制类型转换那一部分内容来。 现在改一下代码，解决上述问题： 代码块23123456int a = 1;float b = 1; float s = a + b; //这种改法就是直接用大类型接收或：int s = a + (int)b; //这种改法就是将大类型强转成小类型，这样它们经过加法运算后类型不会提升，还是int 参考注释，最终还是回归到要么用大类型接收小类型，要么强转的解决方法上来，针对例1，我们可以说是“=”运算符让变量α的值类型提升，而在本例，我们可以说是“+”运算符让a的值类型得到提升。 为了巩固这块的知识，再让我们改造下上面的代码： 代码块24123456int a = 5;int b = 2; float s = a / b; System.out.println(s); 这个结果是2.5吗？答案并不是，最终结果是2. 为什么？？明明5/2是2.5啊，这里结合前面的例子，你只需要稍微思考一下就可以得到答案： ❓ 思考：那如何才能得到自己想要的2.5呢？ 答案：很简单，只需要提升a+b的类型就行了，即：(float)a+b 或 a+(float)b，当然，两个都强转也行：(float)a+(float)b，这样在上图①处在除法运算时就会将a/b的结果提升为float，自然就不会失精。","link":"/2020/02/28/LV1-3%EF%BC%9Ajava%E4%B8%AD%E7%9A%84%E8%BF%90%E7%AE%97%E7%AC%A6/"},{"title":"LV1-2：java中的变量类型","text":"一、基础变量类型1.1：整型即整数类型，如1，2，0，-1，-2这种。 整型变量在java中的声明方式如下： 代码块112int a; //声明名称为a的整型变量（默认值为0）int b = 5; //声明名称为b的整型变量（初始化值为5） 整型变量在内存中占据32个bit位（4字节），每个bit位可存储0和1，通常第一位为符号位（±），所以有效数字部分就占了31位，如图（图中表示十进制数字+6）： 按照这个特性，整型的取值范围为： (-2^31) ~ (2^31)-1 即：-2147483648 ~ 2147483647 除了浮点型，其余只要有符号位的类型，都符合这个范围规则。 为什么负数的数值位比正数多1，计算机是如何做减法运算的？请参考：JAVA有关位运算的全套梳理 1.2：长整型长整型变量在java中的声明方式如下： 代码块212long a; //声明名称为a的长整型变量（默认值为0）long b = 5; //声明名称为b的长整型变量（初始化值为5） 这种类型所表示的数值范围相比整型的位数，多了一倍，占据64个bit位（8字节），因此其取值范围为： (-2^64) ~ (2^64)-1 即：-9223372036854775808 ~ 9223372036854775807 1.3：短整型短整型变量在java中的声明方式如下： 代码块312short a; //声明名称为a的长整型变量（默认值为0）short b = 5; //声明名称为b的长整型变量（初始化值为5） 这种类型所表示的数值范围相比整型的位数，少了一倍，只占据16个bit位（2字节），取值范围为： (-2^15) ~ (2^15)-1 即：-32768 ~ 32767 巩固： 这里说明下，short、int、long本质上都表示整数，因此像下面这种写法都是ok的： short a = 1; int a = 1; long a = 1; 那为什么要如此区分呢？这就存在一个取值范围的问题，因为这三种类型的二进制位数不一样，所以他们所能容下的数值范围也不同，具体参考下图： 所以今后在写程序时，需要严格推断数值的极端可能性，然后设计出合理接收数据的类型。 举个反例：比如某个系统的id，id增长特别迅速，开始设计用int类型接收id，后来id值超过了int表示数据的最大范围，就会出现很严重的问题。 1.4：单精度浮点型浮点型变量在java中的声明方式如下： 代码块412float a; //声明名称为a的单精度浮点型变量（默认值为0.0）float b = 5.0f; //声明名称为b的单精度浮点型变量（初始化值为5.0） 单精度浮点型变量在内存中占据32个bit位（4字节），相比整型，它存在指数位，分布如下： 取值范围为：-2^128 ~ 2^127 即：-3.40E+38 ~ +3.40E+38 注：java中如果单独写一个类似0.1这样的小数，默认是双精度，如果用float接收，会报错，一般需要在小数后面加个f修饰符（隐式类型转换）。 1.5：双精度浮点型双精度变量在java中的声明方式如下： 代码块512float a; //声明名称为a的浮点型变量（默认值为0.0）float b = 5.0; //声明名称为b的浮点型变量（初始化值为5.0） 双精度浮点型变量在内存中占据64个bit位（8字节），相比单精度浮点型数据，它用来存放指数和尾数的位数明显更多，分布如下： 1.6：字符型字符变量在java中的声明方式如下： 代码块612char a; //声明名称为a的字符型变量（默认值为0，对应的字符为空字符，BLANK）char b = '我'; //声明名称为b的字符型变量（初始化值为汉字'我'，对应的Unicode码为25105） 字符型变量在内存中占据16个bit位（2字节），java语言的字符基于Unicode，无论是数字、字母，还是汉字，均占2个字节。 按照16位的保存方式，char的数字取值区间为：0 ~ 65535 1.7：字节字节变量在java中的声明方式如下： 代码块712byte a; //声明名称为a的字节变量（默认值为0）byte b = 5; //声明名称为b的字符型变量（初始化值为5） 字节变量在内存中占据8个bit位 取值区间为：(-2^7) ~ (2^7)-1 即：-128~127 一个字节就是一个数字，字节存储的数字聚合在一起就变成了字节流（byte[]）字节流可以被不同的编码方式解码成具体某一个字符或者某一段语句，现有的文本信息，在计算机中以字节的方式进行存储，存储前通过某种编码方式编码，获取到时通过同样的编码方式解码即可获取到具体的文本信息。 1.8：布尔值布尔类型的变量在java中的声明方式如下： 代码块812boolean a; //声明名称为a的布尔变量（无初始化值时，默认值为false）boolean b = true; //声明名称为b的布尔变量（初始化值为true） 这种类型的变量只有true或false两种值，用来表示真或假。 布尔变量在内存中占据8个bit位（1个字节，存在争议，JVM规范中提到应占4个字节）。 二、字符串类型&amp;引用变量上面所有的类型，均为java的基本类型，除了基本类型变量，其他的所有变量均为引用变量，这里介绍一种引用变量：String String相比char只能表示一个字符来说，它可以用来表达一段话，它的声明方式如下： 代码块9123String a; //声明名称为a的String类型变量（由于其非基本类型，因此a属于引用变量，引用类型变量在不赋值的情况下默认值为null）String b = \"这是一段话\"; //声明名称为b的String类型变量（初始值为：这是一段话），常用的声明字符串的方式String c = new String(\"这是一段话\"); //声明名称为c的String类型变量（初始值为：这是一段话）这种写法不建议 字符串的拼接也很简单： 代码块10123String a = \"哔哩哔哩 \";String b = \"乾杯~\";String c = a + b; //c最终的结果为：哔哩哔哩 乾杯~ 三、基本类型转换我们按照基本类型所能表示的数值范围来区分它们的大小，现可以做出如下排序： 所谓类型转换，就是java语法里为了程序严谨性所做的一层保护，举个例子，一个比较大的类型long，声明了一个变量，我们记为α，现在用一个比较小的类型int来接收α，这在java的语法里是不允许的，因为你不知道α的大小是不是已经超出int型所能表达的范围值了，但是相反的，如果现在声明一个int型的变量，我们记为β，这时使用long型接收β在java的语法里是允许的，因为int型的数顶天了也不可能超过long型所能表达的范围，所谓基本类型的转换就是这么回事，下面来看看具体的场景。 注：布尔类型是不参与类型转换的 4.1：强制转换通过上面简短的介绍，已经知道了java为什么要做一些类型转换的操作，那么哪些情况下需要做强制类型转换呢？ 按照类型大小排列，long型可以接收int型数据，而int型要想接收long型数据，就需要做强转，语法如下： 代码块1012345long γ = 1; //声明long型的变量γint λ = (int) γ; //声明int型的变量λ，由于int型表示范围比long型小，因此有超范围的风险，因此java要求必须做一层强转 int α = 1; //声明int型的变量αlong β = α; //声明long型的变量β，使其值等于α，由于long型表示范围比int型大，因此α不需要做类型强转 比较特殊的就是short和char型变量，它们俩由于位数相同，虽然表达的取值范围不同，但是其类型大小在java眼里是相同的，所以它们俩需要互转，不存在谁比谁大的情况： 代码块11123456//short和char不管由谁接收谁，均需要强转short γ = 1;char λ = (char) γ; char α = 1;short β = (short) α; 这个规则放到方法传参时同样是适用的： 代码块121234567891011public static void main(String[] args) { short param1 = 1; method(param1); //method接收的参数是int型，声明的参数值却是short型，short比int表达范围小，因此调用method时，不需要强转 long param2 = 1; method((int) param2); //method接收的参数是int型，声明的参数值却是long型，调用method时，需要强转}//这是一个接收int型参数的方法public static void method(int param) { //方法逻辑省略...} 4.2：自动转换自动转换就是指取值范围小的数据类型可以自动转换成取值范围大的数据类型。 需要结合4.1理解，4.1介绍了强制类型转换，例子中不需要强转的统统都是自动转换的，如代码块10中的α和β的类型转换： 代码块1312int α = 1; //声明int型的变量α，值为1long β = α; //声明long型的变量β，使其值等于α的值，由于long型表示范围比int型大，因此α的值不需要做类型强转，但其实是α的值类型已经自动转成long了 四、小结通过本节，我们知道了java里的基本数据类型，字符串类型，以及引用变量的简单介绍。 下一节将会介绍java中的运算符，让本节里的数据通过运算”活起来“。","link":"/2020/02/26/LV1-2%EF%BC%9Ajava%E4%B8%AD%E7%9A%84%E5%8F%98%E9%87%8F%E7%B1%BB%E5%9E%8B/"},{"title":"LV1-1：安装java环境&第一个java程序","text":"一、java环境组成 JRE：java运行时环境，包含了java虚拟机，java基础类库。是使用java语言编写的程序运行所需要的软件环境，简单来说，如果你只是想运行你的java程序，只安装JRE就行了。 JDK：java开发工具包，提供了全套JRE、编译器、一些开发工具包，如果你想要开发java程序，需要安装一个JDK。 二、java环境安装上网查，太多了 暂时这个吧~https://www.jianshu.com/p/efef80171a4a 三、第一个java程序java程序的入口，都是从main方法开始的，所以在初期做实验的时候，至少要定义一个public类，里面放入一个main方法（不理解啥是类？啥是方法？不要紧，先记住下面的写法即可，反正后面都会介绍到）： 图里可能要素过多，目前了解即可，等学完类和方法部分的内容，再翻过来看这个图，就会很容易理解了。 经过第二步，假设我们已经安装好了java环境，并且配置好了环境变量，现在在终端执行下这个指令： 代码块11java -version 就会打印出java的版本号信息，一旦打印出版本号信息，说明你安装的java环境是没有问题的： 代码块2123java version \"11.0.2\" 2019-01-15 LTSJava(TM) SE Runtime Environment 18.9 (build 11.0.2+9-LTS)Java HotSpot(TM) 64-Bit Server VM 18.9 (build 11.0.2+9-LTS, mixed mode) 现在来写一个最基本的程序： 代码块312345public class Test{ public static void main(String[] args){ System.out.println(\"Hello world!\"); }} 新建一个叫Test.java的文件，将这段代码拷贝进去。 然后现在输入这个指令，用来编译代码： 代码块41javac Test.java 然后你就得到了一个字节码文件：Test.class 字节码文件才可以被java直接运行，通过下面这个指令（注：.class后缀是可以省略不写的）： 可以看到，已经成功运行了。 通过本例子可以知道，java想要执行一个java程序，需要将java源代码文件（例子中的Test.java），通过javac指令编译成字节码文件（例子中的Test.class），然后通过java指令运行对应的字节码文件即可。","link":"/2020/02/25/LV1-1%EF%BC%9A%E5%AE%89%E8%A3%85java%E7%8E%AF%E5%A2%83&%E7%AC%AC%E4%B8%80%E4%B8%AAjava%E7%A8%8B%E5%BA%8F/"},{"title":"JAVA进化论（序）","text":"序章 最近有一项工作，让测试同学对java这门语言进行入门，并且可以独立review开发写出来的代码，还可以朝着测试开发工程师去做。想着自己正式做java开发已经快4年了，不知不觉接触java这门计算机语言也已经6年多了，想着给别人讲一些基础做下入门应该没啥大问题了吧，但其实在整理过程中发现，有些东西想要整理的很通俗详细还是很难的，不过基础篇的教程最后还是坚持下来了，自己收获也不少，很多基础知识在自己整理的过程中又加深了一遍记忆，同时，作为一种个人经历，我也想把这些文档放到个人博客里。关于进阶篇，没有继续更新，因为整理的太细，课程耗时会很久，就采用了另外一种速成的方式交给他们这些进阶的知识点了，所以课程文档本身到基础篇就断掉了。 下面的脑图是对java基础知识&amp;进阶知识的梳理（可能有些地方漏掉了）","link":"/2020/02/24/JAVA%E8%BF%9B%E5%8C%96%E8%AE%BA%EF%BC%88%E5%BA%8F%EF%BC%89/"},{"title":"JVM基础回顾记录（二）：垃圾收集","text":"上一篇介绍了jvm的内存模型，本篇将介绍虚拟机中最为复杂的一部分：垃圾收集，本篇会从垃圾回收前的准备工作到后面的收集阶段的方式以及HotSpot虚拟机对这些工作的实现做个较为系统的记录，方便自己以后查找阅读。 一、栈帧、变量类型、引用分析讲解垃圾收集器的实现之前，结合之前讲的jvm内存区域划分，先来看下HotSpot虚拟机中对于对象的访问定位是怎样的，对象访问定位如下图所示： 一般来说，一个方法视为一个栈帧，栈帧内会存放当前方法所有的变量，这就是栈的本地变量表，变量分为基本类型变量和引用类型变量，基本类型是直接在栈空间分配存储空间的（也即是当前栈帧内，受-Xss一定的影响），而引用类型则是预先无法知晓其内存占用量为多大，因此需要动态分配内存，这就需要借助堆区的存储，这种一般是堆区创建对象实例，而栈帧内创建一个引用reference，其存放的实际上是指向堆区实例对象的地址，实例对象的内部也有很多成员变量，其次实例对象也会有一个指针指向方法区里的属于该对象的类型数据（这一步就标记了当前实例属于哪个Class）。 先来看一段程序： 123456789101112131415161718192021222324public class Student { private int id; private int age; private Teacher teacher;}public class Teacher { private int id;}public static void main(String[] args) { int a = 1; long b = 2L; Student student = new Student(); student.setId(1); student.setAge(2); Teacher teacher = new Teacher(); teacher.setId(1); student.setTeacher(teacher);} 代码块1 上面是很简单的一个程序，main方法里声明了一些变量以及初始化了一些对象，那么上面的程序执行过程中的引用关系表示为下图： 上图就是执行到main方法尾部的最终引用关系，我们知道像栈帧里的数据（本地变量表）随着方法的执行结束，自然就被释放了，因此像main方法里的a、b、student、teacher是不需要GC来关心的，GC需要关心的是图2里堆区的俩实例对象，这俩实例对象是不会随着栈帧的结束而被回收掉的，因此需要借助GC来进行回收，那么什么样的对象可以被回收呢？这又回到上一篇里对可达性分析算法的描述了，其实就是看看当前实例还有没有被GC Root引用，针对上例，GC Root就是student和teacher，这俩引用没了，引用局面就变成了下面这样： 很显然，两个实例已经失去了GC Root的引用，尽管Teacher实例还存在一个强引用，但是基于可达性分析算法的特性，也会被判定为“不可达”对象，最终俩实例会被GC回收掉。 到这里总结一下，栈帧里存放的本地变量表，存在两种类型的变量：基本类型&amp;引用类型，基本类型由于预先知道要为其分配多大内存，因此会直接在栈帧内创建，创建完毕后随着栈帧的结束而销毁，引用变量（非基本类型的其他类型变量）由于预先并不清楚需要分配多大的内存，可能后续需要动态扩容等，需要放入堆空间，栈帧内保留一个指向其内存地址的引用变量，而放入堆空间意味着实例本身无法随着栈帧的消失而被释放，需要借助GC来完成回收。 二、预回收阶段2.1：回收前的对象判活通过第一部分，大致了解了两种变量类型，而GC需要关心的则是GC Root，典型的作为GC Root的类型，则是引用类型，而判定变量是否为引用类型变量，往往影响着GC的实现。 这里所处的阶段是预回收阶段，这个阶段的GC不会发生回收，所以先不说回收阶段的算法（下面将要介绍的标记-清除算法、标记-压缩算法、复制算法），GC要进入回收阶段，首先要做的事情就是判断出“哪些对象还是活着的”，那么这个阶段需要做的事情是分析出对象的存活状态，有哪些方式可以判断出对象的活性呢？这里有两种主要方式：引用计数&amp;可达性分析算法，其中可达性分析算法是根据GC Root来进行判断的，所以需要关心栈帧里的引用类型变量，这个判断引用变量的方式，又分为保守式和准确式，这些判定方法的选择往往会影响GC的实现。 2.1.1：引用计数法这种判定算法非常简单，给对象添加一个引用计数器，每当有一个地方引用它，就加1，每当一个引用失效，就减1，当引用计数器为0的时候，就认为该对象失活，处于可回收状态。但是这种算法是有缺陷的，比如对象的循环引用问题，如下： 123456A a = new A();B b = new B();a.b = b;b.a = a;a = null;b = null; 如上代码，首先A、B的实例首先被a、b引用，这时加1，然后又被a.b、b.a引用一次，再次加1，然后a、b指向null，A、B实例失去了一个引用，但是计数器里的引用数还是1，单看上面的代码，A、B对象的实例应该是需要被回收的的，因为不存在任何方法栈的引用，反而是它们内部的属性互相引用着彼此，因此利用这个算法，很大程度上会造成内存泄漏的问题。jvm在上述代码中，也是会把A和B的实例对象给回收掉的，因此JVM并不是采用这种方法分析对象活性的。 2.1.2：可达性分析算法这个算法的基本思路就是通过一系列的GC Roots的对象作为起点，然后从这些节点往下搜索，搜索走过的路径被称作“引用链”，当一个对象到GC Roots没有任何引用链相连时，则认为该对象死亡，GC Roots通常包含：虚拟机栈（栈帧中的本地变量表）中引用的对象、方法区中的类静态属性引用的对象以及常量引用的对象、本地方法栈中的native方法引用的对象。 那么再利用此算法来看看上述AB循环依赖问题例子的引用变化： 通过图4可以看到，最后失去GC Roots的引用后，A、B的实例对象到GC Roots没有任何引用链相连，因此最终会被判定为死亡对象，进而被回收。 目前JVM的对象死亡判定也是通过该算法来进行判断的，当一个对象到GC Roots“不可达”时，即被认为“可回收”。 通过之前对可达性分析算法的介绍，这种判活方法主要根据判定一个对象是否直接或间接被一个GC Root引用，而引用类型的变量才会作为GC Root，因此采用这种判活算法，往往要先分析出哪些变量属于引用类型，因为有了这一步，所以可达性分析算法又根据JVM的实现不同，影响着GC的实现方式，因而GC又被分为了两种分析方式： 2.1.2.1：可达性分析算法-保守式GC如果JVM不选择记录下来栈帧中的变量哪些是引用类型，哪些是基本类型，对于GC而言，是无法知道变量类型，于是GC在回收前夕，需要遍历栈帧里的每一个变量，通过一些判定条件来分析出当前变量是否是引用变量，这些条件包括边界检查、对齐检查等，符合标准的，会被视为引用类型变量，否则为基本类型变量，这种判定方式比较简单，但是存在缺点，比如一个对象实际上没有任何引用存在了，但是仍然存在“疑似指向”它的指针，使得其逃过被GC的命运，这里“疑似指向”的意思是说，栈帧里可能恰好存在一个通过了条件检查（边界、对齐等检查都符合引用类型数据的条件）的变量，这时GC会认为它就是一个GC Root，被认为是GC Root后，其值正好对应上这个无用对象的地址，那么这个死对象仍然被认为“可达”，此时就会绕过了一次GC。 2.1.2.2：可达性分析算法-准确式GC这是目前HotSpot虚拟机会采用的一种枚举GC Roots的方式（下面③会介绍实现方式），准确式GC不同于保守式GC，准确式GC是在枚举GC Roots时，GC已经知道了栈帧里所有对象的类型，这就免去了上面的不确定检查，因为预先知道栈帧里的变量是引用类型还是基本类型，那么枚举GC Roots就变的非常准确，这就是准确式GC。 2.1.2.3：HotSpot对于准确式GC的实现-OopMapHotSpot虚拟机是采用OopMap来实现引用类型标记的，OopMap可以记录下当前栈帧里所有引用类型变量，GC时，只需要读取这里面的变量即可，很多资料会提到OopMap是提高了枚举GC Roots的效率，其实这不是OopMap真正的目的，OopMap的实现是HotSpot用来实现准确式GC的，而这样处理，恰好对枚举效率的提升也起到一定的帮助（比如枚举时完全可以忽略掉那些基本类型变量了，因为他们不会被记到OopMap里）。 2.2：STW上面说了GC开始前针对对象的判活方法，HotSpot通过OopMap实现了准确式GC，现在来讲下GC前的准备工作，GC前往往需要将所有正在运行的线程挂起，这是为了防止一些引用在GC过程中还在不停的发生变化而做的一致性保护，这种行为叫做STW（Stop The World），一般来说，STW发生时的线程中断，HotSpot虚拟机需要每个线程将自己的程序执行到指定位置，根据线程是否已经让出CPU资源（即线程状态）而分为两种意义上的位置： 2.2.1：安全点（Safe Point）针对的是GC发生时还在运行的线程，这时候需要等待该线程主动运行到指定位置才中断线程，这些指定的位置，被称为安全点，安全点的意义是什么呢？上面说到HotSpot通过OopMap来完成准确式GC，但是引用关系的变化是不可避免的，每变化一次，就更新一次OopMap显然效率不高，因此JVM更新OopMap的实现就用到了安全点，当线程运行到安全点，记录下当前引用，更新至OopMap即可，这也就解释了为什么程序中断前必须要停靠在最近的安全点上。可作为安全点的位置有：循环末尾、方法临近返回前、方法调用后、抛出异常的位置。 2.2.2：安全区域（Safe Region）意义类似安全点，这个概念是针对GC发生前，程序线程处于“CPU让出状态”，比如线程的sleep状态或者blocked状态，这时该线程是没有能力运行到就近的安全点的，针对此情况，便有了安全区域概念，安全区域是指在一段代码区域中，引用关系是不会发生变化的，在此区域的任意位置开始GC都是安全的。线程在运行到安全区域内，首先会标记自己已经到达了安全区域，那么当JVM需要GC时，发现该线程已经进入安全区域，则不会再去管其状态（无视sleep、blocked等让出CPU的状态），直接对其内部进行OopMap更新，完成GC Roots的枚举，当然，如果线程离开安全区域时，就需要判断是否已经完成了GC或者GC Roots枚举，若完成则继续执行，否则就必须等待直到收到可以离开安全区域的信号为止。 2.2.3：STW中断线程的两种方式安全点和安全区域均为HotSpot虚拟机为了保证枚举GC Roots的准确性而做出的实现，结合安全点、安全区域，在达到安全点时GC对线程的中断（STW）又分为两种中断方式： 2.2.3.1：抢先式中断在GC发生时，首先中断所有线程，然后检查各个线程是否执行到了安全点，如果没有，则恢复对应线程让其运行到安全点再次完成中断。 2.2.3.2：主动式中断在GC发生时，不直接操作线程中断，而是简单设置一个标记，让各个线程去轮询这个标记，如果轮询到该标记，则自己主动中断挂起自己。JVM就采用该中断方式，轮询标记的位置和安全点的位置一般是重合的。 三、回收阶段-垃圾回收算法第二部分讲的是GC前的准备工作，以及HotSpot在GC前针对枚举GC Roots的实现，下面来看下完成GC Roots枚举后的回收阶段的几种回收算法。 3.1：标记-清除算法这是一种最基础的收集算法，这种算法分为两个阶段： 标记出所有需要回收的对象（这里的标记使用可达性分析算法进行判定） 标记完成后统一回收所有被标记的对象 这个算法两个阶段的效率都不高，而且会产生大量的内存碎片，碎片过多可能会导致无法找到连续内存存储较大的对象，所以会被迫提前触发一次GC，图示如下： 由上图可以看到，在回收后，产生了大量不连续的内存单元碎片。单纯从图5看，只能知道这个算法会产生内存碎片，并不能了解整个算法的细节，因此单纯通过上图，是没办法体现该算法效率问题的，图5是大部分资料里都会展示的一个最终态，算法细节则全部省略，导致很多时候，标记-清除算法相比复制算法的效率究竟弱在哪里没有很清晰的认知，下面，通过一幅图来说明下其具体的执行细节： 上图就是标记-清除算法的执行过程，首先从标记阶段到清除阶段，GC执行流程如下： 把从root开始的可能被引用的对象（可达对象）进行一个个的标记，图中的“标记阶段”就是在干这件事，被标记的对象图里已染成绿色。 重复1步骤就可以把所有从root出发的被引用或间接引用的对象全部打上标记 在上述两个过程完成后，标记阶段就算告一段落，接下来就是清除阶段，清除阶段将上述被标记的对象视为“存活对象”，这时会扫描全部对象，将没有被标记的对象清除，同时将有标记的对象的标记去除，方便下次GC使用。 了解完这个算法的执行过程，大致上就知道了为什么这个算法的效率会很低了，如果系统中会创建大量的对象，但只有很少的对象会存活的比较久，这时候该算法的效率在清除阶段的时候，耗时就会很久，因为要整体遍历，而且还要进行大量回收。 3.2：复制算法这种算法将可用内存划分为大小相等的两块，每次只使用其中一块，GC触发时，就将该块内存里还存活着的对象整体复制到另外一块内存上去，然后再把已使用的内存一次性清除掉，下面来展示下回收状态： 同样的，这张图仅用来表现清理过程和最终态，下面，通过图8来说明下这种算法的执行流程： 上图就是复制算法的执行过程： 根据root，找出来所有的可达对象（存活对象），整体复制到新空间里（这个过程类似标记-清除算法里的标记阶段）。 将原来的旧空间里的所有对象整体清除掉（完成复制后，可以认为原来的旧空间里的所有对象都可以回收），下次GC的时候，本次GC意义上的“新空间”就变成了下次GC意义上的“旧空间”，以此类推。 相比标记-清除算法，复制算法虽然内存被一分为二，但是节省了整体遍历所有对象这一步操作，对于那种产生大量对象，但是对象生命周期极短的情况，这个算法相比标记-清除算法效率高了不止一个档次，因为每次仅复制一小批存活的对象，没必要整体遍历所有的对象进行标记判断+清理的操作。所以这种算法适合那种对象多，但大部分对象生命周期短的情况，如果对象多，生命周期长，那么意味着复制算法每次对复制这个动作的开销，是非常大的，这也就解释了，为什么jvm新生代的回收算法采用复制算法，而老年代则不用。 3.3：标记-压缩算法也分为两个阶段： 标记阶段，这个阶段跟标记-清除算法一致，具体流程可以参考图6 压缩阶段，相比标记-清除算法，该算法不再整体遍历所有的对象，而是将带有标记的“存活对象”依次压缩排列，排列完成后，存活对象将紧紧挨在一起，清除时只需要将存活对象边界以外的区域全部清理即可。 过程如图所示： 这里不再画算法的执行流程图，标记步骤参考图6，压缩过程参考图9即可。 这个算法的好处就是不会产生内存碎片，不会大量复制，相比复制算法，内存也不会减半，由于少了一层遍历所有对象的操作，因此一般效率也要比标记-清除算法高。 3.4：分代收集算法目前商业虚拟机的垃圾回收都采用分代收集算法，这种算法基于上述几个基本的垃圾回收算法，通过「分代」的方式分类不同生命周期特征的对象，将不同「代」使用适合的收集算法来实现回收，比如jvm新生代，新生代中的对象特征为生命周期短，每次回收只有少量对象存活，且要求快速，因此适合用复制-收集算法来实现（比如新生代里的Eden区和两个Survivor区，就是为复制算法而拆分出来的），而老年代里的对象因为生命周期长，每次回收大量的对象还处于存活期，如果再使用复制-收集算法来实现，那么复制成本是很高的，因此老年代则适合使用标记-整理算法，这种不使用单一算法，会根据对象的生命周期特征进行算法隔离分区的方式就叫做「分代收集」。","link":"/2019/05/07/JVM%E5%9F%BA%E7%A1%80%E5%9B%9E%E9%A1%BE%E8%AE%B0%E5%BD%95%EF%BC%88%E4%BA%8C%EF%BC%89%EF%BC%9A%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86/"},{"title":"Kingdom Rush Origins","text":"中学时玩的一款游戏，当时玩的应该是初版，今天下午在steam下载了最新版-起源，感觉游戏画面很舒服，风格也是以前的样子，一款休闲的塔防游戏 Kingdom Rush发展史： Kingdom Rush Origins游戏截图：","link":"/2019/04/13/Kingdom%20Rush%20Origins/"},{"title":"JVM基础回顾记录（一）：JVM的内存模型","text":"一、JAVA程序执行流程JAVA程序执行的基本流程（基于HotSpot）： 二、内存模块划分2.1：程序计数器程序计数器是一块较小的内存空间，是当前线程执行字节码的行号指示器，字节码解释器就是通过改变这个计数器的值来获取下一条需要执行的字节码指令，其中分支、循环、跳转和异常处理，线程恢复等基础功能均需要依赖该计数器完成。由于jvm的多线程是通过线程轮流切换并分配CPU执行时间的方式实现，在任何时刻，一个CPU都只会执行其中一条线程里的指令，为了使线程发生切换后可以顺利的定位到上次发生切换时的执行位置，每个线程都有一个独立的程序计数器，每条线程的计数器相互独立，这块存储区域被称为线程私有内存。 2.2：方法区也是线程共享的一块区域，这块区域主要用来存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据，HotSpot虚拟机实现该区域时采用堆的一块区域实现，目的是为了让GC分代收集扩展至方法区（比如类卸载时是需要GC参与的），因此在堆空间里划分出一个代来存储方法区里的内容，这块区域通常被称作“永久代”，该块区域大小通过-XX:MaxPermSize来设置（因此可能会出现OOM的情况），不过需要说明的是，J8已经没有这块区域了，而且J7的时候已经将常量池由该块区域转移到实际的堆内存里了（实验证明，J7和J8的时候，常量池已经被存进了实际的堆内存，但区别是J7的类信息等还放在永久代中）。J8有了元数据存储，已经彻底去除了永久代的概念，具体了解：Java8内存划分 2.3：堆jvm管理的最大一块区域，jvm启动时创建，用来存放对象实例，几乎所有的对象都在这里分配内存，通过-Xmx和-Xms控制其大小。这块区域是GC管理的主要区域，从内存回收角度来看，现在的内存回收基本都采用分代收集算法，所以该区域还可以细分（如图1堆空间细分），进一步划分的目的是为了更好的回收内存或更快的分配内存。这里列一下例子里用到的参数（也是比较常用的参数）： 名称 含义 -xms 堆初始化大小 -xmx 堆最大大小（一般来说，-xms和-xmx设置大小一致，以避免每次垃圾回收完成后JVM重新分配内存） -xmn 新生代大小（结合图1理解），这里需要提一下-XX:newSize、-XX:MaxnewSize这俩参数，第一个是指新生代初始大小，第二个是指新生代最大大小，同样为了避免JVM重新分配内存，这俩数值一般设置为一样的，所以jdk4出来了-xmn，一个配置，可以对上述俩参数同时生效 -XX:NewRatio 新生代（Eden+2Survivors）与老年代的大小比值，如果值为2，则新生代:老年代=1:2（如果设置了-xmn指定了新生代大小，则无需设置此项，两个都设置，只生效一个） -XX:SurvivorRatio 新生代中，Eden区与两个Survivor的大小比值，如果设置为4，则eden:survivor1:survivor2 = 4:1:1） -xss 栈大小，一般默认128k，如果调用栈不是很深（比如很深的递归程序），保持默认即可 表1 2.3.1：堆配置详解现在让我们把图1中的“堆空间细分”部分放大，结合下面的配置信息和表1中的含义（-Xss和永久代不会体现在图里），来用图说明下： 配置1：-Xmx3072m -Xms3072m -Xmn2g -XX:SurvivorRatio=4 -Xss128k 上面的配置表示堆区总大小为3550M，新生代大小为2G（2048M，这里只是说明问题才配置这么大，实际生产中，新生代要小于老年代），新生代Eden区和两个Survivor区的比例为4:1:1，用图来直观的表达一下这个配置： 配置2：-Xmx3072m -Xms3072m -XX:NewRatio=2 -XX:SurvivorRatio=4 -Xss128k 上面的配置表示堆区总大小为3550m，新生代:老年代=1:2（HotSpot默认），新生代eden区和两个survivor区的比例为4:1:1，用图来直观的表达一下这个配置： 2.4：虚拟机栈也就是常说的栈，线程私有，生命周期与线程相同，虚拟机栈用来描述java方法（java method）执行的内存模型，每个方法执行时都会创建一个栈帧，用于存储局部变量表、操作数栈、动态链接、方法出口等信息，一个方法的调用到其完成调用对应一个栈帧在虚拟机的入栈到出栈。 局部变量表里存放着编译期可知的基本数据类型、对象引用Reference（可能是指向实例对象地址的一个引用指针，也可能是代表实例对象的一个句柄）、以及returnAddress（指向一条字节码的执行命令的地址）。在此区域，如果线程的请求深度大于虚拟机允许的深度，将会抛出StackOverFlowError异常，这个可以通过一个无限制递归或者递归深度设置一个很大的数来证明： 123456789101112131415161718public class Recursion { public static int i = 0; public static void main(String[] args) { Recursion t = new Recursion(); t.recursion(); System.out.println(\"程序正常结束~\"); } public void recursion(){ i++; if(i &gt; 30000){ //这里做适当调整，减小则不报栈溢出异常，扩大（比如这里的3w）就会报栈溢出 return; } recursion(); }} 代码块1 看注释那里调整即可证明。栈大小通过-Xss参数来调整，需要注意的是，这个参数是对每个线程生效的（栈帧），一般情况下，这个设置的值越小，支持创建的线程数就越多（并非可以无限多，最终受操作系统限制，操作系统针对每个进程也有个最大线程数的限制），栈里存储的大部分数据，都会随着栈帧的结束（即method调用完成）而被回收，所以一般递归更容易造成栈溢出问题。 2.5：本地方法栈意义类似虚拟机栈，只不过本地方法栈服务于本地native方法调用（JNI），我们常用的虚拟机HotSpot已将该区和虚拟机栈做了合并。","link":"/2019/04/06/JVM%E5%9F%BA%E7%A1%80%E5%9B%9E%E9%A1%BE%E8%AE%B0%E5%BD%95%EF%BC%88%E4%B8%80%EF%BC%89%EF%BC%9AJVM%E7%9A%84%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/"},{"title":"顾村的樱花-20190323","text":"记录于2019年3月23日，上海，多云，7~16℃ 下午从杨浦出发去顾村公园，据说撒苦辣开了，兴奋的骑车过去，骑行1.5小时，逛了1.5小时，回来时找不到共享单车，坐了528路公交，因为顾村连个地铁站都没有==，公交到站，又骑了一小时的车回家。 BGM：夜の向日葵","link":"/2019/03/23/%E9%A1%BE%E6%9D%91%E7%9A%84%E6%A8%B1%E8%8A%B1-20190323/"},{"title":"一Van♂年以后","text":"一VAN♂年以后（视频插入测试）","link":"/2019/03/10/%E4%B8%80Van%E2%99%82%E5%B9%B4%E4%BB%A5%E5%90%8E/"},{"title":"致十年后的我-歌词","text":"这是由doriko制作、初音ミク演唱的一首歌，2015年初遇这首歌，转眼间到了2019年。 歌词 好きな人と歩いた場所も曾和喜欢的人一起走过的地方その時見た景色も那时曾看到的景色振り返らず 今を駆け抜け统统抛掉 不再回头 向前飞奔私は何と出会うの我将会遇见些什么呢 立ち止まるほど驻足不前意味を問うほど探索意义きっとまだ大人ではなくて一定是我还不够成熟今見てるもの现在看到的事物今出会う人现在遇见的人その中でただ前だけを見てる在这片纷繁喧嚣之中 我只会看向前方10年後の私へ致十年以后的我今は幸せでしょうか现在的你感到幸福么？それとも悲しみで还是正沉浸在悲伤中泣いているのでしょうか默默地流着泪？けどあなたの傍に不过在你的身旁変わらないものがあり依然会有不变的存在気付いていないだけで未能察觉的你守られていませんか依然在被守护着吧過ぎし日々に 想いを预け把思念寄托于流逝的日子里時間だけ ただ追いかけてく只有时间在不停的追赶背に寄り添った 誰かの夢に托付在我肩上的 是谁的梦想振り向ける日がいつか来るのかな总有一天必须要面对的吧10年後の私へ致十年以后的我今は誰を好きですか现在的你喜欢着谁呢？それとも変わらずに还是和以前一样あの人が好きですか继续喜欢着那个人呢？けどいつか不过 现在的你知らない誰かを爱する前に在爱上某个人之前自分のことを好きと“喜欢自己”这句话言えるようになりましたか能否先说出来呢大切な人たちは我所珍爱的朋友们今も変わらずいますか依然在反复平凡的生活吗？それとも遠く離れ还是已经远去それぞれ歩んでますか踏上了各自的旅途けど そんな出会いを但是在重复了无数次的相遇别れを 缲り返して和离别之后今の私よりも是否比现在的我すてきになっていますか更有魅力呢？10年後の私へ致十年后的我今がもし幸せなら如果现在的你是幸福的あの日の私のこと从前的我思い出してくれますか能否请你想起来呢そこにはつらいことに回忆中的我泣いた私がいるけど一定在伤心的哭泣吧その涙を優しく请将这温柔的泪水思い出に変えてください融入记忆的海洋","link":"/2019/02/12/%E8%87%B4%E5%8D%81%E5%B9%B4%E5%90%8E%E7%9A%84%E6%88%91-%E6%AD%8C%E8%AF%8D/"}],"tags":[{"name":"池化技术","slug":"池化技术","link":"/tags/%E6%B1%A0%E5%8C%96%E6%8A%80%E6%9C%AF/"},{"name":"连接池","slug":"连接池","link":"/tags/%E8%BF%9E%E6%8E%A5%E6%B1%A0/"},{"name":"Druid","slug":"Druid","link":"/tags/Druid/"},{"name":"位运算","slug":"位运算","link":"/tags/%E4%BD%8D%E8%BF%90%E7%AE%97/"},{"name":"教程","slug":"教程","link":"/tags/%E6%95%99%E7%A8%8B/"},{"name":"基础知识","slug":"基础知识","link":"/tags/%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"name":"数据库","slug":"数据库","link":"/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"TSDB","slug":"TSDB","link":"/tags/TSDB/"},{"name":"InfluxDB","slug":"InfluxDB","link":"/tags/InfluxDB/"},{"name":"JVM","slug":"JVM","link":"/tags/JVM/"},{"name":"GC","slug":"GC","link":"/tags/GC/"},{"name":"NIO","slug":"NIO","link":"/tags/NIO/"},{"name":"网络编程","slug":"网络编程","link":"/tags/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"name":"IDEA","slug":"IDEA","link":"/tags/IDEA/"},{"name":"IDEA插件开发","slug":"IDEA插件开发","link":"/tags/IDEA%E6%8F%92%E4%BB%B6%E5%BC%80%E5%8F%91/"},{"name":"游戏","slug":"游戏","link":"/tags/%E6%B8%B8%E6%88%8F/"},{"name":"皇城突袭","slug":"皇城突袭","link":"/tags/%E7%9A%87%E5%9F%8E%E7%AA%81%E8%A2%AD/"},{"name":"Kingdom Rush","slug":"Kingdom-Rush","link":"/tags/Kingdom-Rush/"},{"name":"内部类","slug":"内部类","link":"/tags/%E5%86%85%E9%83%A8%E7%B1%BB/"},{"name":"map","slug":"map","link":"/tags/map/"},{"name":"散列表","slug":"散列表","link":"/tags/%E6%95%A3%E5%88%97%E8%A1%A8/"},{"name":"位图","slug":"位图","link":"/tags/%E4%BD%8D%E5%9B%BE/"},{"name":"并发编程","slug":"并发编程","link":"/tags/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"多线程","slug":"多线程","link":"/tags/%E5%A4%9A%E7%BA%BF%E7%A8%8B/"},{"name":"集合类","slug":"集合类","link":"/tags/%E9%9B%86%E5%90%88%E7%B1%BB/"},{"name":"juc","slug":"juc","link":"/tags/juc/"},{"name":"线程通信","slug":"线程通信","link":"/tags/%E7%BA%BF%E7%A8%8B%E9%80%9A%E4%BF%A1/"},{"name":"loadbalancer","slug":"loadbalancer","link":"/tags/loadbalancer/"},{"name":"P2C","slug":"P2C","link":"/tags/P2C/"},{"name":"负载均衡","slug":"负载均衡","link":"/tags/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"},{"name":"分布式缓存","slug":"分布式缓存","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"name":"redis","slug":"redis","link":"/tags/redis/"},{"name":"nosql","slug":"nosql","link":"/tags/nosql/"},{"name":"ThreadLocal","slug":"ThreadLocal","link":"/tags/ThreadLocal/"},{"name":"树","slug":"树","link":"/tags/%E6%A0%91/"},{"name":"数据结构","slug":"数据结构","link":"/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"wrr","slug":"wrr","link":"/tags/wrr/"},{"name":"加权轮询算法","slug":"加权轮询算法","link":"/tags/%E5%8A%A0%E6%9D%83%E8%BD%AE%E8%AF%A2%E7%AE%97%E6%B3%95/"},{"name":"鬼畜","slug":"鬼畜","link":"/tags/%E9%AC%BC%E7%95%9C/"},{"name":"分布式任务调度","slug":"分布式任务调度","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BB%BB%E5%8A%A1%E8%B0%83%E5%BA%A6/"},{"name":"PowerJob","slug":"PowerJob","link":"/tags/PowerJob/"},{"name":"火焰图","slug":"火焰图","link":"/tags/%E7%81%AB%E7%84%B0%E5%9B%BE/"},{"name":"熔断","slug":"熔断","link":"/tags/%E7%86%94%E6%96%AD/"},{"name":"Resilience4j","slug":"Resilience4j","link":"/tags/Resilience4j/"},{"name":"CompletableFuture","slug":"CompletableFuture","link":"/tags/CompletableFuture/"},{"name":"spring","slug":"spring","link":"/tags/spring/"},{"name":"ioc","slug":"ioc","link":"/tags/ioc/"},{"name":"aop","slug":"aop","link":"/tags/aop/"},{"name":"链表","slug":"链表","link":"/tags/%E9%93%BE%E8%A1%A8/"},{"name":"gRPC","slug":"gRPC","link":"/tags/gRPC/"},{"name":"线程调度","slug":"线程调度","link":"/tags/%E7%BA%BF%E7%A8%8B%E8%B0%83%E5%BA%A6/"},{"name":"mysql","slug":"mysql","link":"/tags/mysql/"},{"name":"事务","slug":"事务","link":"/tags/%E4%BA%8B%E5%8A%A1/"},{"name":"HikariCP","slug":"HikariCP","link":"/tags/HikariCP/"},{"name":"设计模式","slug":"设计模式","link":"/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"生产者消费者模式","slug":"生产者消费者模式","link":"/tags/%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%BC%8F/"},{"name":"工厂模式","slug":"工厂模式","link":"/tags/%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/"},{"name":"策略模式","slug":"策略模式","link":"/tags/%E7%AD%96%E7%95%A5%E6%A8%A1%E5%BC%8F/"},{"name":"memcache","slug":"memcache","link":"/tags/memcache/"},{"name":"JDBC","slug":"JDBC","link":"/tags/JDBC/"},{"name":"济源","slug":"济源","link":"/tags/%E6%B5%8E%E6%BA%90/"},{"name":"黄河三峡","slug":"黄河三峡","link":"/tags/%E9%BB%84%E6%B2%B3%E4%B8%89%E5%B3%A1/"},{"name":"王屋山","slug":"王屋山","link":"/tags/%E7%8E%8B%E5%B1%8B%E5%B1%B1/"},{"name":"旅行","slug":"旅行","link":"/tags/%E6%97%85%E8%A1%8C/"},{"name":"阻塞队列","slug":"阻塞队列","link":"/tags/%E9%98%BB%E5%A1%9E%E9%98%9F%E5%88%97/"},{"name":"ReentrantLock","slug":"ReentrantLock","link":"/tags/ReentrantLock/"},{"name":"miku","slug":"miku","link":"/tags/miku/"},{"name":"letter song","slug":"letter-song","link":"/tags/letter-song/"},{"name":"链路追踪","slug":"链路追踪","link":"/tags/%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA/"},{"name":"OpenTracing","slug":"OpenTracing","link":"/tags/OpenTracing/"},{"name":"顾村公园","slug":"顾村公园","link":"/tags/%E9%A1%BE%E6%9D%91%E5%85%AC%E5%9B%AD/"},{"name":"樱花","slug":"樱花","link":"/tags/%E6%A8%B1%E8%8A%B1/"},{"name":"序列化","slug":"序列化","link":"/tags/%E5%BA%8F%E5%88%97%E5%8C%96/"}],"categories":[{"name":"池化技术","slug":"池化技术","link":"/categories/%E6%B1%A0%E5%8C%96%E6%8A%80%E6%9C%AF/"},{"name":"连接池","slug":"池化技术/连接池","link":"/categories/%E6%B1%A0%E5%8C%96%E6%8A%80%E6%9C%AF/%E8%BF%9E%E6%8E%A5%E6%B1%A0/"},{"name":"JAVA基础","slug":"JAVA基础","link":"/categories/JAVA%E5%9F%BA%E7%A1%80/"},{"name":"DB","slug":"DB","link":"/categories/DB/"},{"name":"JVM","slug":"JAVA基础/JVM","link":"/categories/JAVA%E5%9F%BA%E7%A1%80/JVM/"},{"name":"JAVA进化论","slug":"JAVA基础/JAVA进化论","link":"/categories/JAVA%E5%9F%BA%E7%A1%80/JAVA%E8%BF%9B%E5%8C%96%E8%AE%BA/"},{"name":"网络编程","slug":"网络编程","link":"/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"},{"name":"项目管理","slug":"项目管理","link":"/categories/%E9%A1%B9%E7%9B%AE%E7%AE%A1%E7%90%86/"},{"name":"InfluxDB","slug":"DB/InfluxDB","link":"/categories/DB/InfluxDB/"},{"name":"NIO基础","slug":"网络编程/NIO基础","link":"/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/NIO%E5%9F%BA%E7%A1%80/"},{"name":"日常","slug":"日常","link":"/categories/%E6%97%A5%E5%B8%B8/"},{"name":"游戏","slug":"日常/游戏","link":"/categories/%E6%97%A5%E5%B8%B8/%E6%B8%B8%E6%88%8F/"},{"name":"服务治理","slug":"服务治理","link":"/categories/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/"},{"name":"分布式缓存","slug":"DB/分布式缓存","link":"/categories/DB/%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"},{"name":"并发编程","slug":"并发编程","link":"/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/"},{"name":"负载均衡","slug":"服务治理/负载均衡","link":"/categories/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"},{"name":"数据结构","slug":"数据结构","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/"},{"name":"ThreadLocal","slug":"并发编程/ThreadLocal","link":"/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/ThreadLocal/"},{"name":"划水","slug":"日常/划水","link":"/categories/%E6%97%A5%E5%B8%B8/%E5%88%92%E6%B0%B4/"},{"name":"框架&中间件","slug":"框架-中间件","link":"/categories/%E6%A1%86%E6%9E%B6-%E4%B8%AD%E9%97%B4%E4%BB%B6/"},{"name":"杂记","slug":"杂记","link":"/categories/%E6%9D%82%E8%AE%B0/"},{"name":"火焰图","slug":"服务治理/火焰图","link":"/categories/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/%E7%81%AB%E7%84%B0%E5%9B%BE/"},{"name":"熔断","slug":"服务治理/熔断","link":"/categories/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/%E7%86%94%E6%96%AD/"},{"name":"图解多线程设计模式","slug":"并发编程/图解多线程设计模式","link":"/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/%E5%9B%BE%E8%A7%A3%E5%A4%9A%E7%BA%BF%E7%A8%8B%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"JUC","slug":"并发编程/JUC","link":"/categories/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B/JUC/"},{"name":"链表","slug":"数据结构/链表","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E9%93%BE%E8%A1%A8/"},{"name":"树","slug":"数据结构/树","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%A0%91/"},{"name":"MySQL","slug":"DB/MySQL","link":"/categories/DB/MySQL/"},{"name":"设计模式","slug":"设计模式","link":"/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"name":"散列表","slug":"数据结构/散列表","link":"/categories/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/%E6%95%A3%E5%88%97%E8%A1%A8/"},{"name":"PowerJob","slug":"框架-中间件/PowerJob","link":"/categories/%E6%A1%86%E6%9E%B6-%E4%B8%AD%E9%97%B4%E4%BB%B6/PowerJob/"},{"name":"旅行","slug":"日常/旅行","link":"/categories/%E6%97%A5%E5%B8%B8/%E6%97%85%E8%A1%8C/"},{"name":"Spring","slug":"框架-中间件/Spring","link":"/categories/%E6%A1%86%E6%9E%B6-%E4%B8%AD%E9%97%B4%E4%BB%B6/Spring/"},{"name":"gRPC","slug":"框架-中间件/gRPC","link":"/categories/%E6%A1%86%E6%9E%B6-%E4%B8%AD%E9%97%B4%E4%BB%B6/gRPC/"},{"name":"生产&消费模式","slug":"设计模式/生产-消费模式","link":"/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E7%94%9F%E4%BA%A7-%E6%B6%88%E8%B4%B9%E6%A8%A1%E5%BC%8F/"},{"name":"模式对比","slug":"设计模式/模式对比","link":"/categories/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/%E6%A8%A1%E5%BC%8F%E5%AF%B9%E6%AF%94/"},{"name":"链路追踪","slug":"服务治理/链路追踪","link":"/categories/%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/%E9%93%BE%E8%B7%AF%E8%BF%BD%E8%B8%AA/"}]}